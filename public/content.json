{"pages":[],"posts":[{"title":"æ¬¢è¿æ¥åˆ°Astroblogï¼","text":"æ¸Šæºæœ¬äºº2019å¹´4æœˆåœ¨åä¸ºäº‘è´­ä¹°äº†ä¸€å°äº‘æœåŠ¡å™¨ã€‚æœ¬æ¥æ‰“ç®—æ˜¯ä¸ºäº†ç»™è‡ªå·±â€œæœªæ¥è¦åšçš„â€œå¾®ä¿¡å°ç¨‹åºæä¾›åç«¯æœåŠ¡çš„ï¼Œç»“æœä¸€ç›´æ‹–åˆ°8æœˆä»½æ‰è´­ä¹°äº†åŸŸåå¹¶å®Œæˆäº†å¤‡æ¡ˆã€‚åœ¨è¿™ä¹‹åä¸€æ®µæ—¶é—´å†…åˆæ²¡æœ‰æ–°çš„å°ç¨‹åºè¦åšï¼Œäºæ˜¯è¿™ä¸ªæœåŠ¡å™¨å’ŒåŸŸåä¾¿ä¸€ç›´è’åºŸäº†å¿«å¤§åŠå¹´ã€‚ç”±äºå¤§ä¸‰ä¸Šå­¦æœŸç»“æŸçš„éå¸¸ä¹‹æ—©ï¼Œæˆ‘äººç”Ÿä¸­ç¬¬ä¸€æ¬¡æ‹¥æœ‰äº†å°†è¿‘ä¸¤ä¸ªæœˆå¯èƒ½å°†è¿‘å››ä¸ªæœˆçš„å¯’å‡ã€‚è¶æ­¤æœºä¼šï¼Œæˆ‘å†³å®šå°†è¿™ä¸ªæœåŠ¡å™¨å…ˆåˆ©ç”¨èµ·æ¥ï¼Œäºæ˜¯å°±æœ‰äº†Astroblogã€‚ ç®€ä»‹Astroblogä¸Šä¸»è¦å°†åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š åœ¨å­¦æ ¡çš„è¯¾ç¨‹æ€»ç»“ï¼Œç›®å‰è®¡åˆ’å°†æ€»ç»“çš„èµ„æ–™å…¨éƒ¨ç”µå­åŒ–ï¼ˆå°½é‡ï¼Œçœ‹å¿ƒæƒ…ğŸ˜‚ï¼‰ ä¸è®¡ç®—æœºæŠ€æœ¯ç›¸å…³çš„æŠ€æœ¯æ€»ç»“ï¼Œæ¯”å¦‚ä¸€äº›æ•™ç¨‹æˆ–æ–¹æ³•ç­‰ï¼Œç”¨ä½œå¤‡å¿˜ï¼Œå¤§è‡´åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š ç¼–ç¨‹è¯­è¨€çŸ¥è¯†æ€»ç»“ è®¡ç®—æœºç½‘ç»œæŠ€æœ¯ ç¨‹åºå¼€å‘ é»‘è‹¹æœï£¿ ä¸ªäººæ‘„å½±ä½œå“ä»¥åŠå…¶ä»–ä¼˜ç§€æ‘„å½±ä½œå“çš„å±•è§ˆ ä¸€äº›æ°‘èˆªçŸ¥è¯† å…¶ä»–å†…å®¹ å…³äºAstrobearç«™é•¿ç°åœ¨ï¼ˆ2020å¹´1æœˆï¼‰æ˜¯ä¸€ä¸ªå¤§ä¸‰å­¦ç”Ÿï¼Œä¸“ä¸šæ˜¯æµ‹æ§æ–¹å‘çš„ï¼Œä»å°å–œæ¬¢èˆªç©ºï¼Œé«˜ä¸­æ—¶å¼€å§‹å–œæ¬¢å¤©æ–‡ï¼Œå¤§å­¦åå¼€å§‹å–œæ¬¢è®¡ç®—æœºã€‚ æ€»ç»“ä¸€ä¸‹ï¼ŒAstrobearæ˜¯ï¼šå­¦æ§åˆ¶çš„æ¢¦æƒ³æˆä¸ºé£è¡Œå‘˜çš„å¤©æ–‡å’Œè®¡ç®—æœºçˆ±å¥½è€…ã€‚ ç”±äºæˆ‘å¹¶ä¸æ˜¯è®¡ç®—æœºä¸“ä¸šçš„ï¼Œæ‰€ä»¥å…³äºè®¡ç®—æœºæŠ€æœ¯è¿™ä¸€å—æ˜¯æœ¬ç€â€œæ‹¿æ¥ä¸»ä¹‰â€çš„æ€åº¦å»å­¦çš„â€”â€”ä¼šç”¨å°±è¡Œã€‚å› æ­¤åœ¨è¿™æ–¹é¢éš¾å…ä¼šæœ‰ç–æ¼é”™è¯¯ä¹‹å¤„ï¼Œä¹Ÿè¯·å¤§å®¶æµ·æ¶µã€‚ å¤šäºæœ‰äº†äº’è”ç½‘çš„å‘å±•ï¼ŒçŸ¥è¯†çš„ä¼ æ’­å¯ä»¥å¦‚æ­¤åœ°è¿…é€Ÿã€‚åœ¨æ­¤ï¼Œå¯¹ä¹‹å‰å¯¹æˆ‘æœ‰è¿‡å¸®åŠ©çš„ç½‘å‹è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼","link":"/2020/01/03/About/"},{"title":"APIs of Multirotor in Airsim","text":"APIs of Multirotor in Airsimby Astrobear Preface All APIs listed below need to add the suffix .join(). Actually, .join() is a call on Pythonâ€™s main process to wait for the thread to complete. All APIs listed below has a hidden parameter, which is vehicle_name. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly. This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article. Control APIstakeoffAsync(timeout_sec): the multirotor will take off when this command is being executed. timeout_sec: take off time, second. Better to greater than 3s but less than 10s. hoverAsync(): the multirotor will maintain its attitude when executed. landAsync(timeout_sec): the multirotor will land when executed. timeout_sec: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process. goHomeAsync(timeout_sec): the multirotor will fly back to its starting point automatically. timeout_sec: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. moveByAngleZAsync(pitch, roll, z, yaw, duration): change the attitude of the multirotor and than change its movement. pitch: angle of pitch, radian. roll: angle of roll, radian. z: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter. yaw: angle of yaw, radian. duration: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia. moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration): change the attitude of the multirotor and than change its movement. pitch: angle of pitch, radian. roll: angle of roll, radian. throttle: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power. yaw_rate: angular velocity at yaw axis, radian per second. duration: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time. moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode): change the velocity of the multirotor. vx: velocity projected at x axis, meter per second. vy: velocity projected at y axis, meter per second. vz: velocity projected at z axis, meter per second. duration: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero. drivetrain: the default value is airsim.DrivetrainType.MaxDegreeOfFreedom, it can also be set as airsim.DrivetrainType.ForwardOnly. yaw_mode: the default value is airsim.YawMode(is_rate=True, yaw_or_rate=0.0), it can also be set as airsim.YawMode(is_rate=False, yaw_or_rate=0.0). Please notice that, under the default setting, the multirotor is not able to yaw when executing this command. moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode): change the velocity at horizontal plane and the altitude of multirotor. vx: velocity projected at x axis, meter per second. vy: velocity projected at y axis, meter per second. z: flight altitude, meter. duration: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero. drivetrain: the default value is airsim.DrivetrainType.MaxDegreeOfFreedom, it can also be set as airsim.DrivetrainType.ForwardOnly. yaw_mode: the default value is airsim.YawMode(is_rate=True, yaw_or_rate=0.0), it can also be set as airsim.YawMode(is_rate=False, yaw_or_rate=0.0). Please notice that, under the default setting, the multirotor is not able to yaw when executing this command. moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead): the multirotor will fly according to several given coordinates. path: a Vector3r array, which provides the route coordinates, meter. The form of it is [airsim.Vector3r(x, y, z), ...]. velocity: flight velocity when traveling, meter per second. timeout_sec: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. drivetrain: the default value is airsim.DrivetrainType.MaxDegreeOfFreedom, it can also be set as airsim.DrivetrainType.ForwardOnly. yaw_mode: the default value is airsim.YawMode(is_rate=True, yaw_or_rate=0.0), it can also be set as airsim.YawMode(is_rate=False, yaw_or_rate=0.0). Please notice that, under the default setting, the multirotor is not able to yaw when executing this command. lookahead: the default value is -1. adaptive_lookahead: the default value is 1. moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead): the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop. x: distance projected at x axis, meter. y: distance projected at y axis, meter. z: flight altitude, meter. velocity: flight velocity when flying to the destination, meter per second. timeout_sec: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. drivetrain: the default value is airsim.DrivetrainType.MaxDegreeOfFreedom, it can also be set as airsim.DrivetrainType.ForwardOnly. yaw_mode: the default value is airsim.YawMode(is_rate=True, yaw_or_rate=0.0), it can also be set as airsim.YawMode(is_rate=False, yaw_or_rate=0.0). Please notice that, under the default setting, the multirotor is not able to yaw when executing this command. lookahead: the default value is -1. adaptive_lookahead: the default value is 1. moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead): the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached. z: flight altitude, meter. velocity: flight velocity when flying to the destination, meter per second. timeout_sec: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty. yaw_mode: the default value is airsim.YawMode(is_rate=True, yaw_or_rate=0.0), it can also be set as airsim.YawMode(is_rate=False, yaw_or_rate=0.0). Please notice that, under the default setting, the multirotor is not able to yaw when executing this command. lookahead: the default value is -1. adaptive_lookahead: the default value is 1. rotateByYawRateAsync(yaw_rate, duration): the multirotor will yaw at the given yaw rate. yaw_rate: yawing angular velocity, degree per second. duration: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.","link":"/2020/01/15/AirSimMultirotorAPIs/"},{"title":"Gallery","text":"Photos will continue to updateâ€¦","link":"/2020/01/03/Gallery/"},{"title":"HP Envy-13 ad024TUé»‘è‹¹æœå®‰è£…æ€»ç»“","text":"è¯·å…ˆäº†è§£ä»¥ä¸‹å†…å®¹æœ¬æ–‡ä¸»è¦ä»‹ç»åœ¨å®Œæˆé»‘è‹¹æœçš„åŸºæœ¬å®‰è£…ä»¥åçš„å®Œå–„è¿‡ç¨‹ã€‚å¯¹äºé»‘è‹¹æœå®Œå…¨æ²¡æœ‰æ¦‚å¿µçš„æœ‹å‹ï¼Œè¯·çœ‹è¿™ç¯‡æ–‡ç« ã€‚è€Œæœ¬æ–‡æ˜¯åœ¨å¾ˆæ—©çš„æ—¶å€™å¼€å§‹å†™çš„ï¼Œå¹¶åœ¨åŸåŸºç¡€ä¸Šä¸æ–­å¢æ·»äº†å†…å®¹ã€‚é‚£æ—¶å€™ä½œè€…è¿˜æœªå¯¹EFIåšè¶³å¤Ÿçš„ä¼˜åŒ–ï¼Œå› æ­¤æœ¬æ–‡åœ¨ç°åœ¨çœ‹æ¥æœ‰ä¸€äº›è¿‡æ—¶ã€‚å‡å¦‚ä½ é‡åˆ°äº†æ–‡ç« ä¸­å‡ºç°çš„ç±»ä¼¼æƒ…å†µï¼Œå¸Œæœ›å¯ä»¥ç»™ä½ æä¾›ä¸€äº›è§£å†³æ€è·¯ã€‚ä½†æ˜¯ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœä½ çš„æœºå‹å’Œç¡¬ä»¶ä¸æˆ‘çš„ç›¸åŒä¸”ä½¿ç”¨äº†æˆ‘æä¾›çš„EFIçš„è¯ï¼ŒåŸºæœ¬å®‰è£…å®Œæˆä»¥åæœºå™¨å°±å·²ç»æ˜¯å‡ ä¹å®Œç¾çš„ä¸€ä¸ªçŠ¶æ€äº†ï¼Œåªéœ€è¦åšå¾ˆå°‘çš„ä¼˜åŒ–å³å¯ã€‚ ä½œè€…ç”µè„‘çš„EFIå­˜æ”¾äºè¿™ä¸ªGithubä»“åº“ä¸­ï¼šHackintoshForEnvy13-ad0xxã€‚ ä½œè€…ç”µè„‘å‹å·ä¸ºHP Envy-13 ad024TUï¼Œå…¶ä¸­éƒ¨åˆ†æ–‡ä»¶ä¸å»ºè®®å¤§å®¶ç›´æ¥ç”¨äºå…¶ä»–å‹å·çš„ç”µè„‘ã€‚è‹¥ä½¿ç”¨æœ¬ä»“åº“ä¸­æ–‡ä»¶å¯¼è‡´ç³»ç»Ÿæ•…éšœæˆ–å´©æºƒï¼Œä½œè€…æœ¬äººæ¦‚ä¸è´Ÿè´£ã€‚ ä½œè€…ç”µè„‘çš„ç½‘å¡å’Œç¡¬ç›˜å‡ä½œäº†æ›´æ¢ã€‚æ•…å³ä½¿æœºå‹ç›¸åŒï¼Œç›´æ¥å¥—ç”¨æ­¤EFIä¾æ—§å¯èƒ½ä¼šäº§ç”Ÿé—®é¢˜ï¼Œè¯·çŸ¥ç…§ï¼ æ­¤EFIä¸€å¼€å§‹æ˜¯æ¥è‡ªäºäº¤æµç¾¤ä¸­æ¥æºä¸æ˜çš„Envy-13é€šç”¨EFIï¼Œé‡Œé¢çš„å†…å®¹æ‚ä¹±æ— ç« è€Œä¸”æœ‰å¾ˆå¤šä¸å¿…è¦çš„é©±åŠ¨å’Œè¡¥ä¸ï¼Œä½†è¿˜æ˜¯å¯ä»¥å°†æœºå™¨é©±åŠ¨èµ·æ¥ã€‚ç»è¿‡å¤§åŠå¹´çš„ç»´æŠ¤ï¼Œæˆ‘å¯¹å…¶ä¸­çš„å†…å®¹ä½œäº†ä¸€äº›ç²¾ç®€ï¼Œä½†æ˜¯å…¶ä¸­çš„æ–¹æ³•ä¾æ—§ç›¸å¯¹è½åå’Œæ‚ä¹±ã€‚ç°åœ¨çš„è¿™ä¸ªEFIåŸºæœ¬ä¸Šæ˜¯åŸºäºSlientSliverçš„HP-ENVY13-ad1XX-Hackintoshä¿®æ”¹è€Œæ¥ï¼Œä¿ç•™äº†å…¶ä¸­çš„hotpatchéƒ¨åˆ†ï¼Œæ›´æ”¹äº†ä¸€äº›é©±åŠ¨å’Œè¡¥ä¸ã€‚ç‰¹æ­¤é¸£è°¢ï¼ å…³äºæœ¬æœºçš„åŠŸèƒ½ï¼š CPUï¼šå¯ä»¥æ­£å¸¸å˜é¢‘ ç”µæºï¼šèŠ‚èƒ½äº”é¡¹ä¼¼ä¹æ²¡æœ‰å®Œå…¨åŠ è½½ï¼Œä½†æ˜¯ç”µæ± ç”µé‡æ˜¾ç¤ºæ­£å¸¸ï¼Œä½¿ç”¨ä¸Šæ²¡æœ‰éšœç¢ æ˜¾å¡ï¼šä»¿å†’çš„Intel HD Graphics 520ï¼Œig-platform-idä¸º0x19160000ï¼Œé©±åŠ¨åŸç”Ÿæ˜¾å¡Intel HD Graphics 620ä¼šäº§ç”Ÿéå¸¸è¯¡å¼‚çš„è‰²é˜¶æ–­å±‚ï¼Œä¸¥é‡å½±å“è§‚æ„Ÿ ç¡çœ ï¼šæ­£å¸¸ï¼Œä»¥å‰æ›¾æœ‰è¿‡ç¡çœ å”¤é†’æ‰è“ç‰™çš„é—®é¢˜ï¼Œç°åœ¨å·²ç»è§£å†³ å£°éŸ³ï¼šä½¿ç”¨çš„LayoutIDä¸º03ï¼Œåªèƒ½é©±åŠ¨åº•é¢çš„æ‰¬å£°å™¨ï¼Œå¯¹äºè¿™æ¬¾ç¬”è®°æœ¬ç”µè„‘æ¥è¯´ï¼Œä¸¤ä¸ªæ‰¬å£°å™¨å’Œå››ä¸ªæ‰¬å£°å™¨å¬èµ·æ¥å¹¶æ— ä»€ä¹ˆå·®åˆ«ï¼Œå¯¹éŸ³è´¨æœ‰è¿½æ±‚çš„è¯·ç›´æ¥å¤–æ¥è“ç‰™éŸ³å“æˆ–è€…ä½¿ç”¨è€³æœºï¼Œæ’å…¥è€³æœºåéŸ³é‡å¯ä»¥è‡ªåŠ¨è°ƒèŠ‚ä¸ºä¹‹å‰çš„è®¾ç½®å€¼ ç½‘å¡å’Œè“ç‰™ï¼šåŸé…ç½‘å¡æ— æ³•ä½¿ç”¨ï¼Œæˆ‘æ›´æ¢ä¸ºDW1560ï¼Œæ²¡æœ‰æ•…éšœå‡ºç°ï¼ŒAirdropï¼ŒHandOffï¼ŒSidecaréƒ½å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼Œå¯ä»¥è¿æ¥AirPodså¬éŸ³ä¹å¹¶ä¸”åŠŸèƒ½å®Œæ•´ è§¦æ§æ¿ï¼šåŠ è½½äº†ç™½è‹¹æœæ‰‹åŠ¿ï¼Œä½†é™¤äº†å››æŒ‡æ‰‹åŠ¿å’ŒåŠ›åº¦æ„Ÿåº”ä¹‹å¤–å…¶ä»–æ‰‹åŠ¿éƒ½å¯ä»¥ç”¨ äº®åº¦è°ƒèŠ‚ï¼šå¯è°ƒï¼Œä½†æ˜¯æ¡£ä½é—´éš”ä¸å¤§ï¼Œæœ€ä½æ¡£ä½çš„æ—¶å€™å±å¹•è¿˜æ˜¯è¾ƒäº® USBæ¥å£ï¼šå››ä¸ªæ¥å£å‡å¯æ­£å¸¸ä½¿ç”¨ æ‘„åƒå¤´ï¼šå¯ç”¨ è¯»å¡å™¨ï¼šæ— æ³•é©±åŠ¨ï¼Œæœ‰éœ€è¦çš„å»ºè®®ä½¿ç”¨è¯»å¡å™¨ å£°æ˜ï¼šä»“åº“ä¸­æ‰€æœ‰æ–‡ä»¶å‡å¯ä¾›ä¸ªäººç”¨é€”å’ŒæŠ€æœ¯äº¤æµä½¿ç”¨ï¼Œåœ¨è½¬è½½æ—¶è¯·åŠ¡å¿…æ ‡æ˜å‡ºå¤„ã€‚ä¸å¾—å°†æ­¤ä»“åº“ä¸­çš„ä»»ä½•æ–‡ä»¶ç”¨äºä»»ä½•å•†ä¸šæ´»åŠ¨ï¼ åŸºæœ¬å®‰è£…è¿‡ç¨‹ä¸­çš„ä¸€äº›é—®é¢˜è¿™éƒ¨åˆ†ä¸æ˜¯ä¸»è¦å†…å®¹ï¼Œä½†è¿˜æ˜¯è®²ä¸¤å¥å§ã€‚ è¿›å…¥ä¸äº†å®‰è£…ç•Œé¢ï¼š é¦–å…ˆè¯·ç¡®è®¤ä½ å®‰è£…é•œåƒä¸­çš„EFIæ˜¯é€‚ç”¨äºä½ çš„ç”µè„‘å‹å·çš„ã€‚å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œè¯·åœ¨Cloverä¸­çš„Optioné€‰é¡¹ä¸­é€‰æ‹©-vä»¥å•°å—¦æ¨¡å¼å¯åŠ¨ï¼Œè¿™æ ·å¯åŠ¨çš„æ—¶å€™ä¼šæ˜¾ç¤ºå‡ºè¯¦ç»†çš„ä¿¡æ¯ã€‚å°†æœ€åå‡ºç°çš„æŠ¥é”™ä¿¡æ¯æ‹ä¸‹æ¥æˆ–è€…æ•´ä¸ªå¯åŠ¨è¿‡ç¨‹å½•åˆ¶ä¸‹æ¥ä»¥åï¼Œæ‰¾ç½‘å‹æ±‚åŠ©å§ã€‚ å®‰è£…macOS 10.15çš„è¿‡ç¨‹ä¸­ï¼Œåœ¨å•°å—¦æ¨¡å¼ä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºæŠ¥é”™ï¼š â€‹ è¯·åœ¨Cloverä¸­æ‰“ä¸Šå¦‚å›¾æ‰€ç¤ºçš„è¿™ä¸ªè¡¥ä¸ã€‚ è¿›å…¥å®‰è£…ç•Œé¢ä¸”å¼€å§‹å®‰è£…ä¸€æ®µæ—¶é—´åï¼Œæ— æ³•ç»§ç»­å®‰è£…ï¼š è¯·é‡æ–°ä¸‹è½½é•œåƒï¼Œåœ¨ä¸‹è½½å®Œæˆä»¥åæ£€æŸ¥é•œåƒçš„md5å€¼æ˜¯å¦æ­£ç¡®ã€‚å¦‚æ­£ç¡®ï¼Œå†åˆ¶ä½œä½ çš„é•œåƒUç›˜ã€‚ å¯¹äº10.14.xçš„é•œåƒè¿›å…¥å®‰è£…ç•Œé¢åæç¤ºåº”ç”¨å·²ç»æŸåï¼Œæ— æ³•å®‰è£…ï¼š è¯·å°†ä½ çš„biosæ—¶é—´å¾€å‰è°ƒæ•´è‡³2019å¹´10æœˆ25æ—¥ä»¥å‰ï¼Œä½†æ˜¯ä¸è¦è°ƒæ•´å¾—å¤ªä¹…è¿œã€‚è¿™æ˜¯å› ä¸ºæ—§çš„é•œåƒä¸­çš„è¯ä¹¦ä¼šåœ¨ä¸Šè¿°æ—¶é—´ä»¥åè¿‡æœŸå¯¼è‡´æ— æ³•å®‰è£…ã€‚ åç»­å®Œå–„ä¸­çš„ä¸€äº›é—®é¢˜åœ¨å®‰è£…å®Œæˆä»¥åï¼Œä¾¿å¯ä»¥è¿›å…¥ç³»ç»Ÿäº†ã€‚ä½†æ˜¯è¿™ä¸ªæ—¶å€™çš„ç³»ç»Ÿè¿˜æ˜¯éå¸¸ä¸å®Œå–„çš„ï¼Œéœ€è¦åšå¾ˆå¤šè°ƒæ•´ã€‚è¿›å…¥ç³»ç»Ÿåï¼Œå…ˆåœ¨ å…³äºæœ¬æœº-ç³»ç»ŸæŠ¥å‘Šä¸­æ£€æŸ¥å„ä¸ªç¡¬ä»¶é¡¹ç›®æ˜¯å¦è¢«æˆåŠŸé©±åŠ¨ï¼Œç„¶åå†æ ¹æ®æ²¡æœ‰æˆåŠŸé©±åŠ¨çš„é¡¹ç›®ï¼Œå®‰è£…ç›¸å¯¹åº”çš„é©±åŠ¨æˆ–è€…æ‰“å¿…è¦çš„è¡¥ä¸ã€‚ä½†æ˜¯å‰æ–‡è¯´è¿‡ï¼šå¦‚æœä½ çš„æœºå‹å’Œç¡¬ä»¶ä¸æˆ‘çš„ç›¸åŒä¸”ä½¿ç”¨äº†æˆ‘æä¾›çš„EFIçš„è¯ï¼ŒåŸºæœ¬å®‰è£…å®Œæˆä»¥åæœºå™¨å°±å·²ç»æ˜¯å‡ ä¹å®Œç¾çš„ä¸€ä¸ªçŠ¶æ€äº†ï¼Œåªéœ€è¦åšå¾ˆå°‘çš„ä¼˜åŒ–å³å¯ã€‚ å¦‚æœä½¿ç”¨çš„æ˜¯ä¸ä½œè€…ç›¸åŒå‹å·çš„ç”µè„‘ï¼ˆå‹å·å®Œå…¨ä¸€è‡´ï¼Œä¸”æœªæ›´æ¢è¿‡ä»»ä½•ç¡¬ä»¶ï¼‰ï¼Œä»¥ä¸‹é¡¹ç›®æ˜¯æœ‰æ•…éšœçš„ ç½‘å¡æœªè¢«é©±åŠ¨ï¼Œæ— æ³•ä¸Šç½‘ è“ç‰™æœªé©±åŠ¨ï¼Œæ— æ³•ä½¿ç”¨è“ç‰™ Siri, iMessage, FaceTime, HandOffæ— æ³•ä½¿ç”¨ ä»¥ä¸‹é¡¹ç›®æœ‰å¯èƒ½å‡ºç°æ•…éšœï¼š å£°å¡æœªé©±åŠ¨ï¼Œæ²¡æœ‰å£°éŸ³ï¼Œä¹Ÿæ— æ³•å½•éŸ³ æ— æ³•è°ƒèŠ‚æ˜¾ç¤ºå™¨äº®åº¦ï¼Œåœ¨ç³»ç»Ÿåå¥½è®¾ç½®ä¸­ä¹Ÿæ²¡æœ‰è°ƒèŠ‚äº®åº¦çš„æ‹–åŠ¨æ¡ è§¦æ§æ¿æœªè¢«é©±åŠ¨ï¼Œæ— æ³•ä½¿ç”¨è§¦æ§æ¿ å› æ­¤ï¼Œä»…ä»…å®Œæˆäº†ç³»ç»Ÿçš„å®‰è£…æ˜¯è¿œè¿œä¸å¤Ÿçš„ã€‚æ­¤æ—¶æˆ‘ä»¬çš„ç”µè„‘è¿˜æ— æ³•è¢«ç§°ä¸ºç”Ÿäº§åŠ›å·¥å…·ã€‚ä¸‹é¢å°±ä»‹ç»ä¸€äº›è§£å†³æ•…éšœçš„åŠæ³•ä»¥åŠç³»ç»Ÿä¼˜åŒ–çš„åŠæ³•ã€‚ é¦–å…ˆåº”å½“è·å–è½¯ä»¶å®‰è£…æƒé™ï¼Œåªæœ‰åœ¨æ­¤ä»¥åä½ æ‰å¯ä»¥å®‰è£…éApp Storeä¸‹è½½çš„ï¼Œæˆ–è€…ç”±éå—ä¿¡ä»»çš„å¼€å‘è€…å¼€å‘çš„è½¯ä»¶ï¼š åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼šsudo spctl --master-disable å»ºè®®å®‰è£…çš„è½¯ä»¶ï¼š Clover Configuratorï¼šç”¨äºä¿®æ”¹Cloverçš„é…ç½®æ–‡ä»¶config.plist Hackintoolï¼šåŠŸèƒ½å¼ºå¤§çš„é»‘è‹¹æœé…ç½®å·¥å…· Kext Utilityï¼šç”¨äºé‡å»ºç¼“å­˜ CPU-Sï¼šç”¨äºæµ‹è¯•CPUå˜é¢‘æ¡£ä½ MaciASLï¼šç”¨äºä¿®æ”¹SSDT è¿™äº›è½¯ä»¶å¯ä»¥é€šè¿‡è¿™ä¸ªç™¾åº¦äº‘é“¾æ¥ä¸‹è½½ã€‚å¯†ç ï¼š57qfã€‚ æœºå‹é€‰æ‹©ï¼š ä½¿ç”¨Clover Configuratoræ‰“å¼€config.plistï¼Œç¡®ä¿åœ¨æœºå‹è®¾ç½®ä¸­é€‰æ‹©MacBook Pro 14,1ã€‚å…³äºæœºå‹çš„é€‰æ‹©ï¼ŒåŸåˆ™ä¸Šæ˜¯éœ€è¦å°†ä½ çš„ç”µè„‘çš„é›†æˆæ˜¾å¡çš„å‹å·ä¸æ‰€é€‰æœºå‹çš„é›†æˆæ˜¾å¡å‹å·å¯¹åº”èµ·æ¥çš„ï¼Œå¦åˆ™æ— æ³•é©±åŠ¨ä½ çš„æ˜¾å¡ã€‚å…·ä½“çš„é€‰æ‹©å‚è§ï¼šé»‘è‹¹æœå¿…å¤‡ï¼šIntelæ ¸æ˜¾platform IDæ•´ç†åŠsmbiosé€ŸæŸ¥è¡¨ã€‚ é©±åŠ¨çš„æ­£ç¡®å®‰è£…æ–¹æ³•ï¼š å¦‚æœé©±åŠ¨æ²¡æœ‰æ­£ç¡®å®‰è£…ï¼Œæœ‰æå¤§çš„å¯èƒ½æ€§ä¼šå¯¼è‡´é‡å¯ä¹‹åæ— æ³•è¿›å…¥ç³»ç»Ÿã€‚ä½œè€…æœ¬äººå°±åœ¨è¿™ä¸ªé—®é¢˜ä¸Šåƒäº†å¾ˆå¤§çš„äºã€‚å…³äºé©±åŠ¨çš„å®‰è£…ï¼Œåˆ†ä¸ºä¸¤ç§æƒ…å†µã€‚ æ“ä½œçš„æ˜¯/EFI/CLOVER/kexts/Otherä¸­çš„é©±åŠ¨æ–‡ä»¶ã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œä¸éœ€è¦é‡å»ºç¼“å­˜ã€‚ æ“ä½œçš„æ˜¯/Library/Extensionsæˆ–è€…/System/Library/Extensionsä¸­çš„é©±åŠ¨æ–‡ä»¶ã€‚å¦‚æœæ“ä½œçš„æ˜¯è¿™ä¸ªä¸¤ä¸ªæ–‡ä»¶å¤¹ä¸­çš„é©±åŠ¨æ–‡ä»¶ï¼Œåˆ™éœ€è¦é‡å»ºç¼“å­˜ã€‚å¯ä»¥é€šè¿‡Kext Utilityè½¯ä»¶æˆ–è€…ä½¿ç”¨ç»ˆç«¯å‘½ä»¤è¡Œæ¥é‡å»ºç¼“å­˜ã€‚ é‡å»ºç¼“å­˜çš„å‘½ä»¤ï¼šsudo kextcache -i /ã€‚ å…³äºç½‘ç»œï¼š å¯¹äºä½¿ç”¨å®‰è£…äº†Intelï¼ˆæˆ–è€…å…¶ä»–æŸäº›å“ç‰Œï¼‰çš„ç½‘å¡çš„ç”µè„‘çš„æœ‹å‹ä»¬ï¼Œè¿›å…¥é»‘è‹¹æœç³»ç»Ÿä»¥åç½‘å¡æ˜¯æ²¡æœ‰é©±åŠ¨çš„ï¼Œä¹Ÿå°±æ˜¯è¯´è¿™ä¸ªæ—¶å€™ç”µè„‘æ˜¯æ²¡æœ‰åŠæ³•ä¸Šç½‘çš„ã€‚è‹¥æ˜¯ç”µè„‘å®‰è£…äº†æŸäº›å‹å·çš„å…é©±ç½‘å¡ï¼Œåœ¨macOSç³»ç»Ÿä¸‹ç”µè„‘å°±å¯ä»¥ç›´æ¥è¿æ¥ç½‘ç»œã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœä¸æƒ³æ‹†æœºï¼Œå¯ä»¥ä½¿ç”¨USBç½‘å¡ã€‚ä½†æ˜¯ä½¿ç”¨USBç½‘å¡æ— æ³•ä½¿ç”¨Siri, iMessage, FaceTime, HandOffç­‰åŠŸèƒ½ã€‚ å¯¹äºIntelçš„ç½‘å¡ï¼Œç›®å‰åœ¨macOSä¸‹æ˜¯æ²¡æœ‰å¾ˆå¥½çš„åŠæ³•é©±åŠ¨çš„ã€‚ä½†æ˜¯æƒ…å†µä¹Ÿåœ¨å‘ç”Ÿç€ä¸€äº›æ”¹å˜ã€‚æœ€è¿‘è¿œæ™¯è®ºå›å·²ç»æœ‰å¤§ä½¬å†™å‡ºäº†Intelç½‘å¡çš„é©±åŠ¨ï¼Œä½†æ˜¯è¿˜æ˜¯å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚æœ‰å…´è¶£çš„å¯ä»¥çœ‹çœ‹ä»–çš„GitHubé¡¹ç›®é‡Œé¢æœ‰æ²¡æœ‰æ”¯æŒä½ çš„ç½‘å¡çš„å‹å·ï¼šIntelBluetoothFirmwareã€‚ å¯¹äºç½‘ç»œçš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨USBç½‘å¡ã€‚æˆ–è€…ç›´æ¥å°†ç”µè„‘çš„ç½‘å¡æ‹†ä¸‹å¹¶æ›´æ¢ä¸ºå¯ä»¥ä½¿ç”¨çš„å…é©±ç½‘å¡ã€‚å…³äºå…é©±ç½‘å¡å‹å·çš„é€‰æ‹©ï¼Œå¯ä»¥å‚è€ƒè¿™ä¸ªç½‘ç«™ï¼šé»‘è‹¹æœå»ºè®®çš„æ— çº¿ç½‘å¡ Hackintosh Compatible WiFi(20190505å¢åŠ æ— çº¿è·¯ç”±å™¨æ¨è)ã€‚ å½“å®‰è£…äº†åˆé€‚çš„ç½‘å¡ä»¥åï¼Œç”µè„‘ä¾¿å¯ä»¥ä¸Šç½‘äº†ã€‚è¿™ä¸ªæ—¶å€™ï¼Œè¿™å°ç”µè„‘æ‰åŸºæœ¬å¯ä»¥æŠ•å…¥ä½¿ç”¨ã€‚ å…³äºBCM94352Z(DW1560)ï¼š ä½œè€…ä½¿ç”¨çš„å°±æ˜¯è¿™ç§æ— çº¿ç½‘å¡ã€‚è¿™ä¸ªç½‘å¡æ˜¯Wi-Fiå’Œè“ç‰™äºŒåˆä¸€æ— çº¿ç½‘å¡ã€‚è¯¥ç½‘å¡çš„æ— çº¿å±€åŸŸç½‘åŠŸèƒ½åœ¨macOSå’ŒWindowsç³»ç»Ÿä¸‹éƒ½æ˜¯å…é©±çš„ã€‚ä½†æ˜¯è¿™ä¸ªç½‘å¡åœ¨macOSä¸‹è¦é©±åŠ¨è“ç‰™éœ€è¦ä¸‰ä¸ªé©±åŠ¨æ–‡ä»¶ï¼Œåˆ†åˆ«ä¸ºï¼šAirportBrcmFixup.kextï¼ŒBrcmFirmwareData.kextï¼ŒBrcmPatchRAM3.kextã€‚å°†è¿™äº›é©±åŠ¨æ–‡ä»¶æ”¾å…¥/EFI/CLOVER/kexts/Otherä¸‹ã€‚æ³¨æ„ï¼Œè¯¥ç›®å½•ä¸‹è¿˜åº”å½“å­˜åœ¨Lilu.kextï¼Œå¦åˆ™é©±åŠ¨æ–‡ä»¶æ— æ³•æ­£å¸¸å·¥ä½œï¼ˆä»“åº“ä¸­æä¾›çš„EFIæ–‡ä»¶å¤¹ä¸­éƒ½å·²åŒ…å«è¿™äº›é©±åŠ¨æ–‡ä»¶äº†ï¼‰ã€‚ ä½œè€…çš„ç”µè„‘ä¸€åº¦å‡ºç°äº†ç”µè„‘ç¡çœ å”¤é†’åè“ç‰™å¤±æ•ˆçš„æƒ…å†µï¼Œå¹¶è¢«è¿™ä¸ªé—®é¢˜å›°æ‰°äº†å¾ˆä¹…ã€‚ä¸€å¼€å§‹æ˜¯å‚è€ƒäº†Broadcom BCM94352z/DW1560é©±åŠ¨æ–°å§¿åŠ¿[æ–°æ–¹æ³•]ä¸­çš„æ–¹æ³•ï¼Œä½†æ˜¯é—®é¢˜å¹¶æ²¡æœ‰å¾—åˆ°æ ¹æœ¬è§£å†³ã€‚ä¹‹ååœ¨/EFI/CLOVER/kexts/Otherä¸­åŠ å…¥äº†ACPIDebug.kextï¼Œå°†ç”µè„‘hibernatemodeçš„å€¼è°ƒæ•´ä¸º0ï¼Œå¹¶åœ¨è“ç‰™åå¥½è®¾ç½®-é«˜çº§é€‰é¡¹ä¸­å–æ¶ˆå‹¾é€‰å…è®¸è“ç‰™è®¾å¤‡å”¤é†’è¿™å°ç”µè„‘åï¼Œä¹Ÿæ²¡æœ‰è§£å†³è¯¥é—®é¢˜ã€‚ç„¶åä½œè€…å°è¯•é‡æ–°è®¢åˆ¶USBé©±åŠ¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯è¿˜æ˜¯æ²¡æœ‰èƒ½å¤Ÿè§£å†³è¿™ä¸ªé—®é¢˜ã€‚ æœ€åï¼Œä½œè€…æ›´æ¢äº†æœ€æ–°çš„è“ç‰™é©±åŠ¨ï¼Œæ‰æœ€ç»ˆå®Œç¾è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæœ‰æ—¶åœ¨ç¡çœ å”¤é†’ä¹‹åï¼Œè“ç‰™å›¾æ ‡ä¼šçŸ­æš‚çš„æ˜¾ç¤ºä¸ºå¤±æ•ˆçŠ¶æ€ï¼Œç„¶åå›å¤æ­£å¸¸ã€‚ åœ¨Windowsç³»ç»Ÿä¸‹ï¼Œå¯ä»¥è‡ªè¡Œå®‰è£…é©±åŠ¨äººç”Ÿè½¯ä»¶æ¥å®‰è£…è“ç‰™çš„é©±åŠ¨ã€‚ ç›®å‰å¸‚é¢ä¸ŠDW1560çš„ä»·æ ¼åœ¨300å…ƒå·¦å³ã€‚å®è¯è¯´ï¼Œè¿™ä¸ªä»·æ ¼å®Œå…¨æ˜¯å› ä¸ºé»‘è‹¹æœè¿™è¾¹çš„éœ€æ±‚ç‚’èµ·æ¥çš„ã€‚è€ŒåŒæ—¶ç¤¾åŒºä¸­ä¹Ÿæœ‰å…¶ä»–ç½‘å¡çš„è§£å†³æ–¹æ¡ˆï¼Œé™¤äº†ä¸Šæ–‡æ‰€æåˆ°è¿‡çš„é©±åŠ¨è¿˜å¼€å‘ä¸­çš„éƒ¨åˆ†Intelç½‘å¡ä¹‹å¤–ï¼ŒDW1820æ˜¯å¦ä¸€ä¸ªä»·æ ¼ç›¸å¯¹ä½å»‰çš„é€‰æ‹©ã€‚ä½†æ˜¯æ ¹æ®ç¤¾åŒºä¸­çš„åé¦ˆï¼ŒDW1820çš„è¡¨ç°å¹¶ä¸æ˜¯ç‰¹åˆ«ç¨³å®šï¼Œæœ‰å¯èƒ½ä¼šå‡ºç°å„ç§å¥‡æ€ªçš„é—®é¢˜ã€‚å› æ­¤ï¼Œä½œè€…å»ºè®®è¿˜æ˜¯ç›´æ¥è´­ä¹°DW1560æ¯”è¾ƒå¥½ï¼Œä¸€æ­¥åˆ°ä½ï¼Œçœäº†å„ç§æŠ˜è…¾å’Œé—¹å¿ƒã€‚å¦å¤–ï¼Œä½ ä¹Ÿå¯ä»¥è´­ä¹°Macä¸Šçš„æ‹†æœºç½‘å¡æˆ–è€…DW1830ï¼Œåè€…çš„ä»·æ ¼åœ¨500å…ƒå·¦å³ï¼Œé€Ÿåº¦æ¯”DW1560æ›´å¿«ã€‚ å…³äºç¡çœ ï¼š è¯·æ‰“å¼€Hackintoolè½¯ä»¶ï¼Œå¹¶åˆ‡æ¢åˆ°ç”µæºä¸€æ ã€‚å†ç‚¹å‡»çº¢æ¡†ä¸­çš„æŒ‰é’®ï¼Œä½¿å¾—ç”µæºä¿¡æ¯ä¸­çº¢è‰²çš„ä¸¤è¡Œå˜ä¸ºç»¿è‰²ã€‚æ­¤æ“ä½œå¯èƒ½å¯ä»¥è§£å†³ä¸€äº›ç¡çœ é—®é¢˜ã€‚ å®šåˆ¶USBé©±åŠ¨ï¼š å®šåˆ¶USBé©±åŠ¨æœ‰å¯èƒ½å¯ä»¥å¸®åŠ©è§£å†³ä¸€äº›ç¡çœ ä¸Šçš„é—®é¢˜ï¼Œå…¶æ“ä½œæ­¥éª¤ä¹Ÿååˆ†ç®€å•ï¼Œæ‰€ä»¥åšä¸»å¼ºçƒˆæ¨èå¤§å®¶è¿˜æ˜¯å®šåˆ¶ä¸€ä¸‹ã€‚åœ¨æ­¤å¤„é™„ä¸Šè®¢åˆ¶USBé©±åŠ¨çš„æ•™ç¨‹ï¼šHackintool(Intel FB Patcher) USBå®šåˆ¶è§†é¢‘ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½ æœ‰å¯èƒ½å‘ç°åœ¨ä½¿ç”¨äº†USBInjectALL.kextä»¥åä»æœ‰ç«¯å£æ— æ³•åŠ è½½/æ£€æµ‹ä¸åˆ°ã€‚ä½ å¯ä»¥å°è¯•åœ¨Cloverçš„config.plistä¸­æ·»åŠ ä¸‹åˆ—è§£é™¤USBç«¯å£æ•°é‡é™åˆ¶è¡¥ä¸æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ 123456789Comment: USB port limit patch #1 10.15.x modify by DalianSky(credit ydeng)Name: com.apple.iokit.IOUSBHostFamilyFind: 83FB0F0FReplace: 83FB3F0FComment: USB Port limit patch #2 10.15.x modify by DalianSkyName: com.apple.driver.usb.AppleUSBXHCIFind: 83F90F0FReplace: 83F93F0F å¼€å¯HiDPIä½¿å±å¹•çœ‹èµ·æ¥æ¸…æ™°ï¼š åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼šsh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/xzhih/one-key-hidpi/master/hidpi-zh.sh)&quot;ï¼Œå†æŒ‰æç¤ºæ“ä½œå³å¯ã€‚ è¯¦æƒ…è¯·è§ï¼šHiDPIæ˜¯ä»€ä¹ˆï¼Ÿä»¥åŠé»‘è‹¹æœå¦‚ä½•å¼€å¯HiDPIã€‚ æ‰“å¼€SSD Trimï¼š åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼šsudo trimforce enableï¼Œç„¶åè¾“å…¥yå†å›è½¦ï¼Œé‡å¤ä¸€æ¬¡ï¼Œç”µè„‘å°†è‡ªåŠ¨é‡å¯ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨åŸè£…SSDçš„æœ‹å‹è¯·ä¸è¦æ‰“å¼€è¿™ä¸ªåŠŸèƒ½ï¼Œè¿™ä¼šå¯¼è‡´ä½ çš„ç”µè„‘åœ¨macOSä¸‹éå¸¸å¡é¡¿ï¼Œå‡ ä¹æ— æ³•æ“ä½œã€‚ ç”µè„‘å¡é¡¿çš„è§£å†³åŠæ³•ï¼š åœ¨åˆšå®‰è£…å®Œé»‘è‹¹æœåï¼Œç³»ç»Ÿå¤§æ¦‚ç‡ä¼šå‡ºç°æä¸ºå¡é¡¿çš„æƒ…å†µã€‚è¿™ç§å¡é¡¿ä¸»è¦è¡¨ç°åœ¨ï¼šé¼ æ ‡ç§»åŠ¨å¡é¡¿ã€åŠ¨ç”»ä¸¥é‡æ‰å¸§ã€å¼€æœºé€Ÿåº¦ä»¥åŠåº”ç”¨æ‰“å¼€é€Ÿåº¦å¾ˆæ…¢ã€ç³»ç»Ÿèµ„æºå¤§é‡å ç”¨ã€ç”µè„‘å‘çƒ­ä¸¥é‡ã€æ— æ³•æ­£å¸¸å…³æœºã€‚è¿™äº›é—®é¢˜æœ‰çš„æ—¶å€™ä¸å¤ªæ˜æ˜¾ï¼Œæœ‰çš„æ—¶å€™åˆ™ä»¤ç”µè„‘æ ¹æœ¬æ— æ³•ä½¿ç”¨ã€‚ä¸Šè¿°é—®é¢˜æœ‰æ—¶åœ¨è®©ç”µè„‘ç¡çœ ä¸€æ®µæ—¶é—´ä¹‹åé‡æ–°å”¤é†’å³å¯å¾—åˆ°æ”¹å–„ï¼Œä½†æ˜¯æ— æ³•æ ¹æœ¬è§£å†³ã€‚ å‡ºç°ä¸Šè¿°é—®é¢˜çš„æ ¹æœ¬åŸå› å°±åœ¨äºæœ¬å‹å·ç”µè„‘æ‰€ä½¿ç”¨çš„SSDâ€”â€”Intel SSDPEKKF360G7Hå¯¹macOSçš„å…¼å®¹å¹¶ä¸å¥½ã€‚è‹¥è¦æ­£å¸¸ä½¿ç”¨è¯¥SSDçš„è¯å¿…é¡»åœ¨/EFI/CLOVER/kexts/Otherä¸­æ·»åŠ HackrNVMeFamily.kextã€‚ä½ å¯ä»¥åœ¨GitHubä»“åº“æ–‡ä»¶ä¸»ç›®å½•ä¸‹çš„kextæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°è¿™ä¸ªé©±åŠ¨ã€‚åœ¨æ·»åŠ äº†è¿™ä¸ªé©±åŠ¨ä¹‹åï¼Œç³»ç»Ÿçš„å¡é¡¿ç°è±¡å¯ä»¥å¾—åˆ°éå¸¸æ˜æ˜¾çš„æ”¹å–„ï¼ŒåŸºæœ¬ä¸Šåšåˆ°äº†æµç•…è¿è¡Œï¼Œä½†æ˜¯å¶å°”è¿˜æ˜¯ä¼šæœ‰äº›è®¸å¡é¡¿ã€‚ è§£å†³è¿™ä¸ªé—®é¢˜æœ€æ ¹æœ¬çš„æ–¹æ³•è¿˜æ˜¯æ›´æ¢SSDã€‚ä½œè€…çš„SSDå·²ç»æ›´æ¢ä¸ºè¥¿éƒ¨æ•°æ®çš„SN500ï¼Œæ•…åœ¨EFIæ–‡ä»¶å¤¹ä¸­åˆ é™¤äº†è¿™ä¸ªé©±åŠ¨æ–‡ä»¶ã€‚ ç”µè„‘æ— æ³•è°ƒèŠ‚å±å¹•äº®åº¦çš„è§£å†³åŠæ³•ï¼š ä¸€èˆ¬æƒ…å†µä¸‹ä¸ä¼šå‡ºç°è¿™æ ·çš„æƒ…å†µï¼Œä½†æ˜¯å¦‚æœå‘ç”Ÿäº†ï¼Œä½¿ç”¨Kext Utilityé‡å»ºç¼“å­˜åé‡å¯å³å¯ã€‚ å…³äºæœ¬æœºçš„VoodooPS2Controller.kextï¼š åœ¨æ›´æ¢äº†EFIçš„hotpatchæ–¹æ³•ä»¥åï¼Œæœ€æ–°ç‰ˆæœ¬çš„VoodooPS2Controller.kextå·²ç»å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚æ³¨æ„ï¼Œæ–°ç‰ˆæœ¬çš„VoodooPS2Controller.kextéœ€è¦é…åˆVoodooInput.kextä½¿ç”¨ã€‚ä¸‹é¢æ‰€è¯´çš„å®šåˆ¶VoodooPS2Controller.kextçš„å†…å®¹å·²ç»è¿‡æ—¶ï¼Œä½†æ­¤å¤„ä»åŠ ä»¥ä¿ç•™ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½æŒ‰éœ€ä½¿ç”¨ã€‚ æ—§ç‰ˆæœ¬çš„VoodooPS2Controller.kextå­˜æ”¾äºGitHubä»“åº“æ–‡ä»¶ä¸»ç›®å½•ä¸‹çš„kextæ–‡ä»¶å¤¹ä¸­ï¼Œå®ƒåŒæŒ‡æ‰‹åŠ¿åªæ”¯æŒä¸Šä¸‹å·¦å³æ»‘åŠ¨ï¼Œä¸‰æŒ‡æ‰‹åŠ¿åœ¨ä¿®æ”¹åå®ç°äº†ä¸‹è¡¨æ‰€è¿°åŠŸèƒ½ã€‚å®ƒä¸æ–°ç‰ˆé©±åŠ¨ç›¸æ¯”ï¼Œä¼˜ç‚¹åœ¨äºï¼šååˆ†ç¨³å®šï¼Œä¸‰æŒ‡æ‰‹åŠ¿çš„è¯†åˆ«æˆåŠŸç‡å‡ ä¹è¾¾åˆ°100%ï¼Œå¹¶ä¸”åŒæŒ‡è½»è§¦ååˆ†çµæ•ã€‚ ä¸ºè¿åˆmacOSè°ƒåº¦ä¸­å¿ƒé»˜è®¤çš„é”®ä½ï¼Œæˆ‘å°†è¯¥é©±åŠ¨çš„ä¸‰åªæ»‘åŠ¨æ‰‹åŠ¿çš„é”®ç›˜æ˜ å°„ä½œäº†äº›è®¸è°ƒæ•´ï¼Œå…¶å¯¹åº”å…³ç³»å¦‚ä¸‹è¡¨ï¼š æ‰‹åŠ¿ åŸæœ¬å¯¹åº”çš„å¿«æ·é”® ä¿®æ”¹åçš„å¿«æ·é”® åŠŸèƒ½ ä¸‰æŒ‡ä¸Šæ»‘ âŒ˜+Ë†+â†‘ Ë†+â†‘ è°ƒåº¦ä¸­å¿ƒ ä¸‰æŒ‡ä¸‹æ»‘ âŒ˜+Ë†+â†“ Ë†+â†“ App ExposÃ© ä¸‰æŒ‡å·¦æ»‘ âŒ˜+Ë†+â† Ë†+â†’ å‘å³åˆ‡æ¢ä¸€ä¸ªå…¨å±é¡µé¢ ä¸‰æŒ‡å³æ»‘ âŒ˜+Ë†+â†’ Ë†+â† å‘å·¦åˆ‡æ¢ä¸€ä¸ªå…¨å±é¡µé¢ è§¦æ§æ¿æ²¡æœ‰ååº”çš„æƒ…å†µï¼š ä¸€å¼€å§‹æˆ‘ä»¥ä¸ºæ˜¯ç›¸å…³é©±åŠ¨æ²¡æœ‰æˆåŠŸåŠ è½½çš„ç¼˜æ•…ï¼Œä½†æ˜¯åæ¥å‘ç°è¿™æ˜¯å› ä¸ºè§¦æ§æ¿è¢«è¯¯é”å®šäº†ã€‚æŒ‰ä¸‹ç”µè„‘é”®ç›˜å³ä¸Šè§’çš„prt scé”®å¯ä»¥é”å®š/è§£é”è§¦æ§æ¿ã€‚ å…³äºCPUFriend.kextï¼š è¯¥é©±åŠ¨æ–‡ä»¶ç”¨äºå®ç°CPUçš„å˜é¢‘åŠŸèƒ½ã€‚ç”±äºè¯¥é©±åŠ¨ç¨‹åºåªèƒ½æ ¹æ®ç”¨æˆ·ä¸ªäººçš„ç”µè„‘å®šåˆ¶ï¼Œæ‰€ä»¥è¯·ä¸è¦ç›´æ¥ä½¿ç”¨ä»“åº“EFiæ–‡ä»¶å¤¹ä¸­æ‰€æä¾›çš„é©±åŠ¨æ–‡ä»¶ã€‚å…·ä½“å®‰è£…æ–¹æ³•å‚è§ï¼šåˆ©ç”¨CPUFriend.kextå®ç°å˜é¢‘ã€‚ å®‰è£…å®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨CPU-Sæ¥æ£€æµ‹è‡ªå·±ç”µè„‘çš„å˜é¢‘æ¡£ä½ã€‚ æ‰“å¼€åŸç”Ÿçš„NTFSè¯»å†™åŠŸèƒ½ï¼š è¯¥æ“ä½œæœ‰ä¸€å®šé£é™©ï¼Œæ˜¯å¦éœ€è¦å¼€å¯è¯·è‡ªè¡Œåˆ¤æ–­ã€‚ åœ¨macOSçš„é»˜è®¤çŠ¶æ€ä¸‹ï¼ŒNTFSæ ¼å¼çš„ç£ç›˜æ˜¯åªèƒ½è¯»ä¸èƒ½å†™çš„ã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥å°†éšè—çš„åŠŸèƒ½æ‰“å¼€ï¼Œä»è€Œå¯ä»¥å¯¹è¯¥æ ¼å¼çš„ç£ç›˜è¿›è¡Œå†™æ“ä½œï¼Œè¯¦æƒ…å‚è€ƒè¿™ä¸ªé“¾æ¥ï¼šmacOSæ‰“å¼€åŸç”Ÿçš„NTFSè¯»å†™åŠŸèƒ½ã€‚ å¦‚æœä½ å¯¹NTFSæ ¼å¼çš„ç£ç›˜è¯»å†™åŠŸèƒ½æœ‰åˆšéœ€ï¼Œä¹Ÿæœ‰å¾ˆå¤šç›¸å…³çš„è½¯ä»¶å¯ä¾›é€‰æ‹©ã€‚æ­¤å¤„ç•¥å»ä¸è¡¨ã€‚ ä¿®å¤Windowså’ŒmacOSä¸‹æ—¶é’Ÿä¸åŒæ­¥çš„é—®é¢˜ï¼š å¯¹äºå®‰è£…äº†åŒç³»ç»Ÿçš„ç”µè„‘ï¼Œåœ¨ä»macOSåˆ‡æ¢å›Windowsä¹‹åä¼šå‘ç°Windowsçš„ç³»ç»Ÿæ—¶é—´ä¸å½“å‰æ—¶é—´ä¸ç¬¦ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„åŠæ³•æ˜¯ï¼šåœ¨Windowsä¸‹ï¼Œæ‰“å¼€CMDè¾“å…¥ä¸‹é¢çš„å‘½ä»¤åå›è½¦ã€‚ Reg add HKLM\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation /v RealTimeIsUniversal /t REG_DWORD /d 1ã€‚ å…³äºæ˜¾å¡platform-idçš„é€‰æ‹©ï¼š æœ¬æœºçš„æ˜¾å¡å°±æ˜¯Intel HD Graphics 620ï¼Œæ˜¯å±äº7ä»£Kaby Lakeå¹³å°çš„ï¼Œå…¶platform-idä¸º0x5916000ï¼Œå¯¹åº”æœºå‹ä¸ºMacbookPro 14,2ã€‚ä½†æ˜¯ç»è¿‡æœ¬äººå®è·µå‘ç°ï¼Œå¦‚æœæ³¨å…¥çš„æ˜¯HD 620çš„idï¼Œç³»ç»Ÿæ˜¾ç¤ºå™¨è¾“å‡ºçš„å¸§ç¼“å†²æ·±åº¦(Framebuffer depth)ä¸ºè¯¡å¼‚çš„30ä½ï¼Œè¿™å¯¹åº”çš„æ˜¯10ä½çš„æ˜¾ç¤ºå™¨ã€‚ç”±äºç”µè„‘æ˜¾ç¤ºå™¨æœ¬èº«ä¸º8ä½çš„ï¼Œå› æ­¤10ä½çš„é¢œè‰²è¾“å‡ºä¼šå¯¼è‡´é«˜æ–¯æ¨¡ç³Šå’ŒåŠé€æ˜çš„ç”»é¢å‡ºç°ä¸¥é‡çš„è‰²é˜¶æ–­å±‚ï¼ˆè‰²å¸¦ï¼‰ã€‚ä¸€å¼€å§‹æˆ‘ä»¥ä¸ºæ˜¯æ˜¾ç¤ºå™¨EDIDä¸åŒ¹é…çš„é—®é¢˜ï¼Œä½†æ˜¯ç»è¿‡æœç´¢å‘ç°ï¼Œåœ¨Kaby Lakeå¹³å°ä¸Šï¼Œè¿™ä¸ªé—®é¢˜æ˜¯å› ä¸ºæ˜¾å¡platform-idé€‰æ‹©å¾—ä¸å¯¹ï¼Œåº”è¯¥æ˜¯éœ€è¦ä»¿å†’6ä»£Sky Lakeå¹³å°çš„Intel HD Graphics 520æ‰å¯ä»¥å¾—åˆ°æ­£ç¡®çš„24ä½çš„å¸§ç¼“å†²æ·±åº¦è¾“å‡ºï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ å…³äºè¿™ä¸ªé—®é¢˜çš„å…·ä½“å†…å®¹å’Œè§£å†³æ–¹æ³•å¯ä»¥å‚çœ‹è¿™ä¸ªç½‘é¡µã€‚ è‡³æ­¤ï¼Œé»‘è‹¹æœçš„å®‰è£…å’Œå®Œå–„å°±å·®ä¸å¤šç»“æŸäº†ã€‚ç°åœ¨å¯ä»¥ç™»é™†iCloudä»¥åŠå…¶ä»–è‹¹æœæœåŠ¡ï¼Œå¹¶å®‰è£…è‡ªå·±éœ€è¦çš„è½¯ä»¶äº†ã€‚ é™„ï¼šåšä¸»ç”µè„‘é…ç½® å‹å· HP Envy-13 ad024TU CPU Intel Core i7-7500U(2.7GHz) RAM 8GB DDR4 æ˜¾å¡ Intel HD Graphics 620 ç¡¬ç›˜ Intel SSDPEKKF360G7H 360G ï¼ˆå·²æ›´æ¢ä¸ºWD SN500ï¼‰ ç½‘å¡ Intel 7265NGWï¼ˆå·²æ›´æ¢ä¸ºDW1560ï¼‰ å£°å¡ ALC295","link":"/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/"},{"title":"Pythonå­¦ä¹ ç¬”è®°","text":"è¿™ç¯‡æ–‡ç« ä¸»è¦è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™ä¸ªè¯­è¨€çš„ä¸€äº›ç‰¹æ€§ï¼Œå†…å®¹ä»¥æ—¶é—´é¡ºåºæ•´ç†ï¼Œæ¯”è¾ƒé›¶æ•£æ‚ä¹±ã€‚å¯¹äºä»é›¶å¼€å§‹çš„åŒå­¦ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£Python 3.8.1 ä¸­æ–‡æ–‡æ¡£æˆ–å…¶ä»–ç½‘ç»œä¸Šçš„æ•™ç¨‹ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚ 19/9/14 æ³¨é‡Šæ–¹æ³•ï¼š#ï¼ˆä¸€è¡Œæ³¨é‡Šï¼‰ï¼Œâ€œâ€â€œ â€â€œâ€ï¼ˆå¤šè¡Œæ³¨é‡Šï¼‰ forå¾ªç¯ï¼šfor ï¼ˆå˜é‡ï¼‰ in ï¼ˆèŒƒå›´ï¼‰ï¼ŒèŒƒå›´å¯ä»¥ç”¨rangeå‡½æ•° Inputå‡½æ•°çš„è¾“å…¥æ˜¯charç±»å‹çš„ //æ˜¯æ•´é™¤è¿ç®— é€—å·ä¸å¯ä»¥ç”¨æ¥åˆ†éš”è¯­å¥ ä½¿ç”¨ç¼©è¿›ï¼ˆ4ä¸ªç©ºæ ¼ï¼‰æ¥ä»£æ›¿C/C++ä¸­çš„å¤§æ‹¬å· 19/9/15 for...inå¾ªç¯ä¸­ï¼Œ_å¯ä»¥ä½œä¸ºå¾ªç¯å˜é‡ï¼Œè¿™æ—¶å€™ä»…å¾ªç¯æŒ‡å®šæ¬¡æ•°ï¼Œè€Œä¸éœ€è¦å…³å¿ƒå¾ªç¯å˜é‡çš„å€¼ï¼›äº‹å®ä¸Šï¼Œ_æ˜¯ä¸€ä¸ªåˆæ³•çš„æ ‡è¯†ç¬¦ï¼Œå¦‚æœä¸å…³å¿ƒè¿™ä¸ªå˜é‡ï¼Œå°±å¯ä»¥å°†å…¶å®šä¹‰æˆè¿™ä¸ªå€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ªåƒåœ¾æ¡¶ å®šä¹‰å‡½æ•°æ—¶ï¼Œä½¿ç”¨å‡½æ•°å(*å‚æ•°å)çš„å®šä¹‰æ–¹å¼ï¼Œ * ä»£è¡¨å‡½æ•°çš„å‚æ•°æ˜¯å¯å˜å‚æ•°ï¼Œå¯ä»¥æœ‰0åˆ°å¤šä¸ªå‚æ•° ä¸€ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ¨¡å—(module)ï¼Œè‹¥åœ¨ä¸åŒçš„æ¨¡å—ä¸­å«æœ‰åŒåå‡½æ•°ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡importå¯¼å…¥æŒ‡å®šçš„æ¨¡å—ä¸­çš„å‡½æ•°ï¼Œå¦‚from æ¨¡å— import å‡½æ•°ï¼Œæˆ–è€…import æ¨¡å— as è‡ªå®šä¹‰æ¨¡å—åç§°ï¼Œå†é€šè¿‡è‡ªå®šä¹‰æ¨¡å—åç§°.å‡½æ•°çš„æ–¹å¼è°ƒç”¨ __name__æ˜¯Pythonä¸­ä¸€ä¸ªéšå«çš„å˜é‡ï¼Œä»£è¡¨äº†æ¨¡å—çš„åå­—ï¼Œåªç”¨ç›´æ¥æ‰§è¡Œçš„æ¨¡å—çš„åå­—æ‰æ˜¯__main__ å¯ä»¥ä½¿ç”¨globalæŒ‡å®šä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œå¦‚æœå…¨å±€å˜é‡ä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ï¼Œé‚£ä¹ˆä¼šå®šä¹‰ä¸€ä¸ªæ–°çš„å…¨å±€å˜é‡ åµŒå¥—ä½œç”¨åŸŸï¼šå¯¹äºå‡½æ•°aå†…éƒ¨çš„å‡½æ•°bè€Œè¨€ï¼Œaä¸­å®šä¹‰çš„å˜é‡å¯¹bæ¥è¯´æ˜¯åœ¨åµŒå¥—ä½œç”¨åŸŸä¸­çš„ï¼Œè‹¥è¦æŒ‡å®šä¿®æ”¹åµŒå¥—ä½œç”¨åŸŸä¸­çš„å˜é‡ï¼Œå¯ä»¥ä½¿ç”¨nonlocalæŒ‡ç¤ºå˜é‡æ¥è‡ªåµŒå¥—ä½œç”¨åŸŸ passæ˜¯ä¸€ä¸ªç©ºè¯­å¥ï¼Œåªèµ·åˆ°å ä½ä½œç”¨ å¯ä»¥å®šä¹‰ä¸€ä¸ªmainå‡½æ•°ï¼ˆæˆ–è€…ä¸æ¨¡å—åå­—ç›¸åŒçš„å‡½æ•°ï¼‰ï¼Œå†æŒ‰ç…§if __name__ = '__main__'çš„æ ¼å¼ä½¿è„šæœ¬æ‰§è¡Œ 19/9/17 ä¸å­—ç¬¦ä¸²æœ‰å…³çš„å‡½æ•°çš„è°ƒç”¨æ–¹å¼ä¸ºï¼šå­—ç¬¦ä¸²åç§°.å­—ç¬¦ä¸²æ“ä½œå‡½æ•°()ï¼Œåœ¨æ­¤æ—¶å­—ç¬¦ä¸²æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå­—ç¬¦ä¸²æ“ä½œå‡½æ•°çš„ä½œç”¨æ˜¯å‘å­—ç¬¦ä¸²å¯¹è±¡å‘é€ä¸€ä¸ªæ¶ˆæ¯ å­—ç¬¦ä¸²å®è´¨ä¸Šæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯ä»¥è¿›è¡Œä¸‹æ ‡è¿ç®— å­—ç¬¦ä¸²åˆ‡ç‰‡å¯ä»¥åœ¨ä¸‹æ ‡è¿ç®—ä¸­ä½¿ç”¨å†’å·è¿›è¡Œè¿ç®—ï¼Œ[èµ·å§‹å­—ç¬¦:ç»“æŸå­—ç¬¦:é—´éš”]ï¼Œè‹¥ä¸å®šä¹‰èµ·å§‹ä¸ç»ˆæ­¢å­—ç¬¦ï¼Œåˆ™é»˜è®¤ä¸ºæ•´ä¸ªå­—ç¬¦ä¸²ï¼Œå½“é—´éš”ä¸ºè´Ÿå€¼æ—¶ï¼Œä»¥ä¸ºç€åˆ‡ç‰‡æ“ä½œåå‘ å­—ç¬¦ä¸²çš„ç´¢å¼•ä¸ºè´Ÿå€¼æ—¶ï¼Œæ„å‘³ç€ç´¢å¼•ä»å³åˆ°å·¦æ•° åˆ—è¡¨å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶æ“ä½œä¸å­—ç¬¦ä¸²ç±»ä¼¼ å¯ä½¿ç”¨sortedå‡½æ•°å¯¹åˆ—è¡¨è¿›è¡Œæ’åº å¯ä»¥ä½¿ç”¨ç”Ÿæˆå¼è¯­æ³•åˆ›å»ºåˆ—è¡¨ï¼šf = [x for x in range(1, 10)]ï¼ˆæ­¤æ–¹æ³•åœ¨åˆ›å»ºåˆ—è¡¨åå…ƒç´ å·²ç»å‡†å¤‡å°±ç»ªï¼Œè€—è´¹è¾ƒå¤šå†…å­˜ï¼‰ï¼Œæˆ–f = (x for x in range(1, 10))ï¼ˆæ­¤æ–¹æ³•åˆ›å»ºçš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼Œéœ€è¦æ•°æ®æ—¶åˆ—è¡¨é€šè¿‡ç”Ÿæˆå™¨äº§ç”Ÿï¼ŒèŠ‚çœå†…å­˜ä½†æ˜¯è€—è´¹è¾ƒå¤šæ—¶é—´ï¼‰ å¯ä»¥ä½¿ç”¨yieldå…³é”®å­—æ¥å®ç°è¿­ä»£ï¼Œä½¿ç”¨yieldå°±æ˜¯äº§ç”Ÿäº†ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡é‡åˆ°yieldæ—¶å‡½æ•°ä¼šæš‚åœå¹¶ä¿å­˜å½“å‰æ‰€æœ‰çš„è¿è¡Œä¿¡æ¯ï¼Œè¿”å›yieldçš„å€¼ï¼Œå¹¶åœ¨ä¸‹ä¸€æ¬¡æ‰§è¡Œæ­¤æ–¹æ³•æ˜¯ä»å½“å‰ä½ç½®å¼€å§‹è¿è¡Œ å¯ä»¥å®šä¹‰å…ƒç»„ï¼Œå…¶ç›¸å½“äºä¸èƒ½ä¿®æ”¹çš„æ•°ç»„ï¼Œä¸€ä¸ªå…ƒç»„ä¸­çš„å…ƒç´ æ•°æ®ç±»å‹å¯ä»¥ä¸åŒï¼Œå®šä¹‰å…ƒç»„ä½¿ç”¨t = () åˆ—è¡¨å’Œå…ƒç»„å¯ä»¥äº’ç›¸è½¬æ¢ å¯ä»¥å®šä¹‰é›†åˆï¼Œå®šä¹‰é›†åˆå¯ä»¥ä½¿ç”¨set = {}ï¼Œå…ƒç»„å¯ä»¥è½¬æ¢ä¸ºé›†åˆ å­—å…¸ç±»ä¼¼äºæ•°ç»„ï¼Œä½†æ˜¯å®ƒæ˜¯ç”±å¤šç»„é”®å€¼å¯¹ç»„æˆçš„ 19/9/19 ä½¿ç”¨classå…³é”®å­—å®šä¹‰ç±»ï¼Œå†åœ¨ç±»ä¸­å®šä¹‰å‡½æ•°ï¼Œå¦‚ï¼šclass ç±»å(object) __init__å‡½æ•°æ˜¯ç”¨äºåœ¨åˆ›å»ºå¯¹è±¡æ—¶è¿›è¡Œçš„åˆå§‹åŒ–æ“ä½œ selfæ˜¯ç±»çš„æœ¬èº«ï¼Œæ˜¯å®ƒçš„å®ä¾‹å˜é‡ï¼Œåœ¨ç±»ä¸­æ‰€æœ‰å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°å°±æ˜¯selfï¼Œåœ¨ç±»ä¸­ä¿®æ”¹å±æ€§å€¼éœ€ä½¿ç”¨self.å±æ€§å€¼ = xçš„è¯­æ³• å®ä¾‹åŒ–ç±»çš„æ–¹æ³•ï¼šå¯¹è±¡å = ç±»å(åˆå§‹åŒ–å‡½æ•°å‚æ•°) å¯¹è±¡ä¸­æ–¹æ³•çš„å¼•ç”¨å¯ä»¥é‡‡ç”¨å¯¹è±¡.æ–¹æ³•ï¼ˆä¹Ÿå³å‡½æ•°ï¼‰çš„è¯­å¥ï¼Œé€šè¿‡æ­¤æ–¹å¼å‘å¯¹è±¡å‘é€æ¶ˆæ¯ Pythonä¸­ï¼Œå±æ€§å’Œæ–¹æ³•çš„è®¿é—®æƒé™åªæœ‰publicå’Œprivateï¼Œè‹¥å¸Œæœ›å±æ€§æˆ–æ–¹æ³•æ˜¯ç§æœ‰çš„ï¼Œåœ¨ç»™å®ƒä»¬å‘½åçš„æ—¶å€™è¦ä½¿ç”¨__å¼€å¤´ï¼Œä½†æ˜¯ä¸å»ºè®®å°†å±æ€§è®¾ç½®ä¸ºç§æœ‰çš„ ä½¿ç”¨_å¼€å¤´æš—ç¤ºå±æ€§æˆ–æ–¹æ³•æ˜¯å—ä¿æŠ¤(protected)çš„ï¼Œè®¿é—®å®ƒä»¬å»ºè®®é€šè¿‡ç‰¹å®šçš„æ–¹æ³•ï¼Œä½†å®é™…ä¸Šå®ƒä»¬è¿˜æ˜¯å¯ä»¥ç›´æ¥è¢«å¤–éƒ¨è®¿é—® å¯ä»¥é€šè¿‡åœ¨ç±»ä¸­å®šä¹‰æ–¹æ³•ä»¥è®¿é—®å¯¹è±¡å—ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨å®šä¹‰è¿™äº›æ–¹æ³•ï¼ˆå‡½æ•°ï¼‰æ—¶ï¼Œè¦åœ¨ä¸Šä¸€è¡Œä½¿ç”¨@propertyåŒ…è£…è¿™äº›æ–¹æ³• å¯¹äºè¢«ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨è®¿é—®å®ƒä»¬æ—¶é‡‡ç”¨getteræ–¹æ³•ï¼Œéœ€æ·»åŠ @propertyï¼Œåœ¨ä¿®æ”¹å®ƒä»¬æ—¶é‡‡ç”¨setteræ–¹æ³•ï¼Œéœ€æ·»åŠ @å‡½æ•°ï¼ˆå³æ–¹æ³•ï¼‰å.setter Pythonå¯ä»¥å¯¹å¯¹è±¡åŠ¨æ€ç»‘å®šæ–°çš„å±æ€§æˆ–æ–¹æ³• å¯ä»¥ä½¿ç”¨__slots__é™å®šå¯¹è±¡åªèƒ½ç»‘å®šæŸäº›å±æ€§ï¼Œä½†æ˜¯å®ƒåªå¯¹å½“å‰ç±»çš„å¯¹è±¡ç”Ÿæ•ˆï¼Œå¯¹å­ç±»ä¸èµ·ä½œç”¨ å¯ä»¥é€šè¿‡ç»™ç±»å‘é€æ¶ˆæ¯ï¼Œåœ¨ç±»çš„å¯¹è±¡è¢«åˆ›å»ºå‡ºæ¥ä¹‹å‰ç›´æ¥ä½¿ç”¨å…¶ä¸­çš„æ–¹æ³•ï¼Œæ­¤ç§æ–¹æ³•è¢«ç§°ä¸ºé™æ€æ–¹æ³•ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ @staticmethodï¼Œæ­¤ç±»æ–¹æ³•çš„å‚æ•°ä¸å«æœ‰self é€šè¿‡ç±»æ–¹æ³•å¯ä»¥è·å–ç±»ç›¸å…³çš„ä¿¡æ¯å¹¶ä¸”å¯ä»¥åˆ›å»ºå‡ºç±»çš„å¯¹è±¡ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ @classmethodï¼Œç±»æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯clsï¼Œè¿™ä¸ªclsç›¸å½“äºå°±æ˜¯åœ¨å¤–éƒ¨å®ä¾‹åŒ–ç±»æ—¶å®šä¹‰çš„å¯¹è±¡åï¼Œåªä¸è¿‡å®ƒæ˜¯æ”¾åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨äº†ï¼Œå…¶åŠŸèƒ½å°±æ˜¯å¯ä»¥åƒåœ¨å¤–éƒ¨è°ƒç”¨å¯¹è±¡çš„å±æ€§å’Œæ–¹æ³•ä¸€æ ·åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨å¯¹è±¡ï¼ˆç±»ï¼‰çš„å±æ€§å’Œæ–¹æ³• 19/9/20 ç±»ä¹‹é—´çš„å…³ç³»ï¼š is-aï¼šç»§æ‰¿æˆ–è€…æ³›åŒ–ï¼Œå¦‚ï¼šstudent is a human beingï¼Œcell phone is a electronic device has-aï¼šå…³è”ï¼Œå¦‚ department has an employee use-aï¼šä¾èµ–ï¼Œå¦‚ driver use a car ç±»ä¸ç±»ä¹‹é—´å¯ä»¥ç»§æ‰¿ï¼Œæä¾›ç»§æ‰¿ä¿¡æ¯çš„æˆä¸ºçˆ¶ç±»ï¼ˆè¶…ç±»æˆ–è€…åŸºç±»ï¼‰ï¼Œå¾—åˆ°ç»§æ‰¿çš„ç§°ä¸ºå­ç±»ï¼ˆæ´¾ç”Ÿç±»æˆ–è€…è¡ç”Ÿç±»ï¼‰ Pythonä¸­ç»§æ‰¿çš„å†™æ³•ï¼šclass å­ç±»å(åŸºç±»å) åœ¨ç¼–ç¨‹ä¸­ä¸€èˆ¬ä½¿ç”¨å­ç±»å»æ›¿ä»£åŸºç±» åœ¨å­ç±»ä¸­ï¼Œé€šè¿‡é‡æ–°å®šä¹‰çˆ¶ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥è®©åŒä¸€ç§æ–¹æ³•åœ¨ä¸åŒçš„å­ç±»ä¸­æœ‰ä¸åŒçš„è¡Œä¸ºï¼Œè¿™ç§°ä¸ºé‡å†™ 20/1/11 Pythonä¸­æä¾›ä¸¤ä¸ªé‡è¦çš„åŠŸèƒ½ï¼šå¼‚å¸¸å¤„ç†å’Œæ–­è¨€ï¼ˆAssertionsï¼‰æ¥å¤„ç†è¿è¡Œä¸­å‡ºç°çš„å¼‚å¸¸å’Œé”™è¯¯ï¼Œä»–ä»¬çš„åŠŸèƒ½æ˜¯ç”¨äºè°ƒè¯•Pythonç¨‹åº å¼‚å¸¸ï¼šæ— æ³•æ­£å¸¸å¤„ç†ç¨‹åºæ—¶ä¼šå‘ç”Ÿå¼‚å¸¸ï¼Œæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚æœä¸æ•è·å¼‚å¸¸ï¼Œç¨‹åºä¼šç»ˆæ­¢æ‰§è¡Œ Pythonä¸­å¼‚å¸¸å¤„ç†çš„å†™æ³•ï¼š 12345678try: #operation1except exception_type, argument: #if error occurs in operation1, execute operation2 #operation2else: #if no error occurs in operation1, execute operation3 #operation3 ä½¿ç”¨exceptå¯ä»¥ä¸å¸¦å¼‚å¸¸ç±»å‹ï¼Œä½†æ˜¯ä¼šè®©try-exceptè¯­å¥æ•è·æ‰€æœ‰çš„å¼‚å¸¸ï¼Œä¸å»ºè®®è¿™æ ·å†™ å¯ä»¥ä½¿ç”¨expect(exception1[, expection2[, expection3]])æ¥æ·»åŠ å¤šä¸ªå¼‚å¸¸ç±»å‹ argumentä¸ºå¼‚å¸¸çš„å‚æ•°ï¼Œå¯ä»¥ç”¨äºè¾“å‡ºå¼‚å¸¸ä¿¡æ¯çš„å¼‚å¸¸å€¼ ä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ï¼Œä½†æ˜¯ä¸try-exceptæœ‰æ‰€ä¸åŒï¼š 1234try: #operation1finally: #in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished finallyå’Œexceptä¸å¯ä»¥åŒæ—¶ä½¿ç”¨ å¯ä»¥ä½¿ç”¨raiseè§¦å‘å¼‚å¸¸ append()æ–¹æ³•ç”¨äºåœ¨åˆ—è¡¨æœ«å°¾æ·»åŠ æ–°çš„å¯¹è±¡ï¼Œå¯¹äºä¸€ä¸ªæ•°ç»„listï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼šlist.append() å¤šçº¿ç¨‹ç”¨äºåŒæ—¶æ‰§è¡Œå¤šä¸ªä¸åŒçš„ç¨‹åºï¼Œå¯ä»¥æŠŠå æ®é•¿æ—¶é—´çš„ç¨‹åºä¸­çš„ä»»åŠ¡æ”¾åˆ°åå°å¤„ç† çº¿ç¨‹ä¸è¿›ç¨‹ï¼šç‹¬ç«‹çš„çº¿ç¨‹æœ‰è‡ªå·±çš„ç¨‹åºå…¥å£ã€æ‰§è¡Œåºåˆ—ã€ç¨‹åºå‡ºå£ï¼Œä½†æ˜¯çº¿ç¨‹ä¸å¯ä»¥ç‹¬ç«‹æ‰§è¡Œï¼Œå¿…é¡»ä¾å­˜åœ¨åº”ç”¨ç¨‹åºä¸­ï¼Œç”±åº”ç”¨ç¨‹åºæä¾›å¤šä¸ªçº¿ç¨‹æ‰§è¡Œæ§åˆ¶ åœ¨Pythonä¸­ä½¿ç”¨çº¿ç¨‹ï¼šthread.start_new_thread(function, args[, kwargs])ï¼Œå…¶ä¸­functionä¸ºçº¿ç¨‹å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°éœ€è¦æå‰å®šä¹‰å¥½ï¼Œargsä¸ºä¼ é€’ç»™çº¿ç¨‹å‡½æ•°çš„å‚æ•°ï¼Œæ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œkwargsä¸ºå¯é€‰å‚æ•°ï¼Œæ­¤ç§æ–¹å¼ç§°ä¸ºå‡½æ•°å¼ï¼Œçº¿ç¨‹çš„ç»“æŸä¸€èˆ¬é å‡½æ•°çš„è‡ªç„¶ç»“æŸ æ­¤å¤–è¿˜å¯ä»¥ä½¿ç”¨Pythonæ‰€æä¾›çš„threadingæ¨¡å—ï¼Œç›´æ¥ä»threading.Threadç»§æ‰¿ï¼šclass myThread(threading.Thread)ï¼Œç„¶åé‡å†™__init__å’Œrunæ–¹æ³•ï¼ŒæŠŠéœ€è¦æ‰§è¡Œçš„ä»£ç å†™åˆ°runæ–¹æ³•é‡Œé¢ï¼Œ__init__çš„é‡å†™æ–¹æ³•å¦‚ä¸‹ï¼š 12345def __init__(self, threadID, name, counter): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.counter = counter ä¸Šè¿°threadç±»æä¾›äº†ä»¥ä¸‹æ–¹æ³•ï¼š run()ï¼šè¡¨ç¤ºçº¿ç¨‹æ´»åŠ¨çš„æ–¹æ³• startï¼šå¯åŠ¨çº¿ç¨‹ join()ï¼šç­‰å¾…ç›´åˆ°çº¿ç¨‹ç»ˆæ­¢ isAlive()ï¼šæŸ¥è¯¢çº¿ç¨‹æ˜¯å¦æ´»åŠ¨ getName()ï¼šè¿”å›çº¿ç¨‹å setName()ï¼šè®¾ç½®çº¿ç¨‹å ä¸ºäº†é¿å…ä¸¤ä¸ªæˆ–å¤šä¸ªçº¿ç¨‹åŒæ—¶è¿è¡Œï¼Œäº§ç”Ÿå†²çªï¼Œå¯ä»¥ä½¿ç”¨çº¿ç¨‹é”æ¥æ§åˆ¶çº¿ç¨‹æ‰§è¡Œçš„ä¼˜å…ˆé¡ºåºï¼Œè¢«é”å®šçš„çº¿ç¨‹ä¼˜å…ˆæ‰§è¡Œï¼Œå…¶ä»–è¿›ç¨‹å¿…é¡»åœæ­¢ å¯ä»¥ä½¿ç”¨threading.Lock().acquire()å’Œthreading.Lock().release()æ¥é”å®šå’Œé‡Šæ”¾çº¿ç¨‹ å¯ä»¥å»ºç«‹ä¸€ä¸ªç©ºæ•°ç»„ç”¨äºå­˜æ”¾çº¿ç¨‹ï¼Œå†é€šè¿‡appendæ–¹æ³•å°†çº¿ç¨‹æ·»åŠ è‡³è¯¥æ•°ç»„ä¸­ï¼Œé€šè¿‡éå†æ•°ç»„å¯ä»¥å¯¹å…¶ä¸­çš„çº¿ç¨‹åšåŒæ ·çš„æ“ä½œ","link":"/2020/01/06/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"Summary of Reinforcement Learning 1","text":"PrefaceThis blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. These series of blogs of mine are mostly based on the following works and Iâ€™m really grateful to the contributors: Online courses of Stanford University CS234: Reinforcement Learning, Emma Brunskill and lecture notes. Blogs of ä»æµåŸŸåˆ°æµ·åŸŸ. Blogs of å¶å¼º. If you find any mistake in my articles, please feel free to tell me in comments. What is reinforcement learning (RL)?RL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision. A RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agentâ€™s desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more â€œsmarterâ€ and has a better performance. Some basic notions of RLBecause in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce â€œtimeâ€ to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript â€œtâ€ means time it is in a time sequence. Agent: The subject of RL, it is agent that interact with the world. Model: The world, the environment, the agent stays in the model. Reward: $ {r_t} $ , the feedback signal from the model, agent recieves the reward. The reward can have different values according to the different states of the agent. State: ${s_t}$ , the state of the agent. The state can be either finite or infinite, and it is set by people. Action: ${a_t}$ , the movement of the agent in the model, actions are different under different states. Observation: ${o_t}$ , the agent need to observe its state and determine the reward. History: a sequence of action, reward, observation, which is: $h_t=(a_1,o_1,r_1,â€¦,a_t,o_t,r_t)$. Sequential Decision Making: make decision base on the history, that is: $a_{t+1}=f(h_t)$. Figure 1.1 shows how an agent interact with its world. How to model the world?Markov Property$P(s_{t+1}|s_t,a_t,â€¦,s_1,a_1)=P(s_{t+1}|s_t,a_t)$ Left-hand side is called the transition dynamics of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. A model consists of the two elements below. Transition dynamics $P(s_{t+1}|s_t,a_t)$The probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. Reward function $R(s,a)$Usually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$. How to make a RL agent?Let the agent state be a function of the history, $s_t^a=g(h_t)$. An agent often consists the three elements below. Policy $\\pi(a_t|s_a^t)$Policy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic. When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability $P(a_t=a)=\\pi(a|s_t^a)$. Value function $V^\\pi$If we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards $V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+â€¦|s_t=s]$. ModelThe agent in RL may have a model. I have introduced how to make a model in section 3. Three questions we are facingDo we need exploration or exploitation?In RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation. Can the agent generalize its experience?In actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states? Delayed consequencesThe action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier? Whatâ€™s next?Now we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And whatâ€™s the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.","link":"/2020/01/17/RLSummary1/"},{"title":"Summary of Reinforcement Learning 2","text":"Markov process (MP)Markov process is a stochastic process that satisfies the Markov property, which means it is â€œmemorylessâ€ and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. We need to make two assumptions before we define the Markov process. The first assumption is that the state of MP is finite, and we have $s_i\\in S, i\\in1,2,â€¦$ , where $|S|&lt;\\infty$. The second assumption is that the transition probabilities are time independent. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,â€¦$. Base on these two assumption, we can define a transition transform matrix: The size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1. Henceforth, we can define a Markov process using a tuple $(S,\\bf P)$. $S$: A finite state space. $\\bf P$: A transition probability. By calculating $S\\bf P$ we can get the distribution of the new state. Figure 1 shows a student MP example. Markov reward process (MRP)MRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it. $S$: A finite state space. $\\bf P$: A transition probability. $R$: A reward function that maps states to rewards (real numbers). $\\gamma$: Discount factor between 0 and 1. Here are some explaintions. Reward functionWhen we are moving from the current state $s$ to a successor state $sâ€™$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $sâ€™$ ). For a state $s\\in S$, we define the expected reward by $R(s)=\\Bbb E[r_t|s_t=s]$. Here we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$. HorizonIt is defined as the number of time steps in each episode of the process. An episode is the whole process of a round of training. The horizon can be finite or infinite. ReturnThe return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon H. We can calculate the return using $G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$. State value functionThe state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression $V_t(s)=\\Bbb E[G_t|s_t=s]$. If the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes. Discount factorWe design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma &lt;1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present. Figure 2 and 3 shows an example of how to calculate the return. It is significant to find out a value function while many problems of RL is how to get a value function essentially. Computing the value functionWe have three ways to compute the value function. Simulation. Through simulation, we can get the value function by averaing many returns of episodes. Analytic solution. We have defined the state value function $V_t(s)=\\Bbb E[G_t|s_t=s]$. Then, make a little transformation, see Figure 4 in detail. Then, we have $V(s)=R(s)+\\gamma \\sum P(sâ€™|s)V(sâ€™)$, $V=R+\\gamma\\bf P\\mit V$. Therefore we have $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. If $0&lt;\\gamma&lt;1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large. Notice that $sâ€™$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle. Iterative solution. $V_t(s)=R(s)+\\gamma \\sum P(sâ€™|s)V_{t+1}(sâ€™), \\forall t=0,â€¦,H-1,V_H(s)=0$. We can iterate it again and again and use $|V_t-V_{t-1}|&lt;\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. Markov decision process (MDP)MDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. $S$: A finite state space. $A$: A finite set of actions which are available from each state $s$. $\\bf P$: A transition probability. $R$: A reward function that maps states to rewards (real numbers). $\\gamma$: Discount factor between 0 and 1. Here are some explanations. Notifications Both $S$ and $A$ are finite. In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as $P(s_{t+1}|s_t,a_t)$. In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$. Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP. PolicyBefore we mention the state value function, we need to talk about the policy for the MDP first. A policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be varying with time, especially when the horizon is finite. A policy can be written as $\\pi(a|s)=P(a_t=a|s_t=s)$. If given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: $P^\\pi(sâ€™|s)=\\sum_{a\\in A}\\pi(a|s) P(sâ€™|s,a)$ When we have a policy $\\pi$, the probability of the state transforms from $s$ to $sâ€™$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $sâ€™$ when executing an action $a$. $R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$ When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$. Value functions in MDP (Bellman expectation equations)Given a policy $\\pi$ can define two quantities: the state value function and the state-action value function. These two value functions are both Bellman expectation equations. State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression $V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. Frequently we will drop the subscript $\\pi$ in the expectation. State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form $Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. It evaluates the value of acting the action $a$ under current state $s$. Now letâ€™s talk about the relationships between these two value functions. Figure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions. We can discover that the value of a state can be denoted as $V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$. In a similar way, Figure 7 shows what states that an action can lead to. We can also find that $Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^\\pi(sâ€™)$. On the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $sâ€™$ and the probability of getting into that new state. If we combine the two Bellman equation with each other, we can get $V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V^\\pi(sâ€™)]$ â€‹ $=R(sâ€™,\\pi(sâ€™))+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,\\pi(s)) V^\\pi(sâ€™)$, and $Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)\\sum_{a\\in A}\\pi(aâ€™|sâ€™)Q_\\pi(sâ€™,aâ€™)$. The example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$. Optimality value function (Bellman optimality equation) Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest. Optimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. Find the best policyThe best policy is defined precisely as optimal policy $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states $s\\in S$ , there is $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$. For an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique. We can compute the optimal policy by $\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$, Which means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. If an optimal policy exists then its value function must be a fixed point of the operator $B^*$. Bellman optimality backup operatorBellman optimality backup operator is written as $B^*$ with a value function behind it $B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V(sâ€™)$. If $\\gamma&lt;1$, $B^*$ is a strict contraction and has a unique fixed point. This means $B^*V(s)\\geq V^\\pi(s)$. Bellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation. Next Iâ€™ll briefly introduce some algorithms to compute the optimal value function and an optimal policy. Policy searchThis algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a value function and a policy. Policy iterationThe algorithm of policy iteration is shown below: while True do â€‹ $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here) â€‹ $\\pi^*$ = Policy improvement $(M,V^\\pi)$ if $\\pi^*(s)=\\pi(s)$ then â€‹ break else â€‹ $\\pi$ = $\\pi^*$ $V^*$ = $V^\\pi$ . Policy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute $Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^{\\pi i}(sâ€™)$ for all the $a$ and $s$ and then take the max return $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$. Notice that there is a relationship $\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$. This means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again. Value iterationThe algorithm of value iteration is shown below: $Vâ€™(s)=0, V(s)=\\infty$, for all $s\\in S$ while $||V-Vâ€™||_\\infty&gt;\\epsilon$ do â€‹ $V=Vâ€™$ â€‹ $Vâ€™(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)Vâ€™(s)$, for all states $s\\in S$ $V^*=V$, for all $s\\in S$ $\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V^ *(sâ€™),\\ \\forall s\\in S$ . The idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.","link":"/2020/01/18/RLSummary2/"},{"title":"Summary of Reinforcement Learning 3","text":"IntroductionIn the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss model-free algorithms in this article. Throughout this article, we will assume an infinite horizon as well as stationary rewards, transition probabilities and policies. First comes the definition of history: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: $h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},â€¦,s_{j,L_j})$, where $L_j$ is the length of the interaction (interaction between agent and environment). In the article Summary of Reinforcement Learning 2 I introduced the iterative solution of value function, which is $V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$ â€‹ $=R(s)+\\gamma \\sum P(sâ€™|s)V_{t+1}(sâ€™), \\forall t=0,â€¦,H-1,V_H(s)=0$. This ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. Monte Carlo on policy evaluationIn general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. In reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: Execute a rollout of policy until termination many times Record the returns $G_t$ that we observe when starting at state $s$ Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. Figure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process. How to Evaluate the Good and Bad of an Algorithm?We use three quntities to evaluate the good and bad of an algorithm. Consider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and itâ€™s a function of observed data $x$. Then we have these quantities of the estimator: Bias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, Variance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, Mean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. First-Visit Monte CarloHere is the algorithm of First-Visit Monte Carlo: Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$ $N(s)$: Increment counter of total first visits $G(s)$: Increment total return $V(s)$: Estimate while each state $s$ visited in episode $i$ do â€‹ while first time $t$ that the state $s$ is visited in episode $i$ do â€‹ $N(s)=N(s)+1$ â€‹ $G(s)=G(s)+G_{i,t}$ â€‹ $V(s)=G(s)/N(s)$ return $V(s)$ First-Visit Monte Carlo estimator is an unbised estimator. Every-Visit Monte CarloHere is the algorithm of Every-Visit Monte Carlo: Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$ $N(s)$: Increment counter of total first visits $G(s)$: Increment total return $V(s)$: Estimate while each state $s$ visited in episode $i$ do â€‹ while every time $t$ that the state $s$ is visited in episode $i$ do â€‹ $N(s)=N(s)+1$ â€‹ $G(s)=G(s)+G_{i,t}$ â€‹ $V(s)=G(s)/N(s)$ return $V(s)$. Every-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. Increment First-Visit/Every-Visit Monte CarloWe can replace $V(s)=G(s)/N(s)$ in both two algorithms by $V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. Because ${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. Replacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general Incremental Monte Carlo on policy evaluation. Setting $\\alpha &gt; {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. Temporal Difference (TD) LearningTD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. In dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, and this is the TD learning update. In TD learning update, there are two concepts which are TD error and TD target. TD error is written as below: $\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. And here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: $r_t+\\gamma V^\\pi(s_{t+1})$. The algorithm of TD learning is shown below. Initialize $V^\\pi(s)=0,\\ s\\in S$ while True do â€‹ Sample tuple $(s_t,a_t,r_t,s_{t+1})$ â€‹ $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ It is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus thatâ€™s why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process. In reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. Figure 2 shows a diagram expressing TD learning. SummaryTable below gives some fundamental properties of these three algorithms (DP, MC, TD). Properties DP MC TD Useble when no models of current domain No Yes Yes Handles continuing domains (episodes will never terminate) Yes No Yes Handles Non-Markovian domains No Yes No Coverges to true value in limit (satisfying some conditions) Yes Yes Yes Unbised estimate of value N/A Yes (First-Visit MC) No Variance N/A High Low Figure 3 shows some other properties that may help us to choose the algorithm. Batch Monte Carlo and Temporal DifferenceThe batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. In the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where . The value function derived from the maximum likehood MDP model is known as the certainty equivalence estimate. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.","link":"/2020/02/01/RLSummary3/"},{"title":"Summary of Reinforcement Learning 6","text":"IntroductionIn the last article we briefly talked about control using linear vlaue function approximation and three different methods. For example in Q-learning, we have: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{aâ€™}\\mit\\hat q^\\pi(sâ€™,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. Then we can take calcullate the weight: $\\vec wâ€™=\\vec w+\\Delta\\vec w$. Finally we can compute the function approximator: $\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. The performance of linear function approximators highly depends on the quality of features ($\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ â€¦\\ x_n(s,a)]$) and it is difficult and time-consuming for us to handcraft an appropriate set of features. To scale up to making decisions in really large domains and enable automatic feature extraction, deep neural networks (DNNs) are used as function approximators. In the following contents, we will introduce how to approximate $\\hat q^\\pi(sâ€™,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$ via end-to-end training. And we will introduce three popular value-based deep reinforcement learning algorithms: Deep Q-Network (DQN), Double DQN and Dueling DQN. It is OK for a deep-learning freshman to study deep reinforcement learning and one doesnâ€™t need to expert in deep learning. He/She just need some basic concepts of deep learning which we will discuss next. Deep Neural Network (DNN)DNN is the composition of miltiple functions. Assuming that $\\vec x$ is the input and $\\vec y$ is the output, a simple DNN can be written as: $\\vec y=h_n(h_{n-1}(â€¦h_1(\\vec x)â€¦))$, Where $h$ are different functions. These functions can be linear or non-linear. For linear functions, $h_n=w_n h_{n-1}+b_n$, $w_n$ is weight and $b_n$ is bias. For non-linear functions, $h_n=f_n(h_{n-1})$. The $f_n$ here is called as activation function, such as sigmoid function or relu function. The purpose of setting activation function is to make the nerual network more like the human nerual system. If all the functions are differentiable, we can use chain rule to back propagate the gradient of $\\vec y$. Now we have some tools such as Tensorflow or Pytorch to help us compute the gradient automatically. Typically we need a loss function to fit the parameters. In DNN (as well as CNN) we update weights and biases to get the desired output. In deep Q-learning, the outputs are always some scalers, in other words, Q-value. Figure 1 shows the structure of a nerual network that is relatively complex. The important components of one of the routes is marked. Figure 2 shows the detailed structure of a node. Benefits Uses distributed representations instead of local representations Universal function approximator Can potentially need exponentially less nodes/parameters to represent the same function Can learn the parameters using SGD Convolutional Nerual Network (CNN)CNN is widely used in computer vision. If you want to make decisions using pictures, CNN is very useful for visual input. Images have structure, they have local structure and correlation. They have distictive features in space and frequency domain. CNN can extract these features and give the output. Figure 3 shows the basic process as well as some features of CNN. Now I am going to give you a brief introduction of how a CNN works. Receptive FieldFirst, we need to randomly choose a part of the image as the input of a hidden unit. That part chosen from the image is called as filter/kernel/receptive field (we will call it filter after that). The range of the filter is called filter size. In the example showned in Figure 3, the filter size is $5\\times 5$. One CNN will have many filters and they form what we called input batch. Input batch is connected to the hidden units. StrideNow we want the filter to scan all over the image. We can slide the $5\\times5$ filter over all the input pixels. If the filter move 1 pixel each time it slides, we define that the stride length is 1. Of course we can use other stride lengths. Assume the input is $28\\times28$, than we need to move $24\\times24$ times and we will have a $24\\times24$ first hidden-layer. For a filter, it will have 25 weights. Shared Weights and Feature MapFor a same feature in the image, we want the algorithm able to recognize it no matter it is showned in any part of it (left side, right side, etc.) or in any direction (vertical, horizontal, etc.). Thus, no matter where the filter moves, we want its weights are always the same. In this example, for the whole CNN we will have 25 weights totally. This feature is called shared weights. The map from the input layer to the hidden layer is therefore a feature map: all nodes detect the same feature in different parts. The feature map is defined by the shared weights and bias and it is the result of the application of a convolutional filter. Convolutional LayersFeature map is the output of convolutional layer. Figure 7 and Figure 8 gives you a visualized example of how it works. In Figure 8, the green matrix is a image (input) while the yellow matrix in it is a $3\\times3$ filter. The red numbers in the filter are weights. The pink matrix at the right is a feature map derives from the left. The value of each unit in feature map is the sum of the value of each unit in the filter times its weight. Pooling LayersPooling layers are usually used immediately after convolutional layers. They compress the information in the output from the convolutional layers. A pooling layer takes each feature map output form convolutional layer and prepares a condensed feature map. ReLU LayersReLU is the abbrivation of rectified linear unit. It is constructed by non-linear functions (activation functions). It increases the nonlinear properties of the overall network without affecting the filters of the convolution layer. Fully Connected LayersThe process we have talked about is designed to catch the features of the image. After we have done this, we are going to do regression. This work is done by fully connected layers. They can do regression and output some scalers (Q-value in deep Q learning domain). We now have a rough idea towards CNN. If you want know more about it, you can go to this website. Deep Q-LearningOur target is to approximate $\\hat q(s,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$. I will give you an example first and then talk about algorithms. DQN in AtariAtari is a video game. Researchers tried to apply DQN to train the computer to play this game. The architecture of the DQN they designed is shown in Figure 11. The input to the network consists of an $84\\times84\\times4$ preprocessed image, followed by three convolutional layers and two fully connected layers with a single output for each valid action. Each hidden layer is followed by a rectifier nonlinearity (ReLU). The network outputs a vector containing Q-values fro each valid action. The reward is change in score for that step. Preprocessing Raw PixelsThe raw Atari frames are of size $260\\times260\\times3$, where the last dimension is corresponding to the RGB channels. The preprocessing step aims at reducing the imput dimensionality and dealing with some artifacts of game emulator. The process can be summarized as follows: Single frame coding: the maximum value of each pixel color value over the frame being encoded and the previous frame is returned. In other words, we return a pixel-wise max-pooling of the 2 consecutive raw pixel frames. Dimensionality reduction: extract the luminance channel, from the encoded RGB frame and rescale it to $84\\times84\\times1$. The above preprocessing is applied to the 4 most recent raw RGB frames and the encoded frames are stacked together to produce the input ($84\\times84\\times4$) to the Q-network. Training Algorithm for DQNEssentially, the Q-network is learned by minimizing the following mean squarred error: $J(\\vec w)=\\Bbb E_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\\hat q(s_t,a_t,\\vec w))^2]$, where $y_t^{DQN}$ is the one-step ahead learning target: $y_t^{DQN}=r_t+\\gamma\\tt max_{aâ€™}\\mit \\hat q(s_{t+1},aâ€™,\\vec w^-)$, where $\\vec w^-$ represents the parameters of the target network (belong to CNN, the desire true value) and the parameters $\\vec w$ of the online network (belong to function approximator) are updated by sampling gradients from minibatches of past transition tuples $(s_t,a_t,r_t,s_{t+1})$. Notice that when we refer to target network/targets, things are related to the so-called true values provided from Q-network (CNN). And when we refer to online network, things are related to the Q-learning process. In the last article, we talked about Q-learning with value function approximation. But Q-learning with VFA can diverge. DQN introduces two major changes in order to avoid divergence, which are experience replay and a separate target network. Experience ReplayThe agentâ€™s experiences (or transitions) at each time step $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in a fixed-sized dataset (or replay buffer) $D_t={e_1,â€¦,e_t}$. Figure 12 shows how a replay buffer looks like. To perform experience replay, we need to repeat the following: $(s,a,r,sâ€™)$~$D$: sample an experience tuple form the dataset Compute the target value for the sampled $s$: $y_t^{DQN}=r_t+\\gamma\\tt max_{aâ€™}\\mit \\hat q(s_{t+1},aâ€™,\\vec w^-)$ Use SGD to update the network weights: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{aâ€™}\\mit\\hat q^\\pi(sâ€™,a,\\vec w^-)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$ Target NetworkTo further improve the stability of learning and deal with non-stationary learning targets, a separate target network is used for generating the targets $y_j$ in the Q-learning update. More specifically, after every $C$ steps the target network $\\hat q(s,a,\\vec w^-)$ is updated by copying the parametersâ€™ values $(\\vec w^-=\\vec w)$ from the online network $\\hat q(s,a,\\vec w)$, and the target network remains unchanged and generates targets $y_j$ for the following $C$ updates. Summary of DQN and Algorithm DQN uses experience replay and fixed Q-tragets Store transition $(s_t,a_t,r_t,s_{t+1})$ in replay buffer $D$ Sample minibatch of transitions $(s,a,r,sâ€™)$ from $D$ Compute Q-learning target with respect to old, fixed parameters $\\vec w^-$ Optimizes MSE between Q-network and Q-learning targets Uses stochastic gradient descent The algorithm of DQN is shown below: Double Deep Q-Network (DDQN)After the successful application of DQN to Atari, people become very interested in it and developed many other improvements, while DDQN and Dueling DQN are two very popular algorithms among them. Letâ€™s talk about DDQN first. Recall in Double Q-learning, in order to eliminate maximization bias, two Q-functions are maintained and learned by randomly assigning transitions to update one of two functions, resulting two different sets of parameters, denote here as $w$ and $wâ€™$. This idea can also be extented to deep Q-learning. The target network in DQN architecture provides a natural candidate for the second Q-function, without introducing additional networks. Similarly, the greedy action is generated accroding to the online network with parameters $w$, but its value is estimated by the target network with parameters $w^-$. The resulting algorithm is reffered as DDQN, which just slightly change the way $y_t$ updates: $y_t^{DDQN}=r_t+\\gamma\\hat q(s_{t+1},\\tt argmax_{aâ€™}\\mit\\hat q(s_{t+1},aâ€™,\\vec w),\\vec w^-)$. Dueling DQNBefore we delve into dueling architecture, letâ€™s first introduce an important quantity, the advantage function, which relates the value and Q-functions (assume following a policy $\\pi$): $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$. Intuitively, the advantage function sbstracts the value of the state from the Q funciton to get a relative measure of the importance of each action. DQN approximates the Q-function by decoupling the value function and the advantage function. Figure 13 illustrates the dueling network architecture and the DQN for comparison. The different between dueling network and DQN is that, the dueling network uses two streams of fully connected layers. One stream is used to provide value function estimate given a state, while the other stream is for estimating advantage function for each valid action. Finally, the two streams are comined in a way to produce and approximate the Q-function. Why these two separated streams are designed? First, for many states, it is unnecessary to estimate the value of each possible action choice. Second, features required to determine the value function may be different than those used to accurately estimate action benefits. Letâ€™s denote the scalar output value function from one stream of fully-connected layers as $\\hat v(s,\\vec w,\\vec w_v)$, and denote the vector output advantage function from the other stream as $A(s,a,\\vec w,\\vec w_A)$. We use $\\vec w$ here to denote the shared parameters in the convolutional layers, and use $\\vec w_v$ and $\\vec w_A$ to represent parameters in the two different streams of fully-connected layers. According to the definition of advantage function, we have: $\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+A(s,a,\\vec w,\\vec w_A)$. However, the expression above is unidentifiable, which means we can not recover $\\hat v$ and $A$ form a given $\\hat q$. This unidentifiable issue is mirrored by poor performance in practice. To make Q-function identifiable, we can force the advantage function to have zero estimate at the chosen action. Then, we have: $\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-\\tt max_{aâ€™\\in A}\\mit A(s,aâ€™,\\vec w,\\vec w_A))$. Or we can just use mean as baseline: $\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-{1\\over|A|}\\sum_{aâ€™}A(s,aâ€™,\\vec w,\\vec w_A))$.","link":"/2020/02/23/RLSummary6/"},{"title":"Summary of Reinforcement Learning 5","text":"IntroductionSo far we have presented value function by a lookup table (vector or matrix). However, this approach might not generalize or sufficient well to problems with very large state and/or action spaces in reality. A popular approach to address this problem via function approximation: $v_\\pi(s)\\approx \\hat v(s,\\vec w)$ or $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. Here $\\vec w$ is usually referred to as the parameter or weights of our function approximator. Our target is to output a reasonable value function (it can also be called as update target in this domain) by calculating the proper $\\vec w$ with the input $s$ or $(s,a)$. In this set of article, we will explore two popular classes of differentiable function approximators: Linear feature representations and Nerual networks. We will only focus on linear feature representations in this article. Linear Feature RepresentationsGradient DescentThe rough definition of gradient is that, for a function that has several variables, gradient (a vector) at a spot $x_0$ tells us the direction of the steepest increase in the objective function at $x_0$. Suppose that $J(\\vec w)$ is an arbitrary function and vector $\\vec w$ is its parameter, the gradient of it at some initial spot $\\vec w$ is: $\\nabla_\\vec wJ(\\vec w)=[{\\partial J(\\vec w)\\over\\partial w_1}{\\partial J(\\vec w)\\over\\partial w_2}â€¦{\\partial J(\\vec w)\\over\\partial w_n}]$. In oreder to minimize our objective function, we take a step along the negative direction of the gradient vector and arrive at $\\vec wâ€™$, mathematically written as: $\\Delta\\vec w=-{1\\over 2}\\alpha \\nabla_\\vec wJ(\\vec w)$, $\\vec wâ€™=\\vec w+\\Delta \\vec w$ ($\\alpha$ is update step). By using this way for many times we can reach the point that our objective function is minimize (local optima). Figure 1 is the visualization of gradient descent. ####Stochastic Gradient Descent (SGD) In linear function representations, we use a feature vector to represent a state: $\\vec x(s)=[x_1(s)\\ x_2(s)\\ â€¦\\ x_n(s)]$. We than approximate our value functions using a linear combination of features: $\\hat v(s,\\vec w)=\\vec x(s)\\vec w=\\sum_{j=1}^nx_j(s)w_j$. Our goal is to find the $\\vec w$ that minimizes the loss between a true value function $v_\\pi(s)$ and its approximation $\\hat v(s,\\vec w)$. So now we define the objective function (also known as the loss function) to be: $J(\\vec w)=\\Bbb E[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$. Then we can use gradient descent to calculate $\\vec wâ€™$ ($w$ at next time step): $\\vec wâ€™=\\vec w-{1\\over2}\\alpha\\nabla_\\vec w[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$ â€‹ $=\\vec w+\\alpha[v_\\pi(s)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$. However, it is impossible for us to know the true value of $v_\\pi(s)$ in real world. So we will then talk about how to do value function approximation without a model, or, in other words, find something to replace the true value to make this idea practicable. Monte Carlo with Linear Value Function Approximation (VFA)As we know, the return $G$ is an unbiased sample of $v_\\pi(s)$ with some noise. So if we substituted $G$ for $v_\\pi(s)$, we have: $\\vec wâ€™=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ â€‹ $=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\vec x(s)$. Tha algorithm of Monte Carlo linear value function approximation is shown below: . This algorithm can also be modified into a every-visit type. Once we have $\\vec wâ€™$ we can calculate the approximation of the value function $\\hat v(s,\\vec w)$ by $\\vec x(s)^T\\vec wâ€™$. Temporal Difference with Linear VFAIn TD learning we use $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ to update $V^\\pi$. To apply this method to VFA, we can rewrite the expression of $\\vec w$ as: $\\vec wâ€™=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(sâ€™,\\vec w)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ â€‹ $=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(sâ€™,\\vec w)-\\hat v(s,\\vec w)]\\vec x(s)$. The algorithm of TD(0) with linear VFA is shown below: . The two algorithm we introduced above can both converge to the weights $\\vec w$ with different minimum mean squared error (MSE). Among them the MSE of TD method is slightly greater than the MC one, but it is good engouh. Control Using VFASimilar to VFAs, we can also use function approximator for action-values and we let $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. In this part we will use VFA to approximate policy evaluation and than perform $\\epsilon$-greedy policy improvement. However, this process can be unstable because it involes the intersection of function approximation, bootstrapping, and off-policy learning. These three things are called as the dadely triad, which may make the result fail to converge or converge to something bad. Now I will quickly pass this part using the basic concept we have mentioned before. First we define our objective function $J(\\vec w)$ as: $J(\\vec w)=\\Bbb E[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))^2]$. Then we define the state-action value feature vector: $\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ â€¦\\ x_n(s,a)]$, and represent state-action value as linear combinations of features: $\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. Compute the gradient: $-{1\\over 2}\\nabla_\\vec wJ(\\vec w)=\\Bbb E_\\pi[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\nabla_\\vec w\\hat q^\\pi(s,a,\\vec w)]$ â€‹ $=(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\vec x(s,a)$. Compute an update step using gradient descent: $\\Delta\\vec w=-{1\\over 2}\\alpha\\nabla_\\vec wJ(\\vec w)$ â€‹ $=\\alpha(q_\\pi(s,a)-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. Take a step towards the local minimum: $\\vec wâ€™=\\vec w+ \\Delta\\vec w$. Just like what we have said before, we cannot get the true value of $q_\\pi(s,a)$ so we gonna use other values to replace it and the difference between those methods is the difference of the value we choose. For Monte Carlo methods, we use return $G$, and the update becomes: $\\Delta\\vec w=\\alpha(G-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. For SARSA we have: $\\Delta\\vec w=\\alpha[r+\\gamma \\hat q^\\pi(sâ€™,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. And for Q-learning: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{aâ€™}\\mit\\hat q^\\pi(sâ€™,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. Notice that because of the value function approximations, which can be expansions, converge is not guaranteed. The table below gives the summary of convergence of control methods with VFA and (Yes) means the result chatters around near-optimal value function. Algorithm Tabular Linear VFA Nonlinear VFA MC Control Yes (Yes) No SARSA Yes (Yes) No Q-learning Yes No No In the next article we will talk about deep reinforcement learning using nerual networks.","link":"/2020/02/19/RLSummary5/"},{"title":"Summary of Reinforcement Learning 4","text":"IntroductionIn this article we will discuss model-free control where we learn good policies under the same constrains (only interactions, no knowledge of reward structure or transition probabilities). In actual world, many problems can be modeled into a MDP and model-free control is important for some problems in two types of domains: MDP model is unknown but we can sample the trajectories from the MDP MDP model is known but computing the value function is really really hard due to the size of the domain There are two types of policy learning under model-free control domain, which are on-policy learning and off-policy learning. On-policy learning: base on direct experience and learn to estimate and evaluate a policy from experience obtained from following that policy Off-policy learning: learn to estimate and evaluate a policy using experience gathered from following a different policy Generalized Policy IterationIn Summarize of Reinforcement Learning 2 we have learned the algorithm of policy iteration, which is: (1) while True do (2) $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here) (3) $\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)=\\tt argmax\\mit \\ [R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^{\\pi i}(sâ€™)]$ (4) if $\\pi^*(s)=\\pi(s)$ then (5) break (6) else (7) $\\pi$ = $\\pi^*$ (8) $V^*$ = $V^\\pi$ . In order to make this algorithm model-free, we can do the policy evaluation (line 2) using the methods we mentioned in the last article. Because we are talking about control, so we use state-action value function $Q^\\pi(s,a)$ to substitute $V^\\pi$ in line 2, in a Monte Carlo way. The algorithum of MC for policy Q evaluation is written below: Initialize $N(s,a)=0,\\ G(s,a)=0,\\ Q^\\pi(s,a)=0,\\ \\forall s\\in S,\\ a\\in A$ Using policy $\\pi$ to sample an episode $i=s_{i,1},a_{i,1},r_{i,1},â€¦$ while each state, action $(s,a)$ visited in episode $i$ do â€‹ while first/every time $t$ that the state, action $(s,a)$ is visited in episode $i$ do â€‹ $N(s,a)=N(s,a)+1$ â€‹ $G(s,a)=G(s,a)+G_{i,t}$ â€‹ $Q^{\\pi i}(s,a)=Q^{\\pi i}(s,a)/N(s,a)$ return $Q^{\\pi i}(s,a)$. Thereby, accroding to the definition, we can modify the line 3 directly as: $\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)$. There are a few caveats to this modified algorithm (MC for policy Q evaluation): If policy $\\pi$ is determiniistic or dosenâ€™t take every action with some positive probability, then we cannot actually compute the argmax in line 3 The policy evaluation algorithm gives us an estimate of $Q^\\pi$, so it is not clear whether (while we want to make sure that) line 3 will monotonically improve the policy like the model-based case. Importance of ExplorationPlease notice the first caveat we just mentioned above, this means, in other words, the policy $\\pi$ needs to explore actions, even if they might be suboptimal with respect to our current Q-value estimates. And this is what we have talked about in the first article: the relationship between exploration and exploitation. Here is a simple way to balance them. $\\epsilon$-greedy PoliciesThis strategy is to take random action with small probability and take the greedy action the rest of the time. Mathematically, an $\\epsilon$-greedy policy with respect to the state-action value $Q^\\pi(s,a)$ takes the following form: . It can be summarized as: $\\epsilon$-greedy policy selects a random action with probability $\\epsilon$ or otherwise follows the greedy policy. Monotonic $\\epsilon $-greedy Policy ImprovementWe have already provided a strategy to deal with the first caveat and now we are going to focus on the second one: to prove the monotonic $\\epsilon$-greedy policy improvement. And here is the proof. Now we have that $Q^{\\pi_i}(s,\\pi_{i+1}(s))\\ge V^{\\pi_i}(s)$ implies $V^{\\pi_{i+1}}(s)\\ge V^{\\pi_i}$ for all states, as desired. Thus, the monotonic $\\epsilon $-greedy policy improvement shows us that our policy does in fact improve if we act $\\epsilon$-greedy on the current $\\epsilon$-greedy policy. Greedy in the Limit of Infinite Exploration (GLIE)$\\epsilon$-greedy is a naive way to balance exploration and exploitation and we can refine it. The new class of exploration strategies is called Greedy in the Limit of Infinite Exploration (GLIE), which allows us to make convergence guarantees about our algorithms. A policy is GLIE if it satisfies the following two properties: All state-action pairs are visited an infinite number of times: $\\lim_{i\\rightarrow\\infty}N_i(s,a)\\rightarrow\\infty$ Behavior policy converges to greedy policy A simple GLIE strategy is $\\epsilon$-greedy policy where $\\epsilon$ is decayed to zero with $\\epsilon_i={1\\over i}$, $i$ is the epsiode number. Monte Carlo ControlHere is the algorithm of online Monte Carlo control: . The algorithm is first-visit online Monte Carlo control precisely and you can modify it to every-visit online Monte control easily. If $\\epsilon$-greedy strategy used in this algorithm is GLIE, then the Q-value derived from the algorithm will converge to the optimal Q-function. Tempooral Difference Methods for ControlThere are two methods of TD-style model-free control: on-policy and off-policy. We first introduce the on-policy method, called SARSA. SARSAHere is the algorithm: . SARSA stands for State, Action, Reward, next State, Action taken in next state. Because this algorithm updates the Q-value after it gets the tuple $(s,a,r,sâ€™,aâ€™)$, it is called SARSA. SARSA is an on-policy method because the actions $a$ and $aâ€™$ used in the update equation are both from the policy that is being followed at the time of the update. SARSA for finite-state and finite-action MDPâ€™s converges to the optimal action-value if the following conditions hold: The sequence of policies $\\pi$ from is GLIE The step-sizes $\\alpha_t$ satisfy the Robbins-Munro sequence such that: $\\sum^\\infty_{t=1}\\alpha_t=\\infty,\\ \\sum^\\infty_{t=1}\\alpha_t^2&lt;\\infty$ (although we generally donâ€™t use the step-sizes satisfy this condition in reality). Q-LearningHere is the algorithm: . The biggest different between Q-learning and SARSA is that, Q-learning takes a maximum over the actions at the next state, this action is not necessarily the same same as the one we would derive from the current policy. On the contrary, the agent will choose the action that brings the biggest reward directly and this behavior actually updates the policy because, when we adopt $\\epsilon$-greedy we definately introduce Q-value. Q-learning updates the Q-value (policy) after it gets the tuple $(s,a,r,sâ€™)$. And this is why it is called off-policy. However, in SARSA, as we stated before, the action $aâ€™$ derives from the current policy that has not been updated. The agent may choose a bad action $aâ€™$ randomly following the $\\epsilon$-greedy policy and this may lower the Q-value of some state-action pairs after the update. This consequently lead to the result that, SARSA might not figure out the optimal trajectory of the agent but the suboptimal one. Double Q-LearningIn Q-learning, the state values $V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$ can suffer from maximization bias (bias introduced by the maximization operation) when we have finitely many samples. Our state value estimate is at least as large as the true value of state $s$, so we are systematically overestimating the value of the state. In Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum. This is called double Q-learning which is shown below: . Double Q-learning can significantly speed up training time by eliminating suboptimal actions more quickly then normal Q-learning.","link":"/2020/02/16/RLSummary4/"},{"title":"åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“","text":"ç”±äºè‡ªå·±æ˜¯å»å¹´ä¸ƒæœˆé…ç½®å¥½çš„æœåŠ¡å™¨ï¼Œæœ‰ä¸€äº›ç»†èŠ‚æˆ–è€…é‡åˆ°çš„é—®é¢˜å·²ç»è®°ä¸å¤ªæ¸…ï¼Œæ•…æœ¬æ–‡å¯èƒ½ä¼šæœ‰ä¸å®Œæ•´çš„åœ°æ–¹ï¼Œé‡åˆ°é—®é¢˜è¯·å–„ç”¨æœç´¢å¼•æ“ï¼Œè€Œä¸”æœåŠ¡å™¨çš„é…ç½®æ–¹æ³•ä¹Ÿä¸åªæœ‰è¿™ä¸€ç§ã€‚æœ¬æ–‡ä¸»è¦ç”¨ä½œå¯¹è‡ªå·±æ“ä½œæ­¥éª¤å’Œæ–¹æ³•çš„ä¸€ä¸ªæ€»ç»“ï¼Œä»¥ä¾¿äºæ—¥åæŸ¥é˜…ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚ è´­ä¹°æœåŠ¡å™¨é¦–å…ˆå»åä¸ºäº‘å®˜ç½‘æ³¨å†Œä¸€ä¸ªè´¦å·ã€‚å¦‚æœæ˜¯å­¦ç”Ÿï¼Œå¯ä»¥æœç´¢â€œå­¦ç”Ÿâ€ï¼Œå¹¶è¿›è¡Œå­¦ç”Ÿè®¤è¯ã€‚å­¦ç”Ÿè®¤è¯çš„æ­¥éª¤å‚è§å­¦ç”Ÿè®¤è¯æµç¨‹ã€‚è¿›è¡Œèº«ä»½éªŒè¯åå¯ä»¥è´­ä¹°å­¦ç”Ÿä¼˜æƒ å¥—é¤ï¼Œäº‘æœåŠ¡å™¨ä»·æ ¼åªè¦99å…ƒ/å¹´ï¼Œæ¯”é˜¿é‡Œäº‘å’Œè…¾è®¯äº‘çš„éƒ½è¦ä¾¿å®œä¸€äº›ã€‚ è´­ä¹°å®Œæˆåï¼Œä½ å¯ä»¥åœ¨æ§åˆ¶å°çœ‹åˆ°è‡ªå·±ç°æœ‰çš„èµ„æºä»¥åŠè¿è¡Œæƒ…å†µã€‚ é…ç½®å®‰å…¨ç»„ å®‰å…¨ç»„æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„åˆ†ç»„ï¼Œä¸ºå…·æœ‰ç›¸åŒå®‰å…¨ä¿æŠ¤éœ€æ±‚å¹¶ç›¸äº’ä¿¡ä»»çš„äº‘æœåŠ¡å™¨æä¾›è®¿é—®ç­–ç•¥ã€‚å®‰å…¨ç»„åˆ›å»ºåï¼Œç”¨æˆ·å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­å®šä¹‰å„ç§è®¿é—®è§„åˆ™ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚ ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªç”¨æˆ·é»˜è®¤åˆ›å»ºä¸€ä¸ªé»˜è®¤å®‰å…¨ç»„ï¼Œé»˜è®¤å®‰å…¨ç»„çš„è§„åˆ™æ˜¯åœ¨å‡ºæ–¹å‘ä¸Šçš„æ•°æ®æŠ¥æ–‡å…¨éƒ¨æ”¾è¡Œï¼Œå…¥æ–¹å‘è®¿é—®å—é™ï¼Œå®‰å…¨ç»„å†…çš„äº‘æœåŠ¡å™¨æ— éœ€æ·»åŠ è§„åˆ™å³å¯äº’ç›¸è®¿é—®ã€‚é»˜è®¤å®‰å…¨ç»„å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚ å®‰å…¨ç»„åˆ›å»ºåï¼Œä½ å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­è®¾ç½®å‡ºæ–¹å‘ã€å…¥æ–¹å‘è§„åˆ™ï¼Œè¿™äº›è§„åˆ™ä¼šå¯¹å®‰å…¨ç»„å†…éƒ¨çš„äº‘æœåŠ¡å™¨å‡ºå…¥æ–¹å‘ç½‘ç»œæµé‡è¿›è¡Œè®¿é—®æ§åˆ¶ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚^1 åœ¨æ§åˆ¶å°ç‚¹å‡»â€œå¼¹æ€§äº‘æœåŠ¡å™¨ECSâ€ï¼Œåœ¨è¿™é‡Œä½ å¯çœ‹åˆ°ä½ çš„æœåŠ¡å™¨çš„å…¬ç½‘IPï¼Œè¯·è®°ä¸‹è¿™ä¸ªIPåœ°å€ã€‚ç„¶åç‚¹å‡»åœ¨åˆ—è¡¨ä¸­ç‚¹å‡»ä½ çš„æœåŠ¡å™¨çš„åç§°ã€‚ è¿›å…¥äº‘æœåŠ¡å™¨ç®¡ç†é¡µé¢åï¼Œç‚¹å‡»â€œå®‰å…¨ç»„â€ã€‚å†ç‚¹å‡»â€œSys-defaultâ€å¯ä»¥çœ‹åˆ°é»˜è®¤å®‰å…¨ç»„ã€‚ç„¶åä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯æˆ‘ç›®å‰çš„å®‰å…¨ç»„è®¾ç½®ï¼Œä»…ä¾›å‚è€ƒã€‚é€‰æ‹©â€œå…¥/å‡ºæ–¹å‘æ–¹å‘è§„åˆ™â€ï¼Œå†ç‚¹å‡»â€œæ·»åŠ è§„åˆ™â€œå³å¯æ‰‹åŠ¨æ·»åŠ è§„åˆ™ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œé…ç½®çš„éƒ½æ˜¯å…¥æ–¹å‘çš„å®‰å…¨ç»„ï¼Œå¹¶ä¸”æºåœ°å€ï¼ˆè®¿é—®æœåŠ¡å™¨çš„è®¾å¤‡çš„IPåœ°å€ï¼‰éƒ½ä¸ºâ€œ0.0.0.0/0â€ï¼ˆæ‰€æœ‰IPåœ°å€ï¼‰ã€‚ é€šå¸¸éœ€è¦é…ç½®å¦‚ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š SSHè¿œç¨‹è¿æ¥Linuxå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šSSHï¼Œç«¯å£ï¼š22ï¼‰ å…¬ç½‘â€œpingâ€ECSå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šICMPï¼Œç«¯å£ï¼šå…¨éƒ¨ï¼‰ å¼¹æ€§äº‘æœåŠ¡å™¨ä½œWebæœåŠ¡å™¨ åè®®ï¼šhttpï¼Œç«¯å£ï¼š80 åè®®ï¼šhttpsï¼Œç«¯å£ï¼š433 è¯¦ç»†é…ç½®è¯·å‚è€ƒå®‰å…¨ç»„é…ç½®ç¤ºä¾‹ã€‚ é…ç½®å®Œæˆåï¼Œå¯ä»¥æ‰“å¼€ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œç”¨ä¸‹é¢çš„è¯­å¥æµ‹è¯•ä¸€ä¸‹ï¼š ping ä½ çš„å…¬ç½‘IP å‡ºç°ç±»ä¼¼ä¸‹é¢çš„å†…å®¹å°±ä»£è¡¨æˆåŠŸäº†ï¼š ä½ å¯ä»¥æŒ‰ä¸‹Ctrl+Cæ¥ç»“æŸpingè¿™ä¸ªè¿›ç¨‹ã€‚ ç„¶ååœ¨ç»ˆç«¯é‡Œè¾“å…¥ï¼š ssh ä½ çš„å…¬ç½‘IP å¦‚æœä½ çš„å®‰å…¨ç»„é…ç½®æ­£ç¡®çš„è¯ï¼Œä¼šè®©ä½ è¾“å…¥æœåŠ¡å™¨çš„ç™»å½•å¯†ç ã€‚è¾“å…¥å¯†ç ï¼ˆæ³¨æ„ï¼šå¯†ç æ˜¯ä¸ä¼šæ˜¾ç¤ºçš„ï¼‰åå›è½¦ï¼Œåº”è¯¥å¯ä»¥çœ‹åˆ°è¿™æ ·çš„è¾“å‡ºï¼š è¿™ä¸ªæ—¶å€™ï¼Œä½ çš„ç»ˆç«¯å°±å·²ç»è¿æ¥ä¸Šäº†æœåŠ¡å™¨çš„ç³»ç»Ÿäº†ï¼Œä½ åœ¨ç»ˆç«¯é‡Œçš„ä¸€åˆ‡æ“ä½œéƒ½æ˜¯ä½œç”¨åœ¨æœåŠ¡å™¨ä¸Šçš„ã€‚ åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginxé¦–å…ˆè¯·åœ¨ç»ˆç«¯ä½¿ç”¨sshç™»å½•ä½ çš„æœåŠ¡å™¨ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢ç»™å‡ºçš„é¡ºåºè¾“å…¥å‘½ä»¤ã€‚ 12345678910yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #å®‰è£…ç¼–è¯‘å·¥å…·åŠåº“æ–‡ä»¶cd /usr/local/ #åˆ‡æ¢åˆ°ç›®æ ‡å®‰è£…æ–‡ä»¶å¤¹wget http://nginx.org/download/nginx-1.16.1.tar.gz #ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„Nginxtar -zxvf nginx-1.16.1.tar.gz #è§£å‹æ–‡ä»¶cd nginx-1.16.1 #è¿›å…¥è§£å‹çš„æ–‡ä»¶å¤¹./configure #æ‰§è¡Œç¨‹åºmake #ç¼–è¯‘make install #å®‰è£…cd /usr/local/nginx/sbin #è¿›å…¥Nginxå®‰è£…ç›®å½•./nginx #è¿è¡ŒNginx æ­¤æ—¶ï¼Œå®‰è£…åº”è¯¥å·²ç»å®Œæˆäº†ã€‚æ‰“å¼€æµè§ˆå™¨ï¼Œåœ¨åœ°å€æ ä¸­è¾“å…¥ä½ çš„å…¬ç½‘ipã€‚å¦‚æœçœ‹åˆ°ä¸‹å›¾æ‰€ç¤ºå†…å®¹ï¼Œå°±ä»£è¡¨å®‰è£…æˆåŠŸäº†ã€‚ åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸååœ¨æ‹¥æœ‰äº†è‡ªå·±çš„æœåŠ¡å™¨ä»¥åï¼Œå°±å¯ä»¥åšå¾ˆå¤šäº‹æƒ…äº†ã€‚ä½†æ˜¯ç°åœ¨ä½ åªèƒ½é€šè¿‡IPåœ°å€è®¿é—®è‡ªå·±çš„æœåŠ¡å™¨ï¼Œçœ‹èµ·æ¥æ€»æ˜¯æœ‰ç‚¹åˆ«æ‰­ã€‚å¦å¤–ï¼Œå¦‚æœä½ æƒ³è¦ç½‘ç«™æœ‰ä¸€å®šçš„å½±å“åŠ›çš„è¯ï¼Œä»…æœ‰IPåœ°å€ä¼šè®©äººå‡ ä¹æ‰¾ä¸åˆ°ä½ çš„ç½‘ç«™ï¼Œè€Œä¸”ä¹Ÿä¸ç¬¦åˆå›½å®¶æ³•å¾‹è§„å®šã€‚æ‰€ä»¥è¿˜æ˜¯å»ºè®®å¤§å®¶å¼„ä¸€ä¸ªè‡ªå·±çš„åŸŸåã€‚ ç°åœ¨å¸‚é¢ä¸Šçš„äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›åŸŸåæ³¨å†Œçš„æœåŠ¡ï¼Œç›´æ¥åœ¨ä½ çš„æœåŠ¡æä¾›å•†çš„å¹³å°ä¸Šé¢æ³¨å†Œå³å¯ã€‚ä¸‹é¢æˆ‘ç»§ç»­ç”¨åä¸ºäº‘çš„å¹³å°æ¼”ç¤ºã€‚ é¦–å…ˆåœ¨åä¸ºäº‘ç½‘ç«™é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œåŸŸåâ€ï¼Œæ‰“å¼€ç¬¬ä¸€ä¸ªé“¾æ¥â€œåŸŸåæ³¨å†ŒæœåŠ¡â€ã€‚ä¹Ÿå¯ä»¥ç›´æ¥ç‚¹å‡»è¿™é‡Œï¼šåŸŸåæ³¨å†ŒæœåŠ¡_åä¸ºäº‘ã€‚ ç„¶åä½ å¯ä»¥åœ¨ç½‘é¡µä¸­é€‰æ‹©ä½ çš„åŸŸåï¼Œå¸¸è§çš„å¦‚.comï¼Œ.cnï¼Œ.netç­‰ã€‚è¿™äº›åŸŸåä¼šç›¸å¯¹æ¯”è¾ƒè´µã€‚ä½œä¸ºå­¦ç”Ÿå…šï¼Œæˆ‘é€‰æ‹©ä¸€ä¸ªæœ€ä¾¿å®œçš„åŸŸå.topï¼Œåªéœ€è¦9å…ƒ/å¹´ã€‚ ç‚¹å‡»ä½ æƒ³è¦çš„åŸŸååï¼Œä¼šè·³è½¬åˆ°ä¸€ä¸ªæ–°çš„é¡µé¢ã€‚æ¥ä¸‹æ¥å†æ¬¡é€‰æ‹©ä½ è¦çš„åŸŸåï¼Œå¹¶ä¸”åœ¨â€œæŸ¥åŸŸåâ€çš„æœç´¢æ¡†å†…è¾“å…¥ä½ æƒ³è¦çš„åŸŸåï¼Œçœ‹çœ‹æ˜¯å¦å·²ç»è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨äº†å°±æ¢ä¸€ä¸ªã€‚è‹¥æ˜¾ç¤ºâ€œåŸŸåå¯æ³¨å†Œâ€ï¼Œå°±ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚ è´­ä¹°å®Œæˆåï¼Œä½ å°±æ‹¥æœ‰äº†è‡ªå·±åŸŸåäº†ï¼ å¤‡æ¡ˆ å¤‡æ¡ˆæ˜¯ä¸­å›½å¤§é™†çš„ä¸€é¡¹æ³•è§„ï¼Œä½¿ç”¨å¤§é™†èŠ‚ç‚¹æœåŠ¡å™¨æä¾›äº’è”ç½‘ä¿¡æ¯æœåŠ¡çš„ç”¨æˆ·ï¼Œéœ€è¦åœ¨æœåŠ¡å™¨æä¾›å•†å¤„æäº¤å¤‡æ¡ˆç”³è¯·ã€‚ æ ¹æ®å·¥ä¿¡éƒ¨ã€Šäº’è”ç½‘ä¿¡æ¯æœåŠ¡ç®¡ç†åŠæ³•ã€‹(å›½åŠ¡é™¢292å·ä»¤)å’Œå·¥ä¿¡éƒ¨ä»¤ç¬¬33å·ã€Šéç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å¤‡æ¡ˆç®¡ç†åŠæ³•ã€‹è§„å®šï¼Œå›½å®¶å¯¹ç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œè®¸å¯åˆ¶åº¦ï¼Œå¯¹éç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œå¤‡æ¡ˆåˆ¶åº¦ã€‚æœªå–å¾—è®¸å¯æˆ–è€…æœªå±¥è¡Œå¤‡æ¡ˆæ‰‹ç»­çš„ï¼Œä¸å¾—ä»äº‹äº’è”ç½‘ä¿¡æ¯æœåŠ¡ï¼Œå¦åˆ™å±è¿æ³•è¡Œä¸ºã€‚é€šä¿—æ¥è®²ï¼Œè¦å¼€åŠç½‘ç«™å¿…é¡»å…ˆåŠç†ç½‘ç«™å¤‡æ¡ˆï¼Œå¤‡æ¡ˆæˆåŠŸå¹¶è·å–é€šä¿¡ç®¡ç†å±€ä¸‹å‘çš„ICPå¤‡æ¡ˆå·åæ‰èƒ½å¼€é€šè®¿é—®ã€‚^2 è¿™ä¸€æ­¥ä¸å¤šè¯´äº†ï¼Œå…·ä½“æ­¥éª¤æ¯”è¾ƒç¹çï¼ŒèŠ±è´¹çš„æ—¶é—´ä¹Ÿæ¯”è¾ƒé•¿ï¼Œéœ€è¦ä¸€ä¸¤å‘¨ã€‚ç½‘ç«™ä¸Šæœ‰å¾ˆæ¸…æ™°çš„æ“ä½œæ–¹æ³•ï¼Œè¯·è‡ªè¡ŒæŸ¥é˜…ï¼Œæ ¹æ®æ­¥éª¤æ“ä½œå³å¯ã€‚éœ€è¦æ³¨æ„ä¸€ç‚¹çš„æ˜¯ï¼Œåœ¨å®¡æ ¸è¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ¥åˆ°æœåŠ¡æä¾›å•†æ‰“æ¥çš„ç”µè¯ï¼Œä¸è¦æ¼æ¥ã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢çš„å¤‡æ¡ˆæ“ä½œæ˜¯åœ¨å·¥ä¿¡éƒ¨å¤‡æ¡ˆçš„ã€‚å®Œæˆäº†åœ¨å·¥ä¿¡éƒ¨çš„å¤‡æ¡ˆä»¥åè¿˜éœ€è¦å…¬å®‰å¤‡æ¡ˆã€‚å…·ä½“æ“ä½œæ–¹æ³•ä¹Ÿè¯·è‡ªè¡ŒæŸ¥é˜…ã€‚ åŸŸåè§£æåœ¨å®Œæˆä¸€ç³»åˆ—ç¹ççš„å¤‡æ¡ˆæµç¨‹ä»¥åï¼Œä½ çš„ç½‘ç«™è¿˜ä¸å¯ä»¥é€šè¿‡åŸŸåè®¿é—®ã€‚åªæœ‰æŠŠä½ çš„åŸŸåè·ŸæœåŠ¡å™¨çš„IPåœ°å€ç»‘å®šåœ¨ä¸€èµ·ä¹‹åï¼Œå¹¶ä¸”åœ¨æœåŠ¡å™¨ä¸Šä¿®æ”¹äº†é…ç½®æ–‡ä»¶ä¹‹åæ‰å¯ä»¥ã€‚ é¦–å…ˆæ‰“å¼€ç®¡ç†æ§åˆ¶å°ï¼Œåœ¨æ§åˆ¶å°ä¸­é€‰æ‹©â€œåŸŸåæ³¨å†Œâ€ã€‚ç„¶ååœ¨ä¸‹é¢çš„é¡µé¢ä¸­ç‚¹å‡»â€œè§£æâ€ã€‚ ç‚¹å‡»ä½ çš„åŸŸåï¼Œæ˜¾ç¤ºå¦‚ä¸‹é¡µé¢ã€‚è¿™é‡Œæ˜¾ç¤ºçš„æ˜¯ä½ åŸŸåçš„è®°å½•é›†ï¼Œå‰ä¸¤ä¸ªè®°å½•é›†åº”è¯¥æ˜¯é¢„ç½®è®¾ç½®ï¼Œä¸å¯æš‚åœæœåŠ¡ã€‚ä½ å¯ä»¥åœ¨è¿™åŸºç¡€ä¸Šæ·»åŠ è‡ªå·±çš„è®°å½•é›†ã€‚ ç‚¹å‡»é¡µé¢å³ä¸Šè§’çº¢è‰²æŒ‰é’®ä»¥æ·»åŠ è®°å½•é›†ã€‚æ·»åŠ è®°å½•é›†çš„é…ç½®å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä¸‹å›¾ä¸­ç»™å‡ºçš„ä¾‹å­æ˜¯æ·»åŠ çš„â€œAâ€å‹è®°å½•é›†ï¼Œä¹Ÿå³é€šè¿‡example.comè®¿é—®ç½‘ç«™ã€‚è‹¥éœ€è¦é€šè¿‡www.example.comè®¿é—®ç½‘ç«™ï¼Œåˆ™éœ€è¦ä¸ºexample.comçš„å­åŸŸåæ·»åŠ â€œAâ€å‹è®°å½•é›†ã€‚å…·ä½“é…ç½®å‚è§ï¼šé…ç½®ç½‘ç«™è§£æ_åä¸ºäº‘ã€‚ç‚¹å‡»â€œç¡®å®šâ€ï¼Œå®Œæˆæ·»åŠ ã€‚ä½ å¯ä»¥é€šè¿‡ping ä½ çš„åŸŸåæ¥æµ‹è¯•ä½ æ·»åŠ çš„è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆäº†ã€‚ é…ç½®nginxæ‰“å¼€ä½ ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œè¾“å…¥å‘½ä»¤ï¼šssh ä½ çš„IPåœ°å€ï¼Œè¾“å…¥ä½ çš„æœåŠ¡å™¨çš„å¯†ç ã€‚ è¿›å…¥ä½ çš„nginxçš„å®‰è£…ç›®å½•ï¼šcd /usr/local/nginx/ã€‚ ä½¿ç”¨vimæ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ï¼švim ./conf/nginx.confã€‚ æŒ‰Iå¼€å§‹è¾“å…¥ã€‚ åœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š 123456server { listen 80; #ç›‘å¬ç«¯å£è®¾ä¸º 80 server_name example.com; #ç»‘å®šæ‚¨çš„åŸŸå index index.htm index.html; #æŒ‡å®šé»˜è®¤æ–‡ä»¶ root html; #æŒ‡å®šç½‘ç«™æ ¹ç›®å½•} ç„¶åæŒ‰escé€€å‡ºç¼–è¾‘ï¼Œå†æŒ‰Shift+zzä¿å­˜ã€‚ è¾“å…¥ï¼šcd ./sbinï¼Œåˆ‡æ¢æ–‡ä»¶å¤¹ã€‚ æ‰§è¡Œå‘½ä»¤ï¼šnginx -s relodï¼Œé‡å¯nginxæœåŠ¡ã€‚ è¿™æ—¶å€™å†å°è¯•ç”¨æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œåº”è¯¥ä¼šæ˜¾ç¤ºä¹‹å‰å‡ºç°è¿‡çš„â€œWelcome to nginx â€çš„é¡µé¢äº†ï¼ ç”³è¯·SSLè¯ä¹¦SSLè¯ä¹¦å¯ä»¥åœ¨æ•°æ®ä¼ è¾“çš„è¿‡ç¨‹ä¸­å¯¹å…¶è¿›è¡ŒåŠ å¯†å’Œéšè—ï¼Œå¯ä»¥æå¤§åœ°æé«˜æ•°æ®ä¼ è¾“çš„å®‰å…¨æ€§ã€‚æ‹¥æœ‰SSLè¯ä¹¦çš„ç½‘ç«™çš„è¯·æ±‚å¤´éƒ½æ˜¯httpsï¼Œå¹¶ä¸”åœ¨é“¾æ¥æ—è¾¹ä¼šå‡ºç°ä¸€æŠŠå°é”ã€‚ä½†æ˜¯ï¼ŒSSLè¯ä¹¦å¹¶ä¸æ˜¯æ‰€æœ‰ç½‘ç«™éƒ½å¿…é¡»çš„ï¼Œè¿™è§†ä½ çš„éœ€è¦è€Œå®šã€‚æ¯”å¦‚ï¼Œå¾®ä¿¡å°ç¨‹åºçš„æœåŠ¡å™¨å°±å¿…é¡»è¦æœ‰åŸŸåå’ŒSSLè¯ä¹¦ã€‚å¦å¤–ï¼Œå‡ºäºä¿¡æ¯ä¼ è¾“çš„å®‰å…¨æ€§æ–¹é¢çš„è€ƒè™‘ï¼Œæœ‰SSLè¯ä¹¦è¿˜æ˜¯æ˜¾å¾—æ›´ä¸ºå¦¥å½“å’Œä¸“ä¸šä¸€ç‚¹ã€‚ ç°åœ¨å¸‚é¢ä¸Šå„å¤§äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›é…å¥—çš„SSLè¯ä¹¦ç”³è¯·æœåŠ¡ï¼Œä¸€èˆ¬éƒ½æ˜¯æä¾›ä¼ä¸šçº§çš„è¯ä¹¦ï¼Œä»·æ ¼æ¯”è¾ƒæ˜‚è´µã€‚ä½†æ˜¯åŒæ—¶ç½‘ç»œä¸Šä¹Ÿæœ‰ä¸€äº›å…è´¹çš„SSLè¯ä¹¦æœåŠ¡å¯ä»¥é€‰æ‹©ã€‚ä¸‹é¢è¿˜æ˜¯ä»¥åä¸ºäº‘çš„å¹³å°ä¸ºä¾‹ï¼Œç®€å•è¯´æ˜ä¸€ä¸‹å¦‚ä½•ç”³è¯·SSLè¯ä¹¦ã€‚ é¦–å…ˆåœ¨åä¸ºäº‘é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œå…è´¹è¯ä¹¦â€œï¼Œç„¶åç‚¹å‡»äºšæ´²è¯šä¿¡åŸŸåå‹DVå•åŸŸåSSLè¯ä¹¦â€“å…è´¹è¯ä¹¦ï¼Œå¯ä»¥çœ‹åˆ°è¯ä¹¦çš„ä»·æ ¼æ˜¯0.00å…ƒã€‚ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚ å®Œæˆè´­ä¹°åè¯·ä¸è¦ç«‹å³å…³é—­é¡µé¢ï¼Œé¡µé¢ä¸­çš„è®¢å•å·åœ¨ä¹‹åè¿˜éœ€è¦ç”¨åˆ°ã€‚å°”åï¼Œç³»ç»Ÿä¼šå‘é€â€HuaweiCloudè´¦æˆ·ç”³è¯·â€é‚®ä»¶è‡³ç”¨æˆ·é‚®ç®±ï¼Œå³ä½ åœ¨åä¸ºäº‘çš„æ³¨å†Œé‚®ç®±ã€‚ ç‚¹å‡»é‚®ä»¶ä¸­çš„ç™»å½•åœ°å€è¿›å…¥ç³»ç»Ÿï¼Œå¹¶ä½¿ç”¨é‚®ä»¶æä¾›çš„è´¦å·å’Œåˆå§‹å¯†ç è¿›è¡Œç™»å½•ã€‚ç™»å…¥ç³»ç»Ÿåè¯·ä¿®æ”¹ä½ çš„åˆå§‹å¯†ç ï¼Œç„¶åè¯·æ ¹æ®åä¸ºäº‘ä¸­ç»™ä½ æä¾›çš„è®¢å•å·åœ¨è¯¥ç³»ç»Ÿä¸­æŸ¥è¯¢ä½ çš„è®¢å•ã€‚æŸ¥è¯¢åˆ°ä½ çš„è®¢å•ä»¥åï¼Œéœ€è¦ä½ è¡¥å……ä¸€äº›ä¿¡æ¯ï¼Œè¯·å¦‚å®å¡«å†™ã€‚ç³»ç»Ÿä¼šè¦ä½ å¡«å†™å…¬å¸ä¿¡æ¯ï¼Œå¦‚æœåªæ˜¯ä¸ªäººç½‘ç«™ï¼Œé‚£ä¹ˆå…¬å¸åç§°ç›´æ¥å¡«å†™ä½ çš„åå­—å³å¯ï¼Œå…¬å¸åœ°å€å°±å¡«å†™ä½ çš„ä½å€ã€‚ å¡«å†™å®Œæˆåä¼šè¿›å…¥å®¡æ ¸é˜¶æ®µï¼Œç³»ç»Ÿä¼šç»™ä½ å‘é€ä¸€å°é‚®ä»¶ã€‚ æ ¹æ®é‚®ä»¶çš„æç¤ºï¼Œéœ€è¦åœ¨è®°å½•é›†ä¸­æ·»åŠ æ–°çš„å†…å®¹ã€‚è¯·æ ¹æ®å‰æ–‡æ‰€è¿°æ–¹æ³•ï¼Œå°†é‚®ä»¶ä¸­çš„å†…å®¹æ·»åŠ è‡³æ–°çš„è®°å½•é›†ã€‚å¡«å†™æ–¹æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ å¡«å†™å®Œæˆåï¼Œå¯ä»¥åœ¨æœ¬åœ°ç”µè„‘çš„ç»ˆç«¯é‡Œè¾“å…¥nslookup -querytype=txt ä½ çš„åŸŸåæ¥æµ‹è¯•è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆã€‚ ä¸€èˆ¬æ¥è¯´ï¼Œè®°å½•é›†ç”Ÿæ•ˆå10åˆ†é’Ÿä»¥å†…è¯ä¹¦å°±ä¼šé¢å‘äº†ã€‚ SSLè¯ä¹¦éƒ¨ç½²æ¥ä¸‹æ¥æˆ‘ä»¬è¦æŠŠSSLè¯ä¹¦éƒ¨ç½²åˆ°æˆ‘ä»¬çš„æœåŠ¡å™¨ä¸Šã€‚ åœ¨æ”¶åˆ°çš„â€œè¯ä¹¦é¢å‘â€çš„é‚®ä»¶çš„åº•éƒ¨æœ‰ä¸€æ¡é“¾æ¥ï¼Œç‚¹å‡»è¿™æ¡é“¾æ¥ï¼Œè¿›å…¥è¯ä¹¦ç®¡ç†ç³»ç»Ÿã€‚ç™»å½•ç³»ç»Ÿï¼Œåœ¨å·¦ä¾§å¯¼èˆªæ ä¸­ç‚¹å‡»â€œSSLè¯ä¹¦â€ï¼Œå†ç‚¹å‡»â€œé¢„è§ˆâ€ï¼Œå†åœ¨å³ä¾§çš„â€œä¿¡æ¯é¢„è§ˆâ€ä¸­ç‚¹å‡»â€œä¸‹è½½æœ€æ–°è¯ä¹¦â€œã€‚ åœ¨å¼¹å‡ºçš„å¯¹è¯æ¡†å†…ï¼Œé€‰æ‹©è¯ä¹¦æ ¼å¼ä¸ºâ€œPEM(é€‚ç”¨äºNginx,SLB)â€ï¼Œè¾“å…¥ä½ çš„è®¢å•å¯†ç ã€‚è¯ä¹¦å¯†ç å¯ä»¥ç•™ç©ºã€‚ ä¸‹è½½å®Œæˆåï¼Œè§£å‹ä¸‹è½½çš„å‹ç¼©åŒ…ï¼Œéœ€è¦è¾“å…¥ä½ çš„è®¢å•å¯†ç ï¼ˆå¦‚æœä½ æ²¡æœ‰è®¾ç½®è¯ä¹¦å¯†ç ï¼‰ã€‚è§£å‹ä»¥åå¯ä»¥å¾—åˆ°ä¸‹å›¾ä¸¤ä¸ªæ–‡ä»¶ã€‚ æ¥ä¸‹æ¥ï¼Œæ‰“å¼€ä½ çš„ç»ˆç«¯ï¼ŒæŒ‰é¡ºåºè¾“å…¥ä¸‹åˆ—å‘½ä»¤ï¼š 123456ssh ä½ çš„å…¬ç½‘IP #sshç™»å½•ï¼Œè¾“å…¥ä½ çš„å¯†ç cd /usr/local/nginx #åˆ‡æ¢åˆ°nginxçš„å®‰è£…ç›®å½•mkdir ./cert #åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹certç”¨äºå­˜æ”¾ä½ çš„è¯ä¹¦exit #æ–­å¼€ä¸æœåŠ¡å™¨çš„è¿æ¥scp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.key ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.keyæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹scp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.crt ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.crtæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹ æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä¿®æ”¹nginxçš„é…ç½®æ–‡ä»¶ã€‚å‚è€ƒå‰æ–‡æ‰€è¿°æ–¹æ³•æ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ã€‚å…ˆå°†ä½ ä¹‹å‰æ’å…¥çš„å†…å®¹åˆ é™¤æˆ–è€…ä½¿ç”¨#æ³¨é‡Šæ‰ï¼Œç„¶ååœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š 1234567891011121314151617181920212223server { listen 443 ssl; server_name example.com; #ä½ è¯ä¹¦ç»‘å®šçš„åŸŸå; ssl_certificate /usr/local/nginx/cert/ä½ çš„åŸŸå.crt; ssl_certificate_key /usr/local/nginx/cert/ä½ çš„åŸŸå.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / { index index.htm index.html; #æŒ‡å®šé»˜è®¤æ–‡ä»¶ã€‚ root html; #æŒ‡å®šç½‘ç«™æ ¹ç›®å½•ã€‚ }}server { #å°†ä½ çš„80ç«¯å£é‡å®šå‘è‡³433ç«¯å£ï¼Œå³å¼ºåˆ¶ä½¿ç”¨httpsè®¿é—® listen 80; server_name; example.com; #ä½ çš„åŸŸå rewrite ^/(.*)$ https://example.com:443/$1 permanent;} å°†æ–‡ä»¶ä¿å­˜ä»¥åé‡å¯nginxæœåŠ¡ã€‚ é‡å¯ä»¥åä½ å¯èƒ½ä¼šé‡åˆ°è¿™æ ·çš„é—®é¢˜ï¼š**unknown directive â€œsslâ€ in /usr/local/nginx/conf/nginx.conf:121**ï¼Œè¿™æ˜¯å› ä¸ºä½ åœ¨å®‰è£…nginxæ—¶ï¼Œæ²¡æœ‰ç¼–è¯‘SSLæ¨¡å—ã€‚ä½ å¯ä»¥åœ¨ç»ˆç«¯é‡ŒæŒ‰ç…§ä¸‹è¿°æ­¥éª¤è§£å†³^ 3ï¼š 1234567cd ../nginx-1.16.1 #è¿›å…¥åˆ°nginxçš„æºç åŒ…çš„ç›®å½•ä¸‹./configure --with-http_ssl_module #å¸¦å‚æ•°æ‰§è¡Œç¨‹åºmake #ç¼–è¯‘cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #å¤‡ä»½æ—§çš„nginxcp ./objs/nginx /usr/local/nginx/sbin/ #ç„¶åå°†æ–°çš„nginxçš„ç¨‹åºå¤åˆ¶ä¸€ä»½cd /usr/local/nginx/sbin/ #åˆ‡æ¢åˆ°sbinç›®å½•./nginx -s reload #é‡å¯nginxæœåŠ¡ å¦‚æœé‡å¯æˆåŠŸçš„è¯ï¼Œæ‰“å¼€æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œè¿™æ—¶å€™åº”è¯¥å¯ä»¥åœ¨é“¾æ¥æ—è¾¹çœ‹åˆ°ä¸€ä¸ªå°é”äº†ï¼","link":"/2020/01/08/%E5%8D%8E%E4%B8%BA%E4%BA%91+nginx%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%E6%80%BB%E7%BB%93/"},{"title":"é»‘è‹¹æœå…¥é—¨å®Œå…¨æŒ‡å—","text":"å…³äºé»‘è‹¹æœæ¬¢è¿æ­¥å…¥é»‘è‹¹æœçš„ä¸–ç•Œï¼ä¼—æ‰€å‘¨çŸ¥ï¼ŒMacå› å…¶ç‹¬ç‰¹çš„macOSç³»ç»Ÿåœ¨ä¼—å¤šWindowsç”µè„‘ä¸­ç‹¬æ ‘ä¸€å¸œã€‚macOSå…·æœ‰è®¸å¤šä¸Windowsä¸åŒçš„ç‰¹æ€§å’Œä¼˜ç‚¹ï¼ˆå½“ç„¶ï¼Œä¹Ÿæœ‰ä¸è¶³ï¼‰ï¼Œè€Œä¸”æœ‰äº›è½¯ä»¶åœ¨macOSä¸Šçš„ä¼˜åŒ–ä¼šæ¯”Windowsæ›´å¥½æˆ–è€…åªæ”¯æŒmacOSå¹³å°ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆMacåœ¨å¸‚åœºä¸Šä¸€ç›´æœ‰ç€å¹¿æ³›çš„éœ€æ±‚çš„æ ¹æœ¬åŸå› â€”â€”å³macOSçš„ç‹¬ç‰¹æ€§ã€‚ç”±äºè‹¹æœçš„å°é—­æ€§ç­–ç•¥ï¼ŒmacOSåœ¨æ­£å¸¸æƒ…å†µä¸‹åªèƒ½å®‰è£…åœ¨Macä¸Šã€‚è€Œé»‘è‹¹æœçš„å‡ºç°ï¼Œç»™å¹¿å¤§å¯¹macOSæœ‰éœ€æ±‚çš„äººä»¬æä¾›äº†ä¸€ä¸ªæ–°çš„é€‰æ‹©â€”â€”ä½ å†ä¹Ÿä¸éœ€è¦ä¸ºäº†ä¸€ä¸ªç³»ç»Ÿå»è´­ä¹°åœ¨åŒç­‰ç¡¬ä»¶æˆ–æ€§èƒ½æ¡ä»¶ä¸‹ä»·æ ¼æ›´ä¸ºæ˜‚è´µçš„ç”µè„‘äº†ã€‚ é»‘è‹¹æœï¼Œæ„æ€å°±æ˜¯å®‰è£…æœ‰macOSçš„ï¼Œå¯ä»¥æ­£å¸¸å·¥ä½œçš„éMacçš„ç”µè„‘ï¼Œä¹Ÿå¯ä»¥æŒ‡ä¸ºéMacçš„ç”µè„‘å®‰è£…macOSçš„è¡Œä¸ºï¼Œäº¦å¯ä»¥æŒ‡å®‰è£…åœ¨éMacç”µè„‘ä¸Šçš„macOSã€‚å¯¹äºè¿™ä¸ªè¯çš„ç¡®åˆ‡å®šä¹‰è¿˜æ˜¯æ¨¡ç³Šä¸æ¸…çš„ï¼Œä¸è¿‡è¿™ä¸æ˜¯å…³é”®æ‰€åœ¨ã€‚ä¸é»‘è‹¹æœç›¸å¯¹ï¼Œç™½è‹¹æœçš„å«ä¹‰å°±éå¸¸æ˜æ˜¾äº†ï¼Œä¹Ÿå°±æ˜¯è‹¹æœçš„Macæˆ–è€…å®‰è£…åœ¨Macä¸Šçš„macOSã€‚ é»‘è‹¹æœçš„åŸç†å°±æ˜¯é€šè¿‡å¯¹ç”µè„‘ä¸»æ¿çš„ç ´è§£å’Œå¯¹ç³»ç»Ÿçš„æ¬ºéª—ï¼Œè®©macOSä»¥ä¸ºè¿™æ˜¯ä¸€å°Macï¼Œå†é€šè¿‡ä¸€ç³»åˆ—é©±åŠ¨å’Œè¡¥ä¸ä½¿å¾—è¿™å°ç”µè„‘å¯ä»¥åœ¨macOSä¸‹æ­£å¸¸è¿è¡Œã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼š å°†macOSå®‰è£…åœ¨éMacçš„ç”µè„‘ä¸Šæ˜¯è¿åè‹¹æœå…¬å¸çš„æ³•å¾‹æ¡æ¬¾çš„ï¼ æ‰€ä»¥å®‰è£…é»‘è‹¹æœæ˜¯å­˜åœ¨ä¸€å®šçš„æ³•å¾‹é£é™©çš„ï¼Œè¿™æœ‰å¯èƒ½ï¼ˆä½†æ˜¯éå¸¸éå¸¸ç½•è§ï¼‰å¯¼è‡´ä½ çš„AppleIDè¢«é”æ­»ã€‚ä½†æ˜¯ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œè‹¹æœå…¬å¸å¯¹è¿™ç§è¡Œä¸ºéƒ½æ˜¯çä¸€åªçœ¼é—­ä¸€åªçœ¼ã€‚åªæ˜¯éšç€é»‘è‹¹æœæ•°é‡ä¸Šçš„æ—¥ç›Šå¢é•¿ï¼Œä¸çŸ¥é“ä»€ä¹ˆæ—¶å€™ä¼šå¼•èµ·è‹¹æœå…¬å¸çš„é‡è§†å¹¶å¯¹æ­¤é‡‡å–æªæ–½ã€‚è€Œåœ¨å¦ä¸€æ–¹é¢ï¼Œå¦‚æœä½ ä½¿ç”¨é»‘è‹¹æœæ¥ç‰Ÿåˆ©çš„è¯ï¼Œæ€§è´¨å°±å®Œå…¨ä¸åŒäº†ï¼Œä½ æœ‰å¯èƒ½ä¼šå—åˆ°æ³•å¾‹çš„åˆ¶è£ã€‚ ç”±äºmacOSä»ä¸€å¼€å§‹å°±ä¸è¢«å…è®¸å®‰è£…åœ¨éMacçš„ç”µè„‘ä¸Šï¼Œå› æ­¤å®‰è£…é»‘è‹¹æœç»å¯¹ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹æƒ…ï¼Œå®ƒæ¶‰åŠåˆ°å¯¹ä¸»æ¿çš„ç ´è§£ï¼Œå¯¹ç¡¬ä»¶çš„é©±åŠ¨ï¼Œå¯¹ç³»ç»Ÿçš„æ¬ºéª—ï¼ŒåŒæ—¶ä¹Ÿä¼šäº§ç”Ÿå¾ˆå¤šå¥‡å¥‡æ€ªæ€ªçš„bugã€‚é»‘è‹¹æœæœ‰å¾ˆå¤šç¼ºç‚¹ï¼š ä¸å®Œç¾çš„é»‘è‹¹æœç›¸å¯¹äºç™½è‹¹æœä¸é‚£ä¹ˆç¨³å®š é»‘è‹¹æœåœ¨ç¡¬ä»¶å±‚é¢ä¸Šçš„ç¼ºå¤±å¯¼è‡´å¾ˆå¤šåŠŸèƒ½æ— æ³•å®ç°ï¼Œå¦‚Touch Barï¼ŒTouch IDï¼ŒåŠ›åº¦è§¦æ§æ¿ç­‰ å®‰è£…é»‘è‹¹æœä»éœ€è¦æ»¡è¶³ä¸€å®šçš„ç¡¬ä»¶æ¡ä»¶ï¼ŒæŸäº›å‹å·çš„ç¡¬ä»¶åœ¨é»‘è‹¹æœä¸‹æ˜¯æ— æ³•é©±åŠ¨çš„ å®‰è£…é»‘è‹¹æœè´¹æ—¶è´¹åŠ›ï¼Œç›¸å½“æŠ˜è…¾ æ—¢ç„¶é»‘è‹¹æœæœ‰é‚£ä¹ˆå¤šç¼ºç‚¹ï¼Œå¹¶ä¸”è¿˜æ˜¯éæ³•çš„è¡Œä¸ºï¼Œé‚£ä¸ºä»€ä¹ˆè¿˜æœ‰é‚£ä¹ˆå¤šäººåœ¨ä½¿ç”¨é»‘è‹¹æœå¹¶ä¸”äººæ•°è¿˜åœ¨æ—¥ç›Šå¢é•¿å‘¢ï¼Ÿå› ä¸ºé»‘è‹¹æœä¸åŒæ ·å®‰è£…æœ‰macOSçš„ç”µè„‘ç›¸æ¯”ï¼Œè¿˜æ˜¯æœ‰å…¶ä¼˜ç‚¹çš„ï¼š å®Œç¾çš„é»‘è‹¹æœåœ¨ä½¿ç”¨ä½“éªŒä¸ŠåŸºæœ¬ä¸è¾“ç»™Mac é»‘è‹¹æœåœ¨åŒç­‰ç¡¬ä»¶æˆ–æ€§èƒ½æ¡ä»¶ä¸‹æ¯”èµ·Macä¾¿å®œè®¸å¤š é»‘è‹¹æœçš„å®šåˆ¶æ€§å’Œå¯æ‰©å±•æ€§åœ¨æŸäº›æ–¹é¢æ¯”Macå¼ºå¤§è®¸å¤š ä»é»‘è‹¹æœçš„ä¼˜ç‚¹æ¥çœ‹ï¼Œå†ç»“åˆå®é™…æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä½¿ç”¨é»‘è‹¹æœçš„äººç¾¤å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š å¯¹macOSæœ‰åˆšéœ€ï¼Œä½†æ˜¯åˆä¸æƒ³èŠ±é’±/æ²¡é’±ä¹°Macçš„ï¼Œå¦‚æŸäº›å½±è§†ã€éŸ³ä¹å·¥ä½œè€… å¯¹macOSæœ‰åˆšéœ€ï¼Œä½†æ˜¯å—é™äºè‹¹æœå°é—­çš„ç”Ÿæ€ï¼Œåªèƒ½é€šè¿‡é»‘è‹¹æœçš„é«˜å¯æ‰©å±•æ€§æ¥æ»¡è¶³è‡ªå·±å¯¹ç¡¬ä»¶çš„éœ€æ±‚çš„ç‰¹å®šè¡Œä¸šä»ä¸šè€… å¯¹macOSæ²¡æœ‰åˆšéœ€ï¼Œå…·æœ‰åå›ç²¾ç¥çš„æå®¢ï¼Œä¸“é—¨ç ”ç©¶æ“ä½œç³»ç»Ÿå’Œç¡¬ä»¶çš„å·¥ç¨‹å¸ˆï¼Œé€šå¸¸è¿™ç±»äººä¹Ÿæœ‰ç™½è‹¹æœ å¯¹macOSæ²¡æœ‰åˆšéœ€ï¼Œåªæ˜¯æƒ³è¦ä½“éªŒmacOSæˆ–è‹¹æœå®Œæ•´ç”Ÿæ€å´åˆä¸æƒ³èŠ±é’±/æ²¡é’±è´­ä¹°Macçš„äºº è€Œåšä¸»ä½œä¸ºä¸€ä¸ªç©·å­¦ç”Ÿï¼Œå°±æ˜¯å±äºæœ€åä¸€ç±»çš„äººğŸ˜‚ã€‚æˆ‘æŠ˜è…¾é»‘è‹¹æœå·²ç»æœ‰1å¹´æ—¶é—´ï¼Œç°åœ¨è‡ªå·±åœ¨ç”¨çš„ç”µè„‘æ˜¯æƒ æ™®çš„Envy-13 ad024TUï¼Œè£…æœ‰Windowså’ŒmacOSä¸¤ä¸ªç³»ç»Ÿã€‚åšä¸»çš„é»‘è‹¹æœå·²ç»åŸºæœ¬å®Œç¾ï¼Œåœ¨ä½¿ç”¨ä½“éªŒä¸Šå·²ç»ä¸ç™½è‹¹æœç›¸å·®æ— å‡ ã€‚å…³äºæˆ‘çš„é»‘è‹¹æœçš„æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥å‚è€ƒæˆ‘çš„GitHubä»“åº“ï¼Œæˆ–è€…æˆ‘çš„å¦ä¸€ç¯‡åšå®¢ï¼Œåœ¨é‚£ç¯‡åšå®¢é‡Œæˆ‘ä¸»è¦æ€»ç»“äº†ç»™è‡ªå·±çš„ç”µè„‘å®‰è£…é»‘è‹¹æœæ—¶è¸©è¿‡çš„ä¸€äº›å‘ã€‚è€Œè¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯é’ˆå¯¹ç¬”è®°æœ¬ç”µè„‘ï¼Œè®©å¤§å®¶å¯¹é»‘è‹¹æœæœ‰ä¸€ä¸ªåˆæ­¥çš„äº†è§£ã€‚çœ‹å®Œè¿™ç¯‡æ–‡ç« ï¼Œä½ å°±åŸºæœ¬å…¥é—¨é»‘è‹¹æœäº†ã€‚ é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒé»‘è‹¹æœçš„åŸç†åœ¨è®¨è®ºè¿™ä¸ªé—®é¢˜ä»¥å‰ï¼Œæˆ‘ä»¬å…ˆè¦äº†è§£ä¸€ä¸‹ç”µè„‘æ˜¯æ€ä¹ˆå¯åŠ¨çš„ã€‚ é¦–å…ˆï¼Œåœ¨ä½ æŒ‰ä¸‹å¼€æœºé”®ä»¥åï¼Œç”µè„‘ä¸Šç”µï¼Œå„ç¡¬ä»¶è¿›å…¥äº†å¾…å‘½çŠ¶æ€ã€‚CPUï¼ˆCentral Processing Unitï¼Œä¸­å¤®å¤„ç†å™¨ï¼‰å¯åŠ¨ä»¥åï¼ŒæŒ‰ç…§å…¶åœ¨è®¾è®¡æ—¶å°±å›ºå®šå¥½çš„åŠŸèƒ½é€å‡ºäº†ç¬¬ä¸€æ¡æŒ‡ä»¤ï¼Œè¿™ä¸€æ¡æŒ‡ä»¤å°†ä¼šä½¿BIOSï¼ˆBasic Input/Output Systemï¼ŒåŸºæœ¬è¾“å…¥è¾“å‡ºç³»ç»Ÿï¼‰èŠ¯ç‰‡ä¸­è£…è½½çš„ç¨‹åºå¼€å§‹æ‰§è¡Œã€‚BIOSç¨‹åºå¯ä»¥å®ç°å¾ˆå¤šåŠŸèƒ½ï¼Œæ¯”å¦‚ç³»ç»Ÿè‡ªæ£€ï¼Œæä¾›ä¸­æ–­æœåŠ¡ç­‰ã€‚ä½†æ˜¯å®ƒæœ€ä¸»è¦çš„åŠŸèƒ½åˆ™æ˜¯å°†å­˜æ”¾äºç¡¬ç›˜å¼•å¯¼åŒºçš„æ“ä½œç³»ç»Ÿå¼•å¯¼ç¨‹åºï¼ˆBoot loaderï¼Œä¸‹æ–‡ç®€ç§°å¼•å¯¼ï¼‰è£…è½½å…¥å†…å­˜ï¼Œå†é€šè¿‡å¼•å¯¼å°†æ“ä½œç³»ç»Ÿè£…è½½è¿›å†…å­˜ã€‚ å½“ç„¶ï¼Œç°åœ¨å¸‚é¢ä¸Šæ–°å‘å”®çš„ç”µè„‘å¤§éƒ¨åˆ†éƒ½å·²ç»é‡‡ç”¨äº†ä¸€ç§æ›´æ–°çš„æ–¹å¼æ¥è£…è½½å¼•å¯¼ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„UEFIï¼ˆUnified Extensible Firmware Interfaceï¼Œç»Ÿä¸€å¯æ‰©éƒ¨ä»¶æ¥å£ï¼‰ã€‚UEFIä½œä¸ºä¸€ç§è¾ƒæ–°çš„æ–¹æ¡ˆï¼Œå®ƒå’ŒBIOSçš„åŒºåˆ«ä¸»è¦æ˜¯åœ¨å¯æ‰©å±•æ€§æ–¹é¢ã€‚ä½†æ˜¯é™¤äº†ä¸€äº›ç»†å¾®çš„å·®åˆ«ï¼Œå®ƒåœ¨æ•´ä¸ªå¯åŠ¨çš„æµç¨‹ä¸Šä¸BIOSåŸºæœ¬ç›¸åŒï¼Œä¸”æœ€ç»ˆç›®çš„éƒ½æ˜¯å°†å¼•å¯¼è£…è½½è¿›å†…å­˜å½“ä¸­ã€‚å¦å¤–åœ¨å¼€å‘è€…åœˆå­ä¸­ï¼ŒBIOSå’ŒUEFIä¹Ÿå¸¸å¸¸è¢«æ··ä¸ºä¸€è°ˆã€‚å› æ­¤å°½ç®¡ç°åœ¨çš„ä¸»æµæ˜¯é‡‡ç”¨æ›´å…ˆè¿›çš„UEFIï¼Œä½†åœ¨ä¸‹é¢çš„å™è¿°ä¸­æˆ‘è¿˜æ˜¯ä¼šä½¿ç”¨BIOSçš„æ¦‚å¿µã€‚è¿™å¹¶ä¸ä¼šç»™ç†è§£å¸¦æ¥å›°éš¾ï¼Œåªæ˜¯ä½ ä»¬éœ€è¦çŸ¥é“è¿™ä¸¤è€…æœ‰äº›è®¸å¾®å¦™çš„åŒºåˆ«å³å¯ã€‚ ä¹Ÿè®¸æœ‰äººä¼šé—®ï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨BIOSç›´æ¥å°†æ“ä½œç³»ç»Ÿè£…è½½è¿›å†…å­˜å‘¢ï¼Ÿé¦–å…ˆï¼Œå¦‚æœæœ‰å¤šä¸ªæ“ä½œç³»ç»Ÿï¼Œé‚£ä¹ˆä¸åŒçš„æ“ä½œç³»ç»Ÿçš„è£…è½½è¿‡ç¨‹ä¼šæœ‰æ‰€ä¸åŒã€‚å¦‚æœè¦è®©BIOSé€‚é…ä¸åŒçš„æ“ä½œç³»ç»Ÿï¼Œé‚£ä¹ˆä¼šå¯¼è‡´å®ƒçš„ä½“ç§¯è¿‡äºåºå¤§ï¼Œç³»ç»Ÿè¿‡äºå¤æ‚ï¼Œä¸åˆ©äºå®ƒçš„çš„ç¨³å®šã€‚å…¶æ¬¡å°±æ˜¯ï¼ŒBIOSæ˜¯å›ºå®šåœ¨BIOSèŠ¯ç‰‡ä¸­çš„ï¼Œä¸æ–¹ä¾¿ä¿®æ”¹ã€‚è¿™ä¹Ÿå¯¼è‡´äº†æˆ‘ä»¬éš¾ä»¥è®©BIOSå¯¹ä¸åŒçš„æ“ä½œç³»ç»Ÿåšé€‚é…ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¼•å¯¼æ¥å®Œæˆæ“ä½œç³»ç»ŸåŠ è½½çš„å·¥ä½œã€‚ å…·ä½“è€Œè¨€ï¼Œå¼•å¯¼éœ€è¦å®Œæˆçš„å·¥ä½œä¸»è¦æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š åˆå§‹åŒ–å…¶ä»–ç¡¬ä»¶è®¾å¤‡ï¼Œä¸ºç³»ç»Ÿæä¾›å¯è®¿é—®çš„è¡¨å’ŒæœåŠ¡ ä¸ºæ“ä½œç³»ç»Ÿåˆ†é…å†…å­˜ç©ºé—´ï¼Œå†å°†å®ƒåŠ è½½è¿›å†…å­˜å½“ä¸­ ä¸ºé«˜çº§è®¡ç®—æœºç¨‹åºè¯­è¨€æä¾›æ‰§è¡Œç¯å¢ƒ å°†æ§åˆ¶æƒç§»äº¤ç»™æ“ä½œç³»ç»Ÿ åœ¨æ­¤ä¹‹åï¼Œç³»ç»Ÿçš„å®Œæ•´çš„å¯åŠ¨è¿‡ç¨‹å°±ç»“æŸäº†ï¼Œæ“ä½œç³»ç»Ÿæ¥ç®¡äº†æ•´ä¸ªç”µè„‘ã€‚ç®€è€Œè¨€ä¹‹ï¼Œç”µè„‘çš„å¯åŠ¨è¿‡ç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºï¼šBIOS-&gt;Bootloder-&gt;OS(æ“ä½œç³»ç»Ÿ)ã€‚ å›åˆ°é»‘è‹¹æœä¸Šæ¥ã€‚æˆ‘ä»¬æƒ³è¦åœ¨ä¸€æ¬¾éMacçš„ç”µè„‘ä¸Šè¿è¡ŒmacOSï¼Œä¸æˆ‘ä»¬åœ¨ç”µè„‘ä¸Šè¿è¡ŒWindowsçš„æœ€å¤§åŒºåˆ«åœ¨å“ªå„¿ï¼Ÿå½“ç„¶æ˜¯æ“ä½œç³»ç»Ÿä¸åŒå•Šï¼ç”±äºmacOSä¸Windowsæ˜¯ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„æ“ä½œç³»ç»Ÿï¼Œå› æ­¤ä»–ä»¬å¯åŠ¨å’ŒåŠ è½½çš„è¿‡ç¨‹ä¹Ÿå®Œå…¨ä¸åŒã€‚æ‰€ä»¥æˆ‘ä»¬è‚¯å®šä¸å¯ä»¥ç”¨å¯åŠ¨Windowsçš„é‚£ä¸€å¥—æ–¹æ³•å»å¯åŠ¨macOSï¼Œè€Œå¿…é¡»è¦æœ‰ä¸“é—¨çš„é€‚åº”macOSçš„ä¸€å¥—å¯åŠ¨æ–¹æ³•ï¼ˆç¨‹åºï¼‰ã€‚ æˆ‘ä»¬æƒ³è¦å°†macOSåŠ è½½åˆ°æˆ‘ä»¬çš„å†…å­˜å½“ä¸­ï¼Œå°±è¦å¯¹å½“å‰æˆ‘ä»¬çš„å¯åŠ¨ç¨‹åºè¿›è¡Œä¿®æ”¹å’Œé€‚é…ã€‚å›é¡¾ä¸Šæ–‡æ‰€è¯´çš„ç”µè„‘çš„å¯åŠ¨è¿‡ç¨‹æˆ‘ä»¬å¯ä»¥å‘ç°ï¼ŒBIOSæ˜¯å›ºå®šåœ¨èŠ¯ç‰‡ä¸­çš„ï¼Œä¸æ˜“ä¿®æ”¹ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æ“ä½œçš„éƒ¨åˆ†å°±åªæœ‰å¼•å¯¼äº†ã€‚æ‰€ä»¥æˆ‘ä»¬è¦æ‰¾åˆ°åˆé€‚çš„å¼•å¯¼ç¨‹åºï¼Œä½¿å…¶å¯ä»¥å°†macOSæ­£ç¡®åœ°è£…è½½è¿›å†…å­˜ï¼Œå¹¶ç»™å®ƒæä¾›æ­£ç¡®çš„æœåŠ¡ï¼Œè®©å®ƒå¯ä»¥ä¸ç¡¬ä»¶æ­£å¸¸äº¤æµï¼Œæœ€ç»ˆä½¿å®ƒæ­£å¸¸è¿è¡Œã€‚ é€šè¿‡ä¸Šé¢çš„ä¸€ç•ªè®²è§£ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œå®‰è£…é»‘è‹¹æœçš„æ ¸å¿ƒå°±æ˜¯å¼•å¯¼ã€‚è€Œå®é™…ä¸Šï¼ŒæŠ˜è…¾é»‘è‹¹æœæŠ˜è…¾çš„ä¹Ÿä¸»è¦å°±æ˜¯å¼•å¯¼ã€‚è€Œç”±äºç™½è‹¹æœçš„ç¡¬ä»¶ï¼ŒBIOSï¼Œå’Œå¼•å¯¼éƒ½æ˜¯é’ˆå¯¹macOSå¼€å‘çš„ï¼Œæ‰€ä»¥å½“ç„¶ä¸è¦ä»»ä½•çš„æŠ˜è…¾ï¼Œå¼€ç®±å³ç”¨å°±è¡Œï¼ˆåºŸè¯â€¦â€¦ï¼‰ã€‚ ç›®å‰ä¸»æµçš„å¯ä»¥ç”¨äºåœ¨éMacçš„ç”µè„‘ä¸Šå¯åŠ¨macOSçš„å¼•å¯¼ä¸»è¦æœ‰ä¸¤ä¸ªï¼Œåˆ†åˆ«æ˜¯Cloverå’ŒOpenCoreï¼ˆä¸‹æ–‡ç®€ç§°OCï¼‰ã€‚ç”±äºOCæ˜¯æ–°å¼€å‘çš„å¼•å¯¼ï¼Œç›®å‰è¿˜åœ¨å…¬æµ‹é˜¶æ®µï¼Œè€Œä¸”å…¶åœ¨ç¤¾åŒºæ™®åŠç‡è¿œè¿œä¸å¦‚Cloverï¼Œæ‰€ä»¥ä¸‹é¢å°†ä¸»è¦è®²è§£Cloverï¼Œè€Œå¯¹äºOCåªä½œéå¸¸ç®€å•çš„ä»‹ç»ã€‚ Clover å¯åŠ¨å™¨çš„åå­—Cloverç”±ä¸€ä½åˆ›å»ºè€…kabylå‘½åã€‚ä»–å‘ç°äº†å››å¶è‰å’ŒMacé”®ç›˜ä¸ŠCommmandé”®ï¼ˆâŒ˜ï¼‰çš„ç›¸ä¼¼ä¹‹å¤„ï¼Œç”±æ­¤èµ·äº†Cloverè¿™ä¸ªåå­—ã€‚å››å¶è‰æ˜¯ä¸‰å¶è‰çš„ç¨€æœ‰å˜ç§ã€‚æ ¹æ®è¥¿æ–¹ä¼ ç»Ÿï¼Œå‘ç°è€…å››å¶è‰æ„å‘³çš„æ˜¯å¥½è¿ï¼Œå°¤å…¶æ˜¯å¶ç„¶å‘ç°çš„ï¼Œæ›´æ˜¯ç¥¥ç‘ä¹‹å…†ã€‚å¦å¤–ï¼Œç¬¬ä¸€ç‰‡å¶å­ä»£è¡¨ä¿¡ä»°ï¼Œç¬¬äºŒç‰‡å¶å­ä»£è¡¨å¸Œæœ›ï¼Œç¬¬ä¸‰ç‰‡å¶å­ä»£è¡¨çˆ±æƒ…ï¼Œç¬¬å››ç‰‡å¶å­ä»£è¡¨è¿æ°”ã€‚â€”â€”æ‘˜è‡ªç»´åŸºç™¾ç§‘ Cloveræ˜¯ä¸€ä¸ªæ“ä½œç³»ç»Ÿå¼•å¯¼ç¨‹åºï¼Œå¯ä»¥é€šè¿‡æ–°è€ä¸¤ç§æ–¹å¼è¿›è¡Œå¯åŠ¨ï¼Œä¹Ÿå°±æ˜¯BIOSæ–¹å¼å’ŒUEFIæ–¹å¼ã€‚ç›®å‰ä¸»æµçš„æ“ä½œç³»ç»Ÿéƒ½å·²ç»æ˜¯é€šè¿‡UEFIæ–¹å¼å¯åŠ¨çš„äº†ï¼Œå¦‚macOSï¼ŒWindows 7/8/10 (64-bit)ï¼ŒLinuxã€‚ æ‰€æœ‰çš„å¼•å¯¼éƒ½æ˜¯æ”¾åœ¨ç”µè„‘ç¡¬ç›˜å¼€å¤´éƒ¨åˆ†çš„å¼•å¯¼åŒºï¼ˆESPåˆ†åŒºï¼‰çš„EFIæ–‡ä»¶å¤¹ä¸­ï¼ŒCloverä¹Ÿä¸ä¾‹å¤–ã€‚å½“ç„¶ï¼ŒEFIæ–‡ä»¶ä¸­è¿˜å­˜æ”¾ç€Windowsï¼ŒLinuxï¼Œæˆ–è€…å…¶ä»–æ“ä½œç³»ç»Ÿçš„å¼•å¯¼ã€‚ä¸‹é¢å°±æ¥çœ‹çœ‹Cloverçš„æ–‡ä»¶ç»“æ„å§ã€‚ åœ¨Cloverä¸‹ä½¿ç”¨UEFIæ–¹å¼å¯åŠ¨çš„æµç¨‹æ˜¯è¿™æ ·çš„ï¼šUEFI-&gt;CLOVERX64.efi-&gt;OSã€‚ ä¸‹é¢æˆ‘å°†ä¸»è¦æ ¹æ®åœ¨å®é™…æ“ä½œä¸­ç”¨åˆ°çš„ä¸€äº›åŠŸèƒ½æ¥ä»‹ç»Cloverã€‚ è¿›å…¥æ“ä½œç³»ç»Ÿ è¿™ä¸€æ­¥éå¸¸ç®€å•ï¼Œå¼€æœºä¹‹åç”¨æ–¹å‘é”®é€‰æ‹©ä½ éœ€è¦è¿›å…¥çš„æ“ä½œç³»ç»Ÿçš„å·æ ‡ï¼ŒæŒ‰ä¸‹å›è½¦å³å¯ã€‚ æ˜¾ç¤ºå¸®åŠ© æŒ‰ä¸‹F1é”®ä¼šå‡ºç°å¸®åŠ©ä¿¡æ¯ã€‚ æ›´æ–°Clover è¯·åœ¨è¿™é‡Œä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„CLOVERX64.efiå¹¶ä½¿ç”¨å®ƒæ›¿æ¢æ‰ä½ çš„EFIæ–‡ä»¶å¤¹ä¸­çš„Cloveræ–‡ä»¶å¤¹ä¸­çš„åŒåæ–‡ä»¶ã€‚ å¼€å¯å•°å—¦æ¨¡å¼å¯åŠ¨ é¦–å…ˆæˆ‘è¦ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯å•°å—¦æ¨¡å¼ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å¯åŠ¨ç³»ç»Ÿçš„æ—¶å€™åªèƒ½çœ‹åˆ°ä¸€ä¸ªè¿›åº¦æ¡æˆ–è€…æ—‹è½¬çš„è¡¨ç¤ºåŠ è½½ä¸­çš„å›¾æ¡ˆã€‚è€Œå•°å—¦æ¨¡å¼å°±æ˜¯å°†ç³»ç»Ÿå¯åŠ¨æ—¶å„ç§è¯¦ç»†å‚æ•°å’Œæ—¥å¿—ä»¥åŠæŠ¥é”™æ¶ˆæ¯å…¨éƒ¨æ˜¾ç¤ºå‡ºæ¥çš„æ¨¡å¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¦‚æœå‘ç”Ÿäº†æ“ä½œç³»ç»Ÿå¯åŠ¨å¼‚å¸¸/å¤±è´¥çš„æƒ…å†µï¼Œé€šè¿‡å¼€å¯å•°å—¦æ¨¡å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿå®šä½åˆ°å‡ºé”™çš„ä½ç½®ã€‚ å¼€å¯å•°å—¦æ¨¡å¼çš„æ–¹æ³•å¾ˆç®€å•ã€‚é¦–å…ˆé€‰æ‹©ä½ æƒ³è¦è¿›å…¥çš„ç³»ç»Ÿçš„å›¾æ ‡ï¼ŒæŒ‰ç©ºæ ¼å³å¯è¿›å…¥ä¸‹å›¾æ‰€ç¤ºçš„é¡µé¢ï¼Œç„¶åå‹¾é€‰å›¾ç¤ºé€‰é¡¹ï¼Œå†é€‰æ‹©Boot macOS with selected optionså¯åŠ¨ã€‚ æ˜¾ç¤ºéšè—çš„å·æ ‡ æœ‰çš„æ—¶å€™åœ¨Cloverçš„å¯åŠ¨é¡µé¢ä¸­ä¼šå‡ºç°å¾ˆå¤šä»¥ä¸åŒæ–¹å¼å¯åŠ¨åŒä¸€ç³»ç»Ÿçš„å·æ ‡ï¼ˆVolumeï¼Œå¯ä»¥ç†è§£ä¸ºå…¥å£ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿®æ”¹Cloverçš„é…ç½®æ–‡ä»¶æ¥éšè—è¿™äº›å·æ ‡ï¼Œä½†æ˜¯æœ‰çš„æ—¶å€™ä½ åˆéœ€è¦å®ƒä»¬æ˜¾ç¤ºå‡ºæ¥ï¼ˆæ¯”å¦‚ä½ è¦é€šè¿‡è¿›å…¥Recoveryå·æ ‡æ¥å…³é—­macOSçš„ç³»ç»Ÿå®Œæ•´æ€§ä¿æŠ¤çš„æ—¶å€™ï¼‰ã€‚è¿™ä¸ªæ—¶å€™æˆ‘ä»¬ä¸å¿…é‡æ–°ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œåªéœ€è¦åœ¨Cloverçš„ä¸»ç•Œé¢æŒ‰ä¸‹F3ï¼Œå³å¯å°†éšè—çš„å·æ ‡æ˜¾ç¤ºå‡ºæ¥ã€‚ å…³äºæ€ä¹ˆéšè—å·æ ‡ï¼Œæˆ‘å°†åœ¨ä¸‹é¢ä»‹ç»ã€‚ æå–DSDT DSDTçš„å…¨ç§°ä¸º Differentiated System Description Tableï¼Œå®ƒæ˜¯ä¸€ä¸ªæè¿°ç³»ç»Ÿç¡¬ä»¶ä¸åŒä¿¡æ¯çš„è¡¨ï¼Œé€šè¿‡æŸ¥é˜…è¿™ä¸ªè¡¨ä¸­çš„ä¿¡æ¯å¯ä»¥çŸ¥é“ä½ çš„ç”µè„‘æœ‰ä»€ä¹ˆç¡¬ä»¶ï¼Œå®ƒä»¬çš„åç§°æ˜¯ä»€ä¹ˆã€‚çŸ¥é“è¿™äº›ä¿¡æ¯æœ‰åˆ©äºæˆ‘ä»¬ç†é¡ºç¡¬ä»¶ä¹‹é—´çš„å…³ç³»ï¼Œå†é€šè¿‡ä¿®æ”¹è¡¥ä¸æ›´æ­£ç¡¬ä»¶ä¿¡æ¯ï¼Œä»¥ä¼˜åŒ–æ“ä½œç³»ç»Ÿçš„å·¥ä½œçŠ¶å†µã€‚ åœ¨Cloverä¸»ç•Œé¢ä¸‹æŒ‰F4å³å¯å°†ä½ çš„DSDTä¿¡æ¯ä¿å­˜åˆ°EFI/CLOVER/ACPI/origin/æ–‡ä»¶å¤¹ä¸­ã€‚è¯·æ³¨æ„ï¼ŒDSDTæ˜¯ç”±å¤šä¸ªæ–‡ä»¶ç»„æˆçš„ã€‚ é€‰æ‹©ä½ æƒ³è¦å¯ç”¨/ç¦ç”¨çš„é©±åŠ¨ç¨‹åº é€šè¿‡CloveråŠ è½½çš„é©±åŠ¨ç¨‹åºä¿å­˜åœ¨EFI/CLOVER/kexts/Otherä¸­ï¼Œè¿™äº›é©±åŠ¨ç¨‹åºæ˜¯é’ˆå¯¹macOSç”Ÿæ•ˆçš„ã€‚åœ¨ä¸Šé¢æ‰€è¯´çš„é‚£ä¸ªæ–‡ä»¶å¤¹ä¸­åŒ…å«äº†å¾ˆå¤šä¸åŒçš„é©±åŠ¨æ–‡ä»¶ï¼Œæœ‰äº›é©±åŠ¨æ–‡ä»¶ä¹‹é—´ä¼šäº§ç”Ÿå†²çªï¼Œè€Œæœ‰äº›é©±åŠ¨æ–‡ä»¶åˆæ˜¯å®Œå…¨æ²¡æœ‰å¿…è¦å­˜åœ¨çš„ã€‚ä¸ºäº†ç®¡ç†å’Œç²¾ç®€ä½ çš„é©±åŠ¨ç¨‹åºï¼Œä½ å¯ä»¥åœ¨Cloverä¸­è®¾ç½®ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åºä»¥æ’æŸ¥å„ç§é©±åŠ¨çš„å·¥ä½œçŠ¶å†µã€‚ é¦–å…ˆä½ è¦é€‰æ‹©macOSçš„å›¾æ ‡ï¼ŒæŒ‰ä¸‹ç©ºæ ¼é”®ã€‚ç„¶ååœ¨æ–°çš„é¡µé¢ä¸­å°†å…‰æ ‡ç§»åŠ¨åˆ°Block injected kextsï¼ŒæŒ‰ä¸‹å›è½¦åè¿›å…¥è¯¥é€‰é¡¹ã€‚å†åœ¨æ–°çš„é¡µé¢ä¸­é€‰æ‹©Otheré€‰é¡¹ï¼Œè¿™ä¸ªæ—¶å€™ä½ å°±å¯ä»¥çœ‹åˆ°ä½ çš„é©±åŠ¨ç¨‹åºäº†ã€‚å‹¾é€‰ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åºä»¥åï¼ŒæŒ‰Escå›åˆ°ä¸»é¡µé¢ï¼Œå†ç›´æ¥å›è½¦è¿›å…¥macOSã€‚ è¯·æ³¨æ„ï¼Œä½ çš„è¿™ä¸€è®¾ç½®åªå¯¹è¿™ä¸€æ¬¡å¯åŠ¨æœ‰æ•ˆï¼Œåœ¨ä¹‹åçš„å¯åŠ¨ä¸­å°†ä¸ä¼šä¿ç•™ã€‚ è®¾ç½®Cloverï¼ˆä¿®æ”¹config.plistï¼‰ å¤šç§æ–¹æ³•è¿›è¡Œè®¾ç½®ã€‚ ä½ å¯ä»¥åœ¨å¼€æœºä»¥åçš„Cloverä¸»ç•Œé¢ä¸‹æŒ‰ä¸‹æŒ‰é”®Oè¿›å…¥è®¾ç½®é¡µé¢ï¼Œç„¶åä½ å°±å¯ä»¥é€‰æ‹©ä¸åŒçš„é€‰é¡¹å¼€å§‹ä¿®æ”¹ä½ çš„é…ç½®æ–‡ä»¶äº†ï¼Œä¸è¿‡ä¸€èˆ¬æƒ…å†µä¸‹æˆ‘ä»¬ä¸ä¼šä½¿ç”¨è¿™ç§æŠ½è±¡çš„æ–¹å¼æ¥ä¿®æ”¹ ä½¿ç”¨Clover Configuratoræ¥ä¿®æ”¹ Clover Configuratoræ˜¯ä¸€æ¬¾è¿è¡Œåœ¨macOSä¸‹çš„åº”ç”¨ç¨‹åºï¼Œä¸“é—¨ç”¨æ¥ä¿®æ”¹Cloverçš„é…ç½®æ–‡ä»¶ã€‚å®ƒå…·æœ‰å‹å¥½çš„å›¾å½¢åŒ–ç•Œé¢ï¼Œæ¯ä¸ªé€‰é¡¹éƒ½æœ‰æ¯”è¾ƒè¯¦ç»†çš„åŠŸèƒ½è¯´æ˜ï¼Œæ“ä½œèµ·æ¥æ¯”åœ¨å¯åŠ¨æ—¶ä¿®æ”¹è¦è½»æ¾å¾—å¤šã€‚Clover Configuratorçš„ä¸‹è½½é“¾æ¥æ”¾åœ¨æ–‡æœ«ã€‚ åœ¨è®¾ç½®ä»¥å‰ï¼Œä½ éœ€è¦åœ¨Clover Configuratorçš„æŒ‚è½½åˆ†åŒºé€‰é¡¹å¡ä¸­æŒ‚è½½ä½ EFIåˆ†åŒºï¼ˆé€šå¸¸æƒ…å†µä¸‹è¿™ä¸ªåˆ†åŒºéƒ½æ˜¯éšè—çš„ï¼‰ã€‚ç„¶ååœ¨ä½ çš„Cloveræ–‡ä»¶å¤¹ä¸‹ä½¿ç”¨Clover Configuratoræ‰“å¼€config.plistæ–‡ä»¶ï¼Œè¿›è¡Œä¿®æ”¹ã€‚ä¿®æ”¹å®Œæˆä»¥åï¼Œè¯·ç‚¹å‡»å·¦ä¸‹è§’çš„ä¿å­˜å›¾æ ‡ï¼ˆå›¾ä¸­ä»¥çº¢æ¡†æ ‡æ˜ï¼‰ã€‚ ä½ è¿˜å¯ä»¥ä½¿ç”¨æ™®é€šçš„æ–‡æœ¬æ–‡æ¡£ç¼–è¾‘å™¨ï¼ˆå¦‚Xcodeæˆ–è€…Visual Studio Codeï¼‰æ‰“å¼€config.plistå¯¹å…¶è¿›è¡Œç¼–è¾‘ï¼Œä½†æ˜¯è¿™ä¸ªæ–¹æ³•ä¾æ—§æ¯”è¾ƒæŠ½è±¡ï¼Œä¸æ¨èæ–°æ‰‹æˆ–è€…ä»£ç å°ç™½è¿™æ ·æ“ä½œ å¢åŠ /åˆ é™¤/ä¿®æ”¹/æŸ¥æ‰¾é©±åŠ¨ç¨‹åº åœ¨å¯åŠ¨ä»¥åï¼Œä½ å¯ä»¥ä½¿ç”¨Clover ConfiguratoræŒ‚è½½EFIåˆ†åŒºï¼Œç„¶åç›´æ¥ä½¿ç”¨è®¿è¾¾åœ¨é©±åŠ¨æ–‡ä»¶å¤¹ä¸­ä»¥å¯è§†åŒ–çš„æ–¹å¼ç®¡ç†ä½ çš„é©±åŠ¨ç¨‹åºã€‚ å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨Disk Geniusåœ¨Windowsä¸‹ç®¡ç†ä½ çš„é©±åŠ¨ç¨‹åºã€‚åœ¨ä¸‹ä¸€ç« èŠ‚ä¸­æœ‰å…³äºDisk Geniusçš„æ›´å¤šä»‹ç»ã€‚ æ›´æ¢Cloverçš„ä¸»é¢˜ Cloveræä¾›äº†å¾ˆå¤šè‡ªå®šä¹‰åŠŸèƒ½ï¼Œä½ å¯ä»¥é€‰æ‹©è‡ªå·±å–œæ¬¢çš„Cloverå¼€æœºä¸»é¢˜ã€‚Cloverçš„ä¸»é¢˜å­˜æ”¾åœ¨EFI/CLOVER/themes/æ–‡ä»¶å¤¹ä¸­ï¼Œä½ å¯ä»¥ä¸‹è½½ä½ å–œæ¬¢çš„ä¸»é¢˜æ–‡ä»¶å¤¹å¹¶å°†å…¶ä¿å­˜åˆ°ä¸Šè¿°è·¯å¾„ä¸­ã€‚ç„¶åï¼Œä½ éœ€è¦åœ¨Clover Configuratorä¸­çš„å¼•å¯¼ç•Œé¢é€‰é¡¹å¡ä¸­å¡«å†™ä½ æƒ³è¦è®¾ç½®çš„ä¸»é¢˜æ–‡ä»¶å¤¹çš„åå­—ï¼ˆå¦‚ä¸‹å›¾ï¼‰å¹¶ä¿å­˜ã€‚ ä½œè€…ç›®å‰ç”¨çš„æ˜¯ä¸€æ¬¾åä¸ºSimpleçš„ä¸»é¢˜ï¼Œå¯ä»¥ç‚¹å‡»æ­¤å¤„ä¸‹è½½ã€‚åœ¨GitHubä¸Šè¿˜æœ‰å¾ˆå¤šä¸åŒçš„Cloverä¸»é¢˜å¯ä¾›é€‰æ‹©ã€‚ éšè—ä½ ä¸éœ€è¦çš„å·æ ‡ å¦‚æœä½ çš„Cloverå¯åŠ¨ç•Œé¢æœ‰å¾ˆå¤šå¼•å¯¼åŒä¸€ç³»ç»Ÿçš„å·æ ‡ï¼Œä½ å¯ä»¥å°†ä»–ä»¬éšè—èµ·æ¥ã€‚å…·ä½“æ–¹æ³•æ˜¯ï¼ŒClover Configuratorä¸­çš„å¼•å¯¼ç•Œé¢é€‰é¡¹å¡ä¸­çš„éšè—å·ä¸€æ ä¸­å¡«å†™ä½ æƒ³è¦éšè—çš„å·æ ‡çš„åç§°ï¼Œç„¶åä¿å­˜æ–‡ä»¶ã€‚ Cloverçš„ä¸»è¦åŠŸèƒ½å°±ä»‹ç»åˆ°è¿™é‡Œäº†ã€‚ç”±äºæœ¬æ–‡æ˜¯çº¯ç²¹çš„æ–°æ‰‹å‘ï¼Œåœ¨è¿™é‡Œå°±ä¸ä»‹ç»å¦‚ä½•é…ç½®config.plistäº†ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåªè¦ä½ èƒ½å¤Ÿæ‰¾åˆ°å®Œå…¨å¯¹åº”ä½ æœºå‹çš„EFIæ–‡ä»¶ï¼ŒåŸºæœ¬ä¸Šå°±ä¸éœ€è¦å†é‡æ–°é…ç½®Cloveräº†ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å†ç®€å•ä»‹ç»ä¸€ä¸‹æ–°æ—¶ä»£çš„å¼•å¯¼å·¥å…·ï¼šOpenCoreã€‚ OpenCoreOpenCoreæ˜¯ä¸€ä¸ªç€çœ¼äºæœªæ¥çš„å…ˆè¿›çš„å¼€æºå¼•å¯¼å·¥å…·ï¼Œä»–æ”¯æŒå¤šç§ä¸»æµæ“ä½œç³»ç»Ÿçš„å¼•å¯¼ã€‚OCçš„å†å²ä½¿å‘½å°±æ˜¯æœ‰æœä¸€æ—¥ä»£æ›¿Cloverï¼Œæˆä¸ºä¸»æµã€‚OCä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªä¼˜åŠ¿ï¼š ä» 2019 å¹´ 9 æœˆä»¥å, Acidantheraï¼ˆç¥çº§å¤§ä½¬ï¼Œé»‘è‹¹æœç°æœ‰çš„å¤§éƒ¨åˆ†é©±åŠ¨ç›®å‰éƒ½æ˜¯ä»–åœ¨å¼€å‘ç®¡ç†ï¼‰å¼€å‘çš„å†…æ ¸é©±åŠ¨ ï¼ˆLilu, AppleALC ç­‰ï¼‰å°†ä¸å†ä¼šåœ¨ Clover ä¸Šåšå…¼å®¹æ€§æµ‹è¯•ï¼ˆè™½ç„¶è¿™ä¸èƒ½ç®—æ˜¯ä¼˜åŠ¿ï¼Œä½†æ˜¯å¾ˆå…³é”®å¥½å—ï¼ï¼‰ OCçš„å®‰å…¨æ€§æ›´å¥½ï¼Œå¯¹æ–‡ä»¶ä¿é™©ç®±ï¼ˆFileVaultï¼‰æœ‰æ›´å¼ºå¤§çš„æ”¯æŒ OCä½¿ç”¨æ›´å…ˆè¿›çš„æ–¹æ³•æ³¨å…¥ç¬¬ä¸‰æ–¹å†…æ ¸é©±åŠ¨ï¼ˆä¹Ÿå°±æ˜¯ä½ EFI/CLOVER/kexts/Otheré‡Œé¢çš„é‚£äº›kextæ–‡ä»¶ï¼‰ OCåœ¨å¯åŠ¨ä½“éªŒä¸Šä¼šæ›´åŠ æ¥è¿‘ç™½è‹¹æœ å½“ç„¶ï¼Œä¸ºä»€ä¹ˆç°åœ¨OCè¿˜æœªèƒ½æˆä¸ºä¸»æµï¼Œé¦–å…ˆæ˜¯å› ä¸ºå®ƒè¿˜å¤„äºå¼€å‘é˜¶æ®µï¼Œå„æ–¹é¢è¿˜æœªè¾¾åˆ°æœ€æˆç†Ÿçš„çŠ¶æ€ï¼›å…¶æ¬¡æ˜¯å› ä¸ºOCçš„é…ç½®ç›¸å¯¹äºCloverè¦å¤æ‚è®¸å¤šï¼Œè€Œä¸”ç›®å‰æ²¡æœ‰åƒClover Configuratorä¸€æ ·ç›´è§‚çš„å›¾å½¢åŒ–ç•Œé¢çš„é…ç½®å·¥å…·ï¼›æœ€åæ˜¯å› ä¸ºï¼ŒOCåœ¨ç¤¾åŒºä¸­æ™®åŠç¨‹åº¦ä¸é«˜ï¼Œå¯¼è‡´é‡åˆ°é—®é¢˜å¾ˆéš¾æ‰¾åˆ°ç°æˆçš„æ¡ˆä¾‹è§£å†³ã€‚è¿™äº›åŸå› ä½¿å¾ˆå¤šäººæ”¾å¼ƒäº†æŠ˜è…¾ã€‚ä½†æ˜¯å†å²çš„å‘å±•æ˜¯ä¸€ä¸ªèºæ—‹ä¸Šå‡çš„è¿‡ç¨‹ï¼Œæœªæ¥å°†ä¸€å®šæ˜¯OCçš„ï¼ï¼ˆç¬‘ï¼‰ é»‘è‹¹æœçš„åˆæ­¥å®‰è£…è®¨è®ºå®Œäº†é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒï¼Œä¸‹ä¸€æ­¥å°±è¯¥è®²è®²å¦‚ä½•å®‰è£…äº†ï¼ä½†æ˜¯è¯·å¤§å®¶æ³¨æ„ï¼Œå› ä¸ºè¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯é¢å‘æ–°æ‰‹çš„ï¼Œæ‰€ä»¥æˆ‘åªä¼šä»‹ç»ä¸€äº›æœ€æœ€åŸºæœ¬å’Œé€šç”¨çš„æ“ä½œï¼Œç›®çš„æ˜¯ä¸ºäº†è®©å¤§å®¶å…ˆæŠŠé»‘è‹¹æœè£…ä¸Šã€‚è€Œå®‰è£…å®Œæˆä»¥åçš„é‚£äº›å„ç§ä¼˜åŒ–çš„æ“ä½œï¼ŒåŒ…æ‹¬é…ç½®Cloverçš„é…ç½®æ–‡ä»¶ï¼Œç»™ç³»ç»Ÿæ‰“è¡¥ä¸ç­‰å®šåˆ¶æ€§æ¯”è¾ƒå¼ºçš„å†…å®¹ï¼Œéƒ½ä¸ä¼šåœ¨æœ¬æ–‡ä¸­æ¶‰åŠã€‚åšä¸»å¯èƒ½åœ¨æ¥ä¸‹æ¥ä¸€æ®µå¾ˆé•¿çš„æ—¶é—´å†…é™†é™†ç»­ç»­æ›´æ–°ä¸€äº›ç³»ç»Ÿä¼˜åŒ–çš„å†…å®¹ï¼Œæ•¬è¯·æœŸå¾…ï¼é—²è¯å°‘è¯´ï¼Œæˆ‘ä»¬å¼€å§‹å§ï¼ åˆ¶ä½œå®‰è£…ç›˜ä¸‹é¢çš„æ“ä½œå‡åœ¨Windowsç³»ç»Ÿä¸‹è¿›è¡Œã€‚ åœ¨é»‘æœå°å…µçš„éƒ¨è½æ ¼æŒ‰ç…§ä½ çš„éœ€è¦ä¸‹è½½æŸä¸ªç‰ˆæœ¬çš„ç³»ç»Ÿé•œåƒæ–‡ä»¶ï¼ˆåç¼€ä¸ºisoï¼‰ æ‰“å¼€WinMD5è½¯ä»¶ï¼Œå°†ä¸‹è½½å®Œæˆçš„isoé•œåƒæ–‡ä»¶æ‹–å…¥è½¯ä»¶çª—å£ï¼Œä¸ç½‘ç«™ä¸Šæä¾›çš„md5å€¼æ¯”å¯¹ï¼Œæ ¡éªŒmd5å€¼æ˜¯å¦æ­£ç¡®ï¼Œå¦‚ä¸æ­£ç¡®ï¼Œè¯·é‡æ–°ä¸‹è½½ï¼ˆmd5å€¼ç›¸å½“äºä¸€ä¸ªæ–‡ä»¶çš„èº«ä»½è¯å·ç ï¼Œå®ƒçš„å€¼æ˜¯å”¯ä¸€çš„ï¼Œå¦‚æœä½ ä¸‹è½½ä¸‹æ¥çš„æ–‡ä»¶çš„md5å€¼ä¸å®˜æ–¹æä¾›çš„ä¸ä¸€æ ·ï¼Œè¯´æ˜ä½ ä¸‹è½½çš„æ–‡ä»¶å¯èƒ½è¢«ä¿®æ”¹è¿‡æˆ–è€…å‡ºé”™äº†ï¼‰ æ‰¾åˆ°ä¸€ä¸ªå®¹é‡ä¸º16GBå¤§å°æˆ–ä»¥ä¸Šçš„ç©ºUç›˜ï¼Œæ’å…¥ç”µè„‘ ä»¥ç®¡ç†å‘˜èº«ä»½æ‰“å¼€TransMacè½¯ä»¶ï¼Œåœ¨çª—å£ä¸­å·¦ä¾§åˆ—è¡¨é¼ æ ‡å³å‡»ä½ çš„Uç›˜ï¼Œç‚¹å‡»Restore With Disk Image ç‚¹å‡»åæœ‰å¯èƒ½ä¼šå¼¹å‡ºä¸‹å›¾æ‰€ç¤ºçš„è­¦å‘Šï¼Œæ˜¯æç¤ºä½ çš„Uç›˜å¯èƒ½å«æœ‰å·²ç»æŒ‚è½½çš„å·ï¼Œè¯·ç¡®ä¿ä½ é€‰æ‹©çš„Uç›˜æ˜¯æ­£ç¡®çš„ï¼Œç„¶åç‚¹å‡»Yes åœ¨å¼¹å‡ºçš„çª—å£ä¸­é€‰æ‹©ä½ åˆšæ‰ä¸‹è½½å¥½çš„isoæ–‡ä»¶ï¼Œç‚¹å‡»OKï¼Œè¿™ä¸ªæ—¶å€™ä¼šæ ¼å¼åŒ–ä½ çš„Uç›˜å¹¶æŠŠç³»ç»Ÿé•œåƒçƒ§å½•åˆ°ä½ çš„Uç›˜ä¸­ï¼Œè€å¿ƒå®‰è£…ç›˜åˆ¶ä½œå®Œæˆå§ï¼Œè¿™ä¸€è¿‡ç¨‹å¤§çº¦è¦æŒç»­20~30åˆ†é’Ÿ åˆ¶ä½œå®Œæˆä»¥åä¼šå¼¹å‡ºå¯¹è¯æ¡†ï¼Œç›´æ¥ç‚¹å‡»OK åœ¨æ­¤ä¹‹åç³»ç»Ÿä¼šæç¤ºä½ è¦æ ¼å¼åŒ–Uç›˜ï¼Œä¸å¿…ç†ä¼šï¼Œç›´æ¥ç‚¹å‡»å–æ¶ˆ æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶å®‰è£…macOSæ—¶ï¼Œæˆ‘ä»¬è¿è¡Œçš„æ˜¯åœ¨Uç›˜ä¸Šçš„macOSå®‰è£…ç¨‹åºï¼Œè¿™ä¸€æ­¥ä¸è¿è¡ŒmacOSå…¶å®æ˜¯å·®ä¸å¤šçš„ã€‚æ­¤æ—¶æˆ‘ä»¬çš„Uç›˜å°±ç›¸å½“äºä¸€ä¸ªå¤–ç½®çš„ç³»ç»Ÿç›˜ï¼Œéœ€è¦é€šè¿‡ä½äºUç›˜ä¸Šçš„Cloverå¼•å¯¼æ¥å¯åŠ¨macOSå®‰è£…ç¨‹åºã€‚ ä¸ºäº†å¯ä»¥æ­£ç¡®å¼•å¯¼æ“ä½œç³»ç»Ÿï¼Œä¸åŒå‹å·ï¼Œç”šè‡³ä¸åŒæ‰¹æ¬¡çš„ç”µè„‘çš„EFIæ–‡ä»¶éƒ½æ˜¯ä¸å¤ªä¸€æ ·çš„ã€‚å› ä¸ºè¿™äº›ç”µè„‘ä¹‹é—´çš„ç¡¬ä»¶æœ‰æ‰€åŒºåˆ«ï¼Œæ‰€ä»¥ä½ éœ€è¦ç¡®ä¿ä½ çš„ç”µè„‘çš„EFIæ–‡ä»¶æ˜¯ä¸ä½ çš„ç”µè„‘ç¡¬ä»¶é€‚é…çš„ã€‚è¿™ä¸ªé—®é¢˜çš„åŸç†æˆ‘ä»¬å·²ç»åœ¨å‰é¢æåˆ°è¿‡äº†ã€‚ ä½†æ˜¯è¿™ä¸ªè½¯ç¡¬ä»¶é€‚é…çš„å·¥ä½œå¯¹äºå°ç™½æ¥è¯´æåº¦ä¸å‹å¥½ï¼Œå› ä¸ºè¿™éœ€è¦ä¸€éƒ¨åˆ†çš„æ•°å­—ç”µè·¯ï¼Œå¾®å‹è®¡ç®—æœºåŸç†ï¼Œä»¥åŠä»£ç ç¼–å†™çš„çŸ¥è¯†ã€‚é‚£æœ‰ä»€ä¹ˆåŠæ³•å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿç­”æ¡ˆå°±æ˜¯ï¼šâ€œæ‹¿æ¥ä¸»ä¹‰â€ã€‚å¤šäºäº†å¼€æºç¤¾åŒºçš„å‘å±•ï¼Œæœ‰è®¸å¤šäººåœ¨ç½‘ç«™ä¸Šå°†ä»–ä»¬å·²ç»å®Œå–„çš„EFIæ–‡ä»¶åˆ†äº«ç»™å…¶ä»–ä½¿ç”¨åŒä¸€å‹å·ç”µè„‘çš„äººã€‚æ‰€ä»¥ä½ ç°åœ¨è¦åšçš„å°±æ˜¯ï¼šæ‰¾åˆ°ä¸ä½ çš„ç”µè„‘å‹å·å¯¹åº”çš„EFIæ–‡ä»¶ï¼Œç„¶åä¸‹è½½ä¸‹æ¥ã€‚ dalianskyæ•´ç†äº†ä¸€ä¸ªæ¸…å•ï¼Œé‡Œé¢æ”¶é›†äº†å¤§é‡ä¸åŒæœºå‹çš„EFIæ–‡ä»¶ï¼Œä½ å¯ä»¥åœ¨é‡Œé¢æ‰¾æ‰¾æœ‰æ²¡æœ‰è‡ªå·±ç”µè„‘çš„å‹å·ï¼šHackintoshé»‘è‹¹æœé•¿æœŸç»´æŠ¤æœºå‹æ•´ç†æ¸…å•ã€‚å¦‚æœæœ‰çš„è¯ï¼Œç‚¹å‡»é“¾æ¥ï¼Œç„¶åå°†åˆ«äººæä¾›çš„è¿™ä¸ªEFIæ–‡ä»¶ä¸‹è½½ä¸‹æ¥å³å¯ã€‚ è¿™æ—¶æœ‰äººä¼šé—®äº†ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è‡ªå·±ç”µè„‘çš„å‹å·æ€ä¹ˆåŠå‘¢ï¼Ÿä¸è¦æ°”é¦ï¼Œä½ ä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨ä¸ä½ çš„ç”µè„‘ç¡¬ä»¶é…ç½®ç±»ä¼¼çš„å…¶ä»–æœºå‹çš„EFIæ–‡ä»¶ï¼Œæˆ–è€…ä½¿ç”¨dalianskyæä¾›çš„é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶ã€‚ æŒ‰ç…§dalianskyçš„å»ºè®®ï¼Œåœ¨å®‰è£…macOSæ—¶ä¸å¿…å°†é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶æ›¿æ¢ä¸ºå¯¹åº”è‡ªå·±æœºå‹çš„EFIæ–‡ä»¶ã€‚ä½†æ˜¯æˆ‘ä¸ªäººè®¤ä¸ºï¼Œå¦‚æœä½ å·²ç»æ‰¾åˆ°äº†ä¸ä½ çš„æœºå‹å¯¹åº”çš„EFIæ–‡ä»¶ï¼Œé‚£ä¹ˆåœ¨å®‰è£…ä¹‹å‰å°±å°†å…¶æ›´æ¢ï¼Œå¯èƒ½ä¼šåœ¨å®‰è£…è¿‡ç¨‹ä¸­é¿å…ä¸€äº›é”™è¯¯çš„å‘ç”Ÿã€‚ ä¸‹é¢å°±æ¥ä»‹ç»ä¸€ä¸‹å¦‚ä½•æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶å§ï¼ æ‰“å¼€DiskGeniusè½¯ä»¶ï¼Œåœ¨å·¦ä¾§åˆ—è¡¨ä¸­æ‰¾åˆ°ä½ å·²ç»åˆ¶ä½œå¥½çš„å®‰è£…ç›˜ï¼Œå¹¶å•å‡»é€‰ä¸­ ä¾æ¬¡åŒå‡»å³ä¾§åˆ—è¡¨ä¸­çš„ESP(0)å·æ ‡ï¼ŒEFIæ–‡ä»¶å¤¹ï¼Œè¿›å…¥å¦‚ä¸‹é¡µé¢ å•å‡»CLOVERæ–‡ä»¶å¤¹ï¼Œç„¶åæŒ‰deleteé”®ï¼Œå¼¹å‡ºå¯¹è¯æ¡†åç‚¹å‡»åˆ é™¤ï¼Œå°†è¿™ä¸ªæ–‡ä»¶å¤¹åˆ é™¤æ‰ é€‰ä¸­ä½ ä»åˆ«äººé‚£å„¿æ‹¿æ¥çš„EFIæ–‡ä»¶ä¸­çš„CLOVERæ–‡ä»¶å¤¹ï¼ŒæŒ‰ä¸‹Ctrl+Cåå°†çª—å£åˆ‡å›DiskGeniusï¼Œç„¶åå†æŒ‰ä¸‹Ctrl+Vå°†æ–°çš„CLOVERæ–‡ä»¶å¤¹å¤åˆ¶è¿›å»ï¼Œè¿™æ ·å°±å®Œæˆäº†EFIæ–‡ä»¶çš„æ›¿æ¢äº† ç»™ç¡¬ç›˜åˆ†åŒºæ¥ä¸‹æ¥æˆ‘ä»¬è¦åœ¨ç”µè„‘çš„ç¡¬ç›˜ä¸Šç»™å³å°†å®‰è£…çš„macOSåˆ†é…ä¸€å—è¶³å¤Ÿå¤§çš„ç©ºé—´ã€‚ ä»¥ä¸‹æ“ä½œå‡åœ¨Windowsä¸‹çš„DiskGeniusè½¯ä»¶ä¸­è¿›è¡Œï¼Œä¸”ä»¥æˆ‘çš„Uç›˜ä½œä¸ºç¤ºä¾‹ï¼Œæ“ä½œæ–¹æ³•ä¸åœ¨ç”µè„‘å†…ç½®ç¡¬ç›˜ä¸Šçš„ä¸€æ ·ã€‚åœ¨è¿›è¡Œä»¥ä¸‹æ“ä½œä¹‹å‰ï¼Œè¯·å…ˆå¤‡ä»½ä½ çš„æ–‡ä»¶ã€‚ æ‰“å¼€DiskGeniusè½¯ä»¶ï¼Œåœ¨å³ä¾§åˆ—è¡¨ä¸­é€‰ä¸­ä½ çš„ç¡¬ç›˜ï¼Œç„¶ååœ¨é¡¶éƒ¨æŸ¥çœ‹ä½ çš„ç¡¬ç›˜ç©ºé—´åˆ†é…æƒ…å†µï¼Œåœ¨é¡¶éƒ¨æœ€å·¦ä¾§æ‰¾åˆ°ä½ çš„EFIåˆ†åŒºï¼Œç¡®ä¿ä½ çš„EFIåˆ†åŒºçš„ç©ºé—´å¤§äº200MBï¼Œå¦åˆ™macOSå°†æ— æ³•å®‰è£… å³é”®å•å‡»ä½ çš„ç¡¬ç›˜ï¼Œé€‰æ‹©è½¬æ¢åˆ†åŒºè¡¨ç±»å‹ä¸ºGUIDæ¨¡å¼ï¼Œå¦åˆ™macOSå°†æ— æ³•å®‰è£…ï¼Œå¦‚æœè¿™ä¸ªé€‰é¡¹æ˜¯ç°è‰²çš„è€Œä¸‹ä¸€ä¸ªé€‰é¡¹å¯é€‰ï¼Œåˆ™æ— é¡»è½¬æ¢ å³é”®å•å‡»ä¸Šæ–¹çš„è“è‰²å®¹é‡æ¡ï¼Œç‚¹å‡»å»ºç«‹æ–°åˆ†åŒº åœ¨å¼¹å‡ºçš„çª—å£ä¸­è°ƒæ•´ä½ è¦åˆ†ç»™macOSçš„å®¹é‡å¤§å°ï¼Œç„¶åç‚¹å‡»å¼€å§‹ï¼Œæ¥ä¸‹æ¥ä¼šæœ‰å¼¹çª—å‡ºç°ï¼Œè¯·ä¸¥æ ¼éµå®ˆå¼¹çª—ä¸­ç»™å‡ºçš„è¦æ±‚æ“ä½œï¼Œä»¥å…å‘ç”Ÿæ„å¤–ï¼Œç„¶åç‚¹å‡»æ˜¯ï¼Œå¼€å§‹åˆ†åŒº åˆ†åŒºå®Œæˆä»¥åï¼Œå³é”®å•å‡»é¡¶éƒ¨è“è‰²å®¹é‡æ¡ï¼Œç‚¹å‡»åˆ é™¤å½“å‰åˆ†åŒºï¼ˆå› ä¸ºmacOSçš„ç£ç›˜æ ¼å¼ä¸ºAPFSï¼Œå› æ­¤ç°åœ¨å¯¹å…¶è¿›è¡Œæ ¼å¼åŒ–æ²¡æœ‰æ„ä¹‰ï¼‰ è®¾ç½®BIOSå‰æ–‡å·²ç»è¯´è¿‡ï¼Œæ“ä½œç³»ç»Ÿçš„å¯åŠ¨é¡ºåºæ˜¯UEFI/BIOS-&gt;CLOVERX64.efi-&gt;OSã€‚å› æ­¤ï¼Œä¸ºäº†ä½¿æˆ‘ä»¬çš„ç”µè„‘å¯ä»¥å¯åŠ¨å®‰è£…ç›˜ä¸Šçš„macOSå®‰è£…ç¨‹åºï¼Œæˆ‘ä»¬è¿˜éœ€è¦æ­£ç¡®è®¾ç½®æˆ‘ä»¬çš„BIOSã€‚ ç”±äºä¸åŒå“ç‰Œçš„ç”µè„‘ä½¿ç”¨ä¸åŒçš„ä¸»æ¿ï¼Œæ‰€ä»¥BIOSçš„è®¾ç½®ä»¥åŠè¿›è¡Œæ“ä½œçš„é”®ä½ä¹Ÿåƒå·®ä¸‡åˆ«ï¼Œè¿™é‡Œä»…ä»¥ä½œè€…çš„ç”µè„‘ä¸¾ä¾‹ã€‚ç”±äºä½œè€…ç”µè„‘çš„BIOSååˆ†åƒåœ¾ï¼Œå¯ä¾›è°ƒæ•´çš„é€‰é¡¹å¯¥å¯¥æ— å‡ ï¼Œå› æ­¤ä¸‹é¢æ‰€ç»™å‡ºçš„æ“ä½œæ­¥éª¤ä¸­çš„è®¾ç½®é…ç½®è¦æ±‚æ˜¯æœ€åŸºæœ¬çš„ã€‚å¦‚æœä½ çš„ç”µè„‘çš„BIOSåŠŸèƒ½è¶³å¤Ÿå¼ºå¤§ä¸”æœ‰å¾ˆå¤šå…¶ä»–çš„è®¾ç½®é€‰é¡¹çš„è¯ï¼Œè¯·å°½é‡å¼„æ‡‚è¿™äº›é€‰é¡¹çš„å«ä¹‰ï¼Œå¹¶æŒ‰ç…§éœ€è¦è¿›è¡Œè®¾ç½®ã€‚ å¼€æœºä»¥åï¼ŒæŒ‰F10è¿›å…¥BIOSè®¾ç½® æŒ‰æ–¹å‘é”®è¿›å…¥ç³»ç»Ÿè®¾ç½®èœå•ä¸­çš„å¯åŠ¨é€‰é¡¹ï¼Œè¯·å¼€å¯ä¼ ç»Ÿæ¨¡å¼ï¼Œç¦ç”¨å®‰å…¨å¯åŠ¨æ¨¡å¼ï¼Œå¯ç”¨USBå¯åŠ¨ æŒ‰F10ä¿å­˜è®¾ç½®ï¼Œç”µè„‘å°†è‡ªåŠ¨é‡å¯ ç°åœ¨BIOSä¹Ÿå·²ç»è®¾ç½®å®Œæˆã€‚åšå®Œè¿™äº›å‰æœŸå‡†å¤‡å·¥ä½œä»¥åï¼Œæ¥ä¸‹æ¥å°±è¦æ­£å¼å¼€å§‹å®‰è£…ç³»ç»Ÿäº†ï¼ å®‰è£…ç³»ç»Ÿä¸‹é¢ä»¥macOS 10.15.3çš„å®‰è£…è¿‡ç¨‹ä¸ºä¾‹ã€‚ é‡å¯ç”µè„‘ï¼Œçœ‹åˆ°å·¦ä¸‹è§’çš„æç¤ºä»¥åï¼ŒæŒ‰escæš‚åœå¯åŠ¨ è¿›å…¥å¯åŠ¨èœå•ï¼ŒæŒ‰F9è¿›å…¥å¯åŠ¨è®¾å¤‡é€‰é¡¹ åœ¨åˆ—å‡ºçš„ä¸€ä¸²å¼•å¯¼ä¸­ï¼Œé€‰æ‹©USBç¡¬ç›˜ï¼ˆUEFIï¼‰çš„é€‰é¡¹ä»¥å¯åŠ¨å®‰è£…ç›˜ä¸­çš„å¼•å¯¼ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯dalianskyæä¾›çš„è¾ƒæ–°çš„ç³»ç»Ÿé•œåƒï¼Œå®‰è£…ç›˜ä¸­ä¼šå‡ºç°ä¸¤ä¸ªå¼•å¯¼ï¼Œä¸€ä¸ªæ˜¯å¾®PEï¼ˆåé¢ä¼šæåˆ°ï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯Cloverï¼Œæˆ‘ä»¬éœ€è¦å¯åŠ¨çš„æ˜¯Clover è¿›å…¥Cloverç•Œé¢ä»¥åï¼ŒæŒ‰ç…§å‰æ–‡æ‰€è¯´è¿‡çš„æ–¹æ³•ï¼Œå¼€å¯å•°å—¦æ¨¡å¼ å¦‚æœä½ éœ€è¦ä½¿ç”¨é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶ï¼Œé‚£ä¹ˆè¯·æ‰§è¡Œä¸‹é¢çš„æ­¥éª¤ï¼Œå¦åˆ™ç›´æ¥è·³è¿‡ï¼š åœ¨Cloverä¸»ç•Œé¢æŒ‰Oè¿›å…¥é€‰é¡¹ï¼Œå…‰æ ‡ç§»åŠ¨åˆ°ConfigsåæŒ‰å›è½¦è¿›å…¥è¿›å…¥è¯¥é€‰é¡¹ï¼Œè¿™ä¸ªé€‰é¡¹æ˜¯ç”¨æ¥é€‰æ‹©éœ€è¦ç”Ÿæ•ˆçš„Cloveré…ç½®æ–‡ä»¶çš„ é€‰æ‹©config_Installè¿™ä¸ªé…ç½®æ–‡ä»¶ æŒ‰ä¸¤æ¬¡escè¿”å›åˆ°Cloverä¸»ç•Œé¢ åœ¨Cloverä¸»ç•Œé¢é€‰æ‹©å·æ ‡Boot macOS Install from Install macOS Catalinaï¼Œç„¶åæŒ‰ä¸‹å›è½¦ï¼Œå¼€å§‹å¼•å¯¼å®‰è£…ç¨‹åº è¿™ä¸ªæ—¶å€™ä¼šå‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºçš„å®‰è£…æ—¥å¿—ï¼Œå¦‚æœä½ å¾ˆä¸å¹¸åœ°å¡ä½äº†ï¼Œé‚£ä¹ˆä½ å¯ä»¥å‚è€ƒmacOS Catalina 10.15å®‰è£…ä¸­å¸¸è§çš„é—®é¢˜åŠè§£å†³æ–¹æ³•ï¼Œæˆ–è€…é™„ä¸Šä½ å¡ä½çš„åœ°æ–¹çš„ç…§ç‰‡å’Œä½ çš„ç”µè„‘é…ç½®ï¼Œåœ¨å„ç§äº¤ æµ ç¾¤ä¸­è¯¢é—®å¤§ä½¬ å¦‚æœæ²¡æœ‰å¡ä½ï¼Œä½ çš„æ—¥å¿—ä¼šæ¶ˆå¤±ï¼Œç„¶åå‡ºç°è‹¹æœçš„logoå’Œè¿›åº¦æ¡ ç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥åï¼Œä¼šå‡ºç°è¯­è¨€é€‰æ‹©ç•Œé¢ï¼Œè¯·é€‰æ‹©ä¸­æ–‡å¹¶ç‚¹å‡»ç»§ç»­ï¼Œå¦‚æœæœ‰è£…é€¼éœ€æ±‚æˆ–è€…æƒ³ç»ƒä¹ å¤–è¯­ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©å…¶ä»–è¯­è¨€ é€‰æ‹©ç£ç›˜å·¥å…·å¹¶ç‚¹å‡»ç»§ç»­ è¿›å…¥ç£ç›˜å·¥å…·ä»¥åï¼Œåœ¨å·¦ä¸Šè§’å³é”®ç‚¹å‡»ä½ çš„ç£ç›˜ï¼Œå¹¶é€‰æ‹©æ˜¾ç¤ºæ‰€æœ‰è®¾å¤‡ï¼Œå¹¶æ‰¾åˆ°ä½ ä¹‹å‰å·²ç»å‡†å¤‡å¥½å®‰è£…macOSçš„åˆ†åŒº é€‰ä¸­ä½ ä¹‹å‰å·²ç»å‡†å¤‡å¥½å®‰è£…macOSçš„åˆ†åŒºï¼Œç„¶åç‚¹å‡»æŠ¹æ‰ï¼Œåœ¨å¼¹å‡ºçš„çª—å£ä¸­ï¼Œä½ å¯ä»¥èµ·ä¸€ä¸ªåå­—ï¼Œå¹¶å°†æ ¼å¼è®¾ç½®æˆAPFSï¼Œå°†æ–¹æ¡ˆè®¾ç½®ä¸ºGUIDåˆ†åŒºå›¾ï¼Œå†ç‚¹å‡»æŠ¹æ‰ï¼Œè¿™ä¸€æ­¥ä¼šå°†ä½ ç”µè„‘ä¸Šçš„ç¡¬ç›˜åˆ†åŒºæ ¼å¼åŒ– æ“ä½œå®Œæˆä»¥åï¼Œç‚¹å‡»å·¦ä¸Šæ–¹ç£ç›˜å·¥å…·ï¼Œåœ¨å¼¹å‡ºçš„é€‰é¡¹ä¸­é€‰æ‹©é€€å‡ºç£ç›˜å·¥å…·å¹¶è¿”å›åˆ°å®‰è£…ç•Œé¢ åœ¨ä¸»ç•Œé¢é€‰æ‹©å®‰è£…macOSå¹¶ç‚¹å‡»ç»§ç»­ï¼Œå†é—­ç€çœ¼ç›åŒæ„æ¡æ¬¾ åœ¨ä¸‹å›¾æ‰€ç¤ºçš„ç•Œé¢ä¸­é€‰æ‹©ä½ è¦å®‰è£…çš„ç£ç›˜åˆ†åŒºï¼Œç„¶åç‚¹å‡»å®‰è£…ï¼Œæ¥ä¸‹æ¥å®‰è£…ç¨‹åºä¼šå°†å®‰è£…æ–‡ä»¶å¤åˆ¶åˆ°ä½ çš„åˆ†åŒºä¸­ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šæŒç»­å‡ åˆ†é’Ÿï¼Œå¾…å¤åˆ¶å®Œæˆä»¥åï¼Œç”µè„‘ä¼šé‡æ–°å¯åŠ¨ é‡å¯ä¹‹åï¼ŒæŒ‰ç…§æœ¬èŠ‚ä¸€å¼€å§‹æ‰€è¿°æ–¹æ³•è¿›å…¥Cloverï¼Œè¿™æ—¶å€™ä½ ä¼šå‘ç°ï¼ŒCloverä¸»ç•Œé¢ä¼šå¤šå‡ºæ¥å‡ ä¸ªå·æ ‡ï¼Œä»ç°åœ¨å¼€å§‹ç›´åˆ°å®‰è£…å®Œæˆï¼Œè¯·éƒ½é€‰æ‹©Boot macOS Install form xxxï¼ˆä½ ç»™ä½ çš„macOSåˆ†åŒºèµ·çš„åå­—ï¼‰å·æ ‡å¯åŠ¨ï¼Œåœ¨å®‰è£…è¿‡ç¨‹ä¸­è¯·è€å¿ƒç­‰å¾…ï¼Œæ— è®ºä½ åšäº†ä»€ä¹ˆå¥‡æ€ªçš„äº‹æƒ…è®©ä½ å¢åŠ äº†ä»€ä¹ˆå¥‡æ€ªçš„çŸ¥è¯†ï¼Œéƒ½ä¸è¦åœ¨å‡ºç°ç™½è‹¹æœlogoçš„æ—¶å€™ä¹±åŠ¨é¼ æ ‡æˆ–è€…é”®ç›˜ ç»è¿‡ä¸¤åˆ°ä¸‰æ¬¡é‡å¯ä»¥åï¼Œä½ ä¼šå‘ç°Boot macOS Install form xxxçš„å·æ ‡æ¶ˆå¤±äº†ï¼Œæ–°å‡ºç°äº†Boot macOS form xxxçš„å·æ ‡ï¼Œé€‰ä¸­å®ƒï¼Œç„¶åè¿›å…¥ï¼Œå†å¯¹ç€ç™½è‹¹æœç­‰å¾…å‡ åˆ†é’Ÿï¼Œéš¾å¾—éš¾å¾—ä¼‘æ¯æ—¶é—´é©¬ä¸Šå°±è¦ç»“æŸäº† è¿›åº¦æ¡èµ°å®Œï¼Œå‡ºç°è®¾ç½®å‘å¯¼ï¼Œæ¥ä¸‹æ¥ä¼šè®©ä½ è®¾ç½®ä½ çš„å›½å®¶å’Œåœ°åŒºï¼Œè¯­è¨€å’Œè¾“å…¥æ³•ï¼ŒæŒ‰ç…§ä½ çš„éœ€è¦è®¾ç½®å³å¯ï¼Œç„¶åä¼šè¿›å…¥æ•°æ®å’Œéšç§ç•Œé¢ï¼Œç‚¹å‡»ç»§ç»­ æ¥ä¸‹æ¥ä¼šé—®ä½ æ˜¯å¦éœ€è¦å°†macOSä»ä½ çš„å¤‡ä»½ä¸­æ¢å¤ï¼Œé»‘è‹¹æœç©å®¶ä¸€æ— æ‰€æœ‰ï¼Œé€‰æ‹©ç°åœ¨ä¸ä¼ è¾“ä»»ä½•ä¿¡æ¯å¹¶ç‚¹å‡»ç»§ç»­ æ¥ä¸‹æ¥è¦ä½ ä½¿ç”¨Apple IDç™»é™†ï¼Œè¿™é‡Œå…ˆè·³è¿‡ è¿˜æ˜¯é—­ç€çœ¼æ¥å—æ¡æ¬¾ æ¥ä¸‹æ¥ä½ éœ€è¦åˆ›å»ºä¸€ä¸ªç”µè„‘ç”¨æˆ·ï¼Œè¿™æ˜¯ä¸€ä¸ªç®¡ç†å‘˜å¸æˆ·ï¼Œè¯·æ³¨æ„ï¼Œåœ¨è¿™é‡Œè®¾ç½®äº†ç”¨æˆ·åä»¥åï¼Œå¦‚æœæœªæ¥è¦æ›´æ”¹çš„è¯ä¼šæä¸ºéº»çƒ¦ï¼Œå»ºè®®æƒ³æ¸…æ¥šäº†å†ç»§ç»­ä¸‹ä¸€æ­¥ è¿›å…¥å¿«æ·è®¾ç½®é¡µé¢ï¼Œç‚¹å‡»ç»§ç»­ï¼Œç„¶åä¼šè¿›å…¥åˆ†æé¡µé¢ï¼Œå–æ¶ˆå‹¾é€‰ä¸Appå¼€å‘å…±äº«å´©æºƒä¸ä½¿ç”¨æ•°æ®ï¼Œé»‘è‹¹æœè¿™ç§ä¸œè¥¿è‡ªå·±å·æ‘¸ç€ç”¨å°±è¡Œ æ¥ä¸‹æ¥è¿˜ä¼šè¦ä½ è®¾ç½®å±å¹•ä½¿ç”¨æ—¶é—´ï¼ŒSiriï¼Œä»¥åŠå¤–è§‚ï¼Œè¿™äº›é€‰é¡¹æŒ‰ç…§ä½ çš„éœ€è¦è®¾ç½®å°±è¡Œï¼Œä¸€è·¯ç»§ç»­ä¸‹å»ï¼Œç›´åˆ°å‡ºç°æ­£åœ¨è®¾ç½®ä½ çš„Macé¡µé¢ï¼Œè¯·ç¨ç­‰ç‰‡åˆ» ç»ˆäºè¿›å…¥äº†æ¡Œé¢ï¼Œè¿™æ—¶macOSçš„åŸºæœ¬å®‰è£…å·²ç»å®Œæˆäº†ï¼å…ˆåº†ç¥ä¸€ä¸‹ï¼ŒæŠ˜è…¾çš„äº‹æƒ…è¿˜åœ¨åå¤´å‘¢ï¼ˆè™½ç„¶è¿™ç¯‡æ–‡ç« ä¸ä¼šå†™å§â€¦â€¦ï¼‰ å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåºç°åœ¨ï¼ŒmacOSå·²ç»æˆåŠŸå®‰è£…åˆ°æˆ‘ä»¬ç”µè„‘çš„ç¡¬ç›˜ä¸Šäº†ï¼Œä½†æ˜¯æˆ‘ä»¬ç”µè„‘ç¡¬ç›˜ä¸Šçš„macOSè¿˜æ˜¯é€šè¿‡Uç›˜é‡Œçš„Cloverå¼•å¯¼çš„ã€‚è¿™å°±æ„å‘³ç€ï¼Œå¦‚æœæ‹”æ‰Uç›˜ï¼Œæˆ‘ä»¬å°†ä¸èƒ½å¤Ÿå¯åŠ¨macOSã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†Uç›˜å¼•å¯¼åŒºä¸­çš„Cloveræ–‡ä»¶å¤¹å¤åˆ¶åˆ°ç¡¬ç›˜å¼•å¯¼åŒºçš„EFIæ–‡ä»¶å¤¹ä¸­ï¼Œä»¥å®ç°è„±ç¦»Uç›˜å¯åŠ¨ã€‚è¿™ä¸€æ­¥çš„æ“ä½œä¸å‰æ–‡æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶è¿™ä¸€å°èŠ‚çš„æ“ä½œåŸºæœ¬æ˜¯ä¸€è‡´çš„ï¼Œéœ€è¦ä½ åœ¨Windowsç³»ç»Ÿä¸‹ä½¿ç”¨DiskGeniusæ“ä½œï¼Œè¿™é‡Œå°±ä¸å†èµ˜è¿°äº†ã€‚ å¦‚æœç°åœ¨é‡å¯ç”µè„‘ï¼Œä½ è¿˜æ˜¯ä¼šå‘ç°ç›´æ¥è¿›å…¥äº†Windowsçš„å¼•å¯¼è€Œä¸æ˜¯Cloverã€‚è¿™æ˜¯å› ä¸ºé™¤äº†Cloverä¹‹å¤–ï¼Œç”µè„‘å½“ç„¶è¿˜æœ‰è®¸å¤šå…¶ä»–çš„å¼•å¯¼é¡¹ï¼Œè¿™äº›å¼•å¯¼é¡¹æŒ‰é¡ºåºæ’åˆ—åœ¨å¯åŠ¨åºåˆ—ä¹‹ä¸­ã€‚ç°åœ¨æˆ‘ä»¬åªæ˜¯æŠŠCloverçš„æ–‡ä»¶å¤¹æ”¾å…¥äº†ç¡¬ç›˜çš„å¼•å¯¼åŒºä¸­ï¼Œä½†æ˜¯è¿˜æ²¡æœ‰æŠŠCloveræ·»åŠ åˆ°å¯åŠ¨åºåˆ—ä¹‹ä¸­ã€‚ç”µè„‘ä¸çŸ¥é“è‡ªå·±å±…ç„¶è¿˜å¯ä»¥ç”¨Cloverå¼•å¯¼macOSï¼Œåªèƒ½ç»§ç»­ç”¨è€ä¸€å¥—æ–¹æ³•ç›´æ¥å¼•å¯¼Windowså¯åŠ¨äº†ã€‚é‚£ä¹ˆä¸‹é¢æˆ‘ä»¬å°±è¦å‘Šè¯‰ç”µè„‘ï¼Œè®©å®ƒçŸ¥é“è‡ªå·±å¯ä»¥ä½¿ç”¨Cloverå¼•å¯¼æ“ä½œç³»ç»Ÿã€‚ æ‰“å¼€EasyUEFIè½¯ä»¶ï¼Œä½ å¯ä»¥çœ‹åˆ°æ‰€æœ‰çš„å¼•å¯¼é¡¹ä¹‹ä¸­æ²¡æœ‰Cloverï¼Œç‚¹å‡»çº¢æ¡†ä¸­æŒ‰é’®åˆ›å»ºæ–°çš„å¼•å¯¼é¡¹ åœ¨å¼¹å‡ºçš„çª—å£ä¸­ï¼Œç±»å‹é€‰æ‹©Linuxæˆ–è€…å…¶å®ƒæ“ä½œç³»ç»Ÿï¼Œæè¿°å¯ä»¥éšä¾¿å¡«å†™ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯CLOVERï¼Œç›®æ ‡åˆ†åŒºé€‰æ‹©ç£ç›˜0çš„ESPåˆ†åŒºï¼ˆå”¯ä¸€å¯é€‰çš„é‚£ä¸€ä¸ªï¼‰ åœ¨æ–‡ä»¶è·¯å¾„ä¸€è¡Œä¸­ï¼Œç‚¹å‡»æµè§ˆï¼Œåœ¨å¼¹å‡ºçš„çª—å£ä¸­æ˜¾ç¤ºäº†ä¸€ä¸ªç¡¬ç›˜çš„å›¾æ ‡ï¼Œè¿™ä¸ªå°±æ˜¯ä½ ç”µè„‘ä¸Šç¡¬ç›˜çš„ESPåˆ†åŒºäº†ï¼Œç‚¹å‡»å®ƒå·¦ä¾§çš„åŠ å·å°†å…¶å±•å¼€ï¼Œåœ¨EFIæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°CLOVERX64.efiï¼Œè¿™ä¸ªå°±æ˜¯Cloverçš„å¼•å¯¼æ–‡ä»¶ï¼Œé€‰ä¸­åç‚¹å‡»ç¡®å®š å›åˆ°åŸå…ˆçš„ç•Œé¢ä¹‹åï¼Œç‚¹å‡»ç¡®å®šï¼Œå¯ä»¥å‘ç°Cloverå·²ç»æ·»åŠ åˆ°å¯åŠ¨åºåˆ—ä¸­äº† åˆ°è¿™é‡Œè¿˜æ²¡ç»“æŸï¼Œå› ä¸ºCloverè¢«ä¸Šé¢ä¼—å¤šå¼•å¯¼é¡¹å‹ç€ï¼Œå¯åŠ¨çš„æ—¶å€™æ€ä¹ˆä¹Ÿè½®ä¸åˆ°å®ƒï¼Œå› æ­¤æˆ‘ä»¬ç‚¹å‡»çº¢æ¡†ä¸­çš„æŒ‰é’®ï¼Œå°†Cloverç§»åˆ°å¯åŠ¨åºåˆ—çš„ç¬¬ä¸€ä½ï¼Œä½¿ç”µè„‘å¼€æœºçš„æ—¶å€™é»˜è®¤ä½¿ç”¨Cloverå¼•å¯¼æ“ä½œç³»ç»Ÿ ç°åœ¨å†é‡å¯ç”µè„‘ï¼Œä¸è¦æŒ‰escæš‚åœå¯åŠ¨ï¼Œç”µè„‘ä¼šé»˜è®¤ä½¿ç”¨Cloverè¿›è¡Œå¼•å¯¼ã€‚é€‰æ‹©macOSåˆ†å·ï¼ŒæŒ‰å›è½¦è¿›å…¥ã€‚å¦‚æœæˆåŠŸå¯åŠ¨äº†ï¼Œé‚£ä¹ˆä½ ä¾¿å¯ä»¥é‡æ–°è®¾ç½®ä½ çš„BIOSï¼Œå°†ä¼ ç»Ÿæ¨¡å¼å…³é—­äº†ï¼ˆä½†ä¸è¦å¼€å¯å®‰å…¨å¯åŠ¨æ¨¡å¼ï¼‰ã€‚ åˆ°è¿™é‡Œï¼ŒmacOSçš„å‰æœŸå®‰è£…å·²ç»æ­£å¼å®Œæˆï¼å¤¸èµä¸€æ³¢è‡ªå·±å§ï¼ é»‘è‹¹æœå•ç³»ç»Ÿå®‰è£…æŒ‰ç…§ä¸Šé¢æ‰€è¯´çš„æ­¥éª¤ï¼Œå¦‚æœä¸å‡ºé—®é¢˜ï¼Œä½ ä¾¿åœ¨ç”µè„‘ä¸ŠæˆåŠŸå®‰è£…äº†Windowså’ŒmacOSåŒç³»ç»Ÿã€‚å¦‚æœä½ åªéœ€è¦macOSçš„å•ç³»ç»Ÿï¼Œæ“ä½œæ­¥éª¤ä¸ä¸Šé¢æ‰€è¯´æœ‰äº›è®¸ä¸åŒï¼Œä½†æ˜¯ç»å¤§éƒ¨åˆ†æ­¥éª¤æ˜¯ä¸€æ ·çš„ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äºç»™ç£ç›˜åˆ†åŒºå’Œå°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåºè¿™ä¸¤éƒ¨æ­¥ã€‚å¦‚æœä½ åœ¨åˆ¶ä½œå®‰è£…ç›˜çš„æ—¶å€™ï¼Œä¸‹è½½çš„æ˜¯dalianskyæä¾›çš„è¾ƒæ–°ç³»ç»Ÿç‰ˆæœ¬çš„é•œåƒï¼Œæˆ–è€…ä½ åœ¨åˆ¶ä½œå®Œç³»ç»Ÿå¯åŠ¨Uç›˜ä»¥åï¼Œåœ¨æ­¤ç”µè„‘ä¸­å¯ä»¥çœ‹åˆ°æœ‰è¯¸å¦‚å¾®PEå­—æ ·çš„ç£ç›˜ï¼Œé‚£ä¹ˆä¸‹é¢æ­¥éª¤ä¸­çš„å‰ä¸‰æ­¥å¯ä»¥çœç•¥æ‰ã€‚å¤§è‡´çš„æ“ä½œæ–¹æ³•å¦‚ä¸‹ï¼š äºå®˜ç½‘ä¸‹è½½å¾®PEå·¥å…·ç®±V2.0 64ä½ç‰ˆæœ¬ æ‰“å¼€è½¯ä»¶ï¼Œå°†å¾®PEå·¥å…·å®‰è£…åˆ°ä½ çš„å·²ç»åˆ¶ä½œå¥½çš„macOSå®‰è£…ç›˜ä¸­ å°†DiskGeniuså’ŒUEFIManageræ‹·è´åˆ°å¾®PEçš„æ–‡ä»¶ç›˜ä¸­ï¼ˆå¾®PEç³»ç»Ÿä¸­æœ¬èº«è‡ªå¸¦éä¸“ä¸šç‰ˆçš„DiskGeniusï¼ŒæŸäº›åŠŸèƒ½æœ‰ç¼ºå¤±ï¼‰ è®¾ç½®BIOS é‡å¯ï¼Œåœ¨BIOSä¸­ä½¿ç”¨å®‰è£…ç›˜ä¸­å¾®PEçš„å¼•å¯¼å¯åŠ¨ è¿›å…¥ç³»ç»Ÿåä½ å¯ä»¥å‘ç°ç•Œé¢ä¸Windows10å‡ ä¹ä¸€æ ·ï¼Œè¿è¡Œä½ å­˜æ”¾åœ¨Uç›˜ä¸­çš„DiskGeniusï¼Œåˆ é™¤ä½ ç¡¬ç›˜ä¸­Windowsä½¿ç”¨çš„åˆ†åŒºï¼Œå¹¶åˆ é™¤ç¡¬ç›˜EFIåˆ†åŒºçš„Windowsæ–‡ä»¶å¤¹ å°†ç¡¬ç›˜åˆ†åŒºè¡¨ç±»å‹è½¬æ¢ä¸ºGUIDæ ¼å¼ æŒ‰ç…§ä½ çš„éœ€è¦ä»¥åŠå‰æ–‡æ‰€è¿°è¦æ±‚ï¼Œé‡æ–°åˆ†é…ä½ çš„ç¡¬ç›˜åˆ†åŒºï¼Œå¹¶å°†ä»–ä»¬æ ¼å¼åŒ– æ¥ä¸‹æ¥å°±æ˜¯å®‰è£…ç³»ç»Ÿäº†ï¼Œå¦‚æœä¸€åˆ‡é¡ºåˆ©è¿›å…¥äº†macOSçš„æ¡Œé¢ï¼Œä½ å¯ä»¥ç»§ç»­ä¸‹é¢çš„æ­¥éª¤ é‡å¯ï¼Œä½¿ç”¨å®‰è£…ç›˜ä¸­å¾®PEçš„å¼•å¯¼å¯åŠ¨ è¿è¡ŒDiskGeniusï¼Œå°†å®‰è£…ç›˜EFIæ–‡ä»¶å¤¹ä¸­CLOVERæ–‡ä»¶å¤¹å¤åˆ¶åˆ°ç”µè„‘ç¡¬ç›˜çš„EFIæ–‡ä»¶å¤¹ä¸­ è¿è¡ŒUEFIManagerï¼Œç„¶åå‚è€ƒä¸Šæ–‡æ‰€è¯´çš„æ–¹æ³•ï¼Œæ·»åŠ å¹¶è°ƒæ•´ä½ çš„å¼•å¯¼é¡¹ å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œå…³é—­BIOSçš„ä¼ ç»Ÿæ¨¡å¼å¯åŠ¨ å¤§åŠŸå‘Šæˆï¼ å®‰è£…å®Œæˆåå¯èƒ½å‡ºç°çš„é—®é¢˜å®ŒæˆmacOSçš„å®‰è£…å¹¶ä¸ä»£è¡¨ä½ çš„ç”µè„‘å°±å·²ç»æ˜¯å¯å ªé‡ç”¨çš„ç”Ÿäº§åŠ›/å¨±ä¹å·¥å…·äº†ã€‚ç»å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œåˆšåˆšå®Œæˆå®‰è£…çš„é»‘è‹¹æœè¿˜ä¼šå­˜åœ¨ç€å„ç§å„æ ·çš„é—®é¢˜ã€‚åŠæ—¶ä½ ä½¿ç”¨çš„æ˜¯å®Œå…¨å¯¹åº”ä½ çš„ç”µè„‘å‹å·çš„EFIæ–‡ä»¶ï¼Œä¾ç„¶æœ‰å¤§æ¦‚ç‡ä¼šå‡ºç°è¿™äº›é—®é¢˜ã€‚é»‘è‹¹æœçš„æŠ˜è…¾ä¹‹å¤„ä¸æ˜¯å®‰è£…macOSçš„è¿‡ç¨‹ï¼Œå®Œå…¨è§£å†³è¿™äº›é—®é¢˜çš„è¿‡ç¨‹æ‰æ˜¯ã€‚æ‰€ä»¥è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘å»ºè®®å¤§å®¶ä¸è¦åœ¨å®‰è£…çš„æœ€åå‡ æ­¥ï¼ˆåŒ…æ‹¬å®Œæˆå®‰è£…ä»¥åï¼‰ç™»é™†ä½ çš„è‹¹æœæœåŠ¡ï¼Œå› ä¸ºä½ çš„ç”µè„‘å­˜åœ¨çš„ä¸€äº›é—®é¢˜ä¼šå¯¼è‡´è‹¹æœæœåŠ¡ç™»ä¸ä¸Šå»ï¼Œè€Œä¸”æŠ˜è…¾çš„è¿‡ç¨‹ä¹Ÿæœ‰å¯èƒ½æŠŠä½ çš„Apple IDä¸­çš„ä¿¡æ¯æä¹±ï¼Œå°±åƒä¸‹å›¾ä¸€æ ·ã€‚ å®‰è£…å®Œæˆä»¥åï¼Œå¤§å®¶å¯ä»¥æ£€æŸ¥ä¸€ä¸‹è‡ªå·±çš„ç”µè„‘æœ‰æ²¡æœ‰å‡ºç°ä¸‹é¢åˆ—å‡ºçš„è¿™äº›é—®é¢˜ã€‚ä¸‹é¢çš„æ£€æŸ¥å¤§éƒ¨åˆ†éƒ½åœ¨macOSçš„è®¾ç½®ä¸­å®Œæˆï¼Œè¿˜æœ‰ä¸€äº›ç›´æ¥è§‚å¯Ÿå³å¯ã€‚åœ¨æ¯ä¸ªé—®é¢˜çš„æœ«å°¾éƒ½ä¼šç»™å¤§å®¶æä¾›ä¸€äº›è§£å†³é—®é¢˜çš„æ€è€ƒæ–¹å‘ï¼Œä½†å¹¶ä¸ä¼šæä¾›å…·ä½“çš„è§£å†³åŠæ³•ã€‚å¦å¤–è¿˜é™„ä¸Šæ­£å¸¸æ•ˆæœçš„ç¤ºæ„å›¾ä¾›å¤§å®¶å‚è€ƒã€‚ ç½‘ç»œä¸è“ç‰™çš„é—®é¢˜ï¼šä¸‹é¢çš„è¿™äº›é—®é¢˜ä¸ä½ çš„ç½‘å¡çš„å‹å·æˆ–è€…é©±åŠ¨æœ‰å…³ æ‰“å¼€ç³»ç»Ÿåå¥½è®¾ç½®-ç½‘ç»œé€‰é¡¹ï¼Œé‡Œé¢æ²¡æœ‰æœ‰Wi-Fié€‰é¡¹ï¼Œå³ä½¿æœ‰ä¹Ÿæ‰“ä¸å¼€Wi-Fi æ‰“å¼€ç³»ç»Ÿåå¥½è®¾ç½®-è“ç‰™é€‰é¡¹ï¼Œæ— æ³•å¼€å¯è“ç‰™ æ— æ³•ä½¿ç”¨éšèˆª æ— æ³•ä½¿ç”¨Siriï¼ŒFaceTimeï¼ŒiMessage å£°éŸ³çš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜çš„è¡¨ç°å½¢å¼å¾ˆå¤šï¼Œå‡ºç°è¿™äº›é—®é¢˜æ˜¯å› ä¸ºå£°å¡æ²¡æœ‰é©±åŠ¨ æ‰“å¼€ç³»ç»Ÿç³»ç»Ÿåå¥½è®¾ç½®-å£°éŸ³é€‰é¡¹ï¼Œæ— æ³•è°ƒèŠ‚éŸ³é‡ å‹¾é€‰å½“æ›´æ”¹éŸ³é‡æ—¶æ’­æ”¾åé¦ˆå†è°ƒèŠ‚éŸ³é‡ï¼Œç”µè„‘æ²¡æœ‰å£°éŸ³ éº¦å…‹é£æ²¡æœ‰è¾“å…¥ç”µå¹³çš„å˜åŒ– ä½¿ç”¨å¿«æ·é”®è°ƒèŠ‚éŸ³é‡ï¼Œå–‡å­å›¾æ ‡ä¸‹å‡ºç°ç¦è¡Œæ ‡å¿— è§¦æ§æ¿çš„é—®é¢˜ï¼šè§¦æ§æ¿æ ¹æœ¬æ²¡æœ‰ååº”ï¼Œæˆ–è€…åœ¨ç³»ç»Ÿåå¥½è®¾ç½®-è§¦æ§æ¿é€‰é¡¹ä¸­æŸäº›æ‰‹åŠ¿æ— æ³•ä½¿ç”¨ï¼Œæˆ–è€…æŸäº›åŠŸèƒ½ä¸æ˜¾ç¤ºï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„è§¦æ§æ¿é©±åŠ¨æœ‰å…³ æ˜¾ç¤ºçš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜ä¹Ÿæ¶‰åŠåˆ°å¾ˆå¤šæ–¹é¢ï¼Œæ³¨æ„ä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯é”™è¯¯ç¤ºä¾‹ï¼Œä¸æ˜¯æ­£ç¡®çš„æ‰“å¼€æ–¹å¼ è‰²åä¸¥é‡ï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„æ˜¾ç¤ºå™¨æè¿°æ–‡ä»¶å’ŒEDIDæœ‰å…³ æ–‡å­—æ˜¾ç¤ºè¿‡å°ï¼Œå›¾æ ‡ä¸æ–‡å­—æ¯”ä¾‹å¤±è°ƒï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„EDIDä»¥åŠæ˜¯å¦å¼€å¯äº†HiDPIæœ‰å…³ å‡ºç°é¢œè‰²æ–­å±‚ï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„EDIDå’Œæ˜¾å¡ç¼“å†²å¸§æœ‰å…³ ç”µæºç®¡ç†çš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜è¡¨ç°å½¢å¼å¾ˆå¤šï¼Œå¯¼è‡´è¿™ä¸ªé—®é¢˜äº§ç”Ÿçš„åŸå› ä¹Ÿå¾ˆå¤š èŠ‚èƒ½ç®¡ç†æœªåŠ è½½ï¼šåœ¨ç³»ç»Ÿåå¥½è®¾ç½®-èŠ‚èƒ½é€‰é¡¹ä¸­æ²¡æœ‰å°†4ä¸ªï¼ˆå°å¼æœºä¸º5ä¸ªï¼‰é€‰é¡¹å…¨éƒ¨åŠ è½½ï¼Œå‡ºç°é—®é¢˜åœ¨äºä½ æ²¡æœ‰åŠ è½½macOSåŸç”Ÿçš„ç”µæºç®¡ç† ç¡çœ å¤±çµï¼šç¡çœ ç§’é†’æˆ–è€…ç¡çœ è‡ªåŠ¨å…³æœº/æ­»æœº/é‡å¯ï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„ç”µæºç®¡ç†æˆ–è€…USBé©±åŠ¨æœ‰å…³ USBæ€»çº¿çš„é—®é¢˜ï¼šUSBæ¥å£éƒ¨åˆ†æˆ–è€…å…¨éƒ¨å¤±çµï¼Œæ‰“å¼€Photo Boothåæ‘„åƒå¤´æ— ç”»é¢ï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„USBé©±åŠ¨æœ‰å…³ï¼ˆè¯è¯´å›æ¥Photo Boothè¿˜æ˜¯è›®æœ‰æ„æ€çš„ğŸ˜‚ï¼‰ ç‹¬ç«‹æ˜¾å¡æ— æ³•é©±åŠ¨ï¼šé»‘è‹¹æœä¸‹åªæœ‰éƒ¨åˆ†ç‹¬ç«‹æ˜¾å¡å¯ä»¥é©±åŠ¨ï¼Œå¦‚æœä½ çš„ç‹¬æ˜¾æœ‰ç‹¬ç«‹è¾“å‡ºå¹¶ä¸”æ»¡è¶³ç‰¹å®šå‹å·è¦æ±‚çš„è¯å¯ä»¥å°è¯•å°†å…¶é©±åŠ¨ï¼Œå¦åˆ™ä½ å°±éœ€è¦å±è”½ç‹¬æ˜¾ï¼Œä½¿ç”¨é›†æ˜¾äº†ï¼Œè¿™é‡Œä¸å±•å¼€å™è¿° å¦å¤–ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨å·¦ä¸Šè§’è‹¹æœå›¾æ ‡-å…³äºæœ¬æœº-ç³»ç»ŸæŠ¥å‘Šä¸­ç›´æ¥æŸ¥çœ‹ä½ ç”µè„‘çš„ç¡¬ä»¶æƒ…å†µã€‚é€šè¿‡æ£€æŸ¥å„ä¸ªç¡¬ä»¶çš„é©±åŠ¨æƒ…å†µå’Œç›¸å…³æ•°æ®ï¼Œä¸€æ ·å¯ä»¥åˆ¤æ–­ä½ çš„ç”µè„‘æ˜¯å¦ä¼šæœ‰ä¸Šé¢çš„é—®é¢˜ã€‚ ä¸Šé¢ç»™å¤§å®¶ä»‹ç»çš„éƒ½æ˜¯ä¸€äº›å…¸å‹çš„é—®é¢˜ï¼Œä½ ä¹Ÿæœ‰å¯èƒ½é‡åˆ°å…¶ä»–çš„ç–‘éš¾æ‚ç—‡ã€‚å¸Œæœ›å¤§å®¶é¢å¯¹é—®é¢˜ä¸è¦æœ›è€Œå´æ­¥ï¼Œå°½æƒ…äº«å—æŠ˜è…¾çš„è¿‡ç¨‹å§ï¼ (ï½ï¿£â–½ï¿£)ï½ é»‘è‹¹æœç›¸å…³èµ„æºæ¨èæŠ˜è…¾é»‘è‹¹æœï¼Œå®œå¹¿é›†ä¿¡æ¯ï¼Œå¤šå¤šæé—®ï¼›å¿Œç›²ç›®çæï¼Œé‡å¤å»ºè®¾ã€‚ é»‘è‹¹æœç›¸å…³ä¼˜ç§€ç½‘ç«™ é»‘æœå°å…µçš„éƒ¨è½é˜ï¼šä¹Ÿå°±æ˜¯dalianskyâ€”â€”å›½å†…é»‘è‹¹æœé¢†å†›äººç‰©çš„åšå®¢ï¼Œä»–çš„ç½‘ç«™ä¼šéå¸¸åŠæ—¶åœ°æ›´æ–°ç³»ç»Ÿé•œåƒå¹¶ä¸å®šæ—¶åœ°æä¾›ä¸€äº›ç²¾å“æ•™ç¨‹ ITå¯†ç ï¼šç½‘ç«™ä¸Šé¢çš„èµ„æºéå¸¸ä¸°å¯Œï¼Œä»ç³»ç»Ÿé•œåƒåˆ°è½¯ä»¶èµ„æºå†åˆ°æ–¹æ³•æŠ€å·§ä¸€åº”ä¿±å…¨ï¼Œåšä¸»ä¹Ÿæ˜¯éå¸¸ç‰›å•¤çš„ OCç®€ä½“ä¸­æ–‡å‚è€ƒæ‰‹å†Œï¼šç”±ä¸šç•Œå¤§ä½¬åˆåŠ›å®Œæˆï¼Œä»åœ¨ç»´æŠ¤ä¸­ï¼Œå­¦ä¹ OCå¿…å¤‡ GitHubï¼šè¿™ä¸ªä¸ç”¨å¤šè¯´äº†ï¼Œç»å¤§éƒ¨åˆ†é»‘è‹¹æœè½¯ä»¶å’Œé©±åŠ¨çš„æ¥æºï¼Œå…¨çƒæœ€å¤§åŒæ€§äº¤å‹ç½‘ç«™ğŸ¶ï¼Œç¥å¥‡çš„åœ°æ–¹ è¿œæ™¯è®ºå›ï¼šå›½å†…æœ€ä¸»è¦çš„é»‘è‹¹æœäº¤æµè®ºå›ï¼Œæ³¨å†Œéœ€è¦é‚€è¯·ç  tonymacx86ï¼šå›½å¤–çŸ¥åçš„é»‘è‹¹æœäº¤æµè®ºå›ï¼Œèµ„æºä¸°å¯Œï¼Œéœ€è¦ä¸€å®šçš„è‹±è¯­èƒ½åŠ› insanelymacï¼šä¸tonymacx86ç±»ä¼¼çš„è®ºå› é»‘è‹¹æœè½¯ä»¶ã€é©±åŠ¨èµ„æºä¸‹é¢åªåˆ—å‡ºäº†ä¸€äº›è‡³å…³é‡è¦çš„é©±åŠ¨å’Œè½¯ä»¶ï¼Œå…¶ä»–åŠŸèƒ½çš„è¿˜æœ‰å¾ˆå¤šï¼Œè¿™é‡Œå°±ä¸ä¸€ä¸€åˆ—å‡ºäº†ã€‚ Clover Configuratorï¼šCloverçš„å›¾å½¢åŒ–é…ç½®è½¯ä»¶ Hackintoolï¼šé»‘è‹¹æœå®Œå–„å¿…å¤‡å·¥å…· Cloverï¼šåœ¨è¿™é‡Œå¯ä»¥æ‰¾åˆ°å·²ç»ç¼–è¯‘å¥½çš„Clover Lilu.kextï¼šä¼—å¤šå¸¸ç”¨é©±åŠ¨çš„ä¾èµ– AppleALC.kextï¼šå¸¸ç”¨å£°å¡é©±åŠ¨ VoodooPS2Controller.kextï¼šPS2æ€»çº¿è¾“å…¥è®¾å¤‡ï¼ˆé¼ æ ‡ï¼Œé”®ç›˜ï¼Œè§¦æ§æ¿ï¼‰çš„é©±åŠ¨ï¼Œæ­¤å¤–å¯¹äºI2Cæ€»çº¿çš„è¾“å…¥è®¾å¤‡è¿˜æœ‰VoodooI2C.kext VoodooInput.kextï¼šVoodooPS2Controllerçš„ä¾èµ– WhateverGreen.kextï¼šç”¨äºé©±åŠ¨Intelé›†æˆæ˜¾å¡ FakeSMC.kextï¼šå¿…å¤‡é©±åŠ¨ï¼Œç”¨äºä»¿å†’SMCè®¾å¤‡ï¼Œæ¬ºéª—macOSï¼Œè®©ä»–ä»¥ä¸ºæˆ‘ä»¬çš„ç”µè„‘å°±æ˜¯Mac å£°æ˜ä¸è‡´è°¢é»‘è‹¹æœç¤¾åŒºçš„å¥åº·éœ€è¦å¤§å®¶å…±åŒç»´æŠ¤ï¼Œæ³è¯·æ–°äººä»¬æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š ä¸è¦æŠŠç¤¾åŒºçš„æˆæœï¼ˆå¦‚å„ç§æœºå‹çš„EFIï¼Œå¼€æºè½¯ä»¶ç­‰ï¼‰æ‹¿æ¥ä½œå•†ä¸šç”¨é€” ä¸è¦è´­ä¹°æ·˜å®ä¸Šé¢çš„EFIï¼æ‰€æœ‰ç°å­˜çš„EFIéƒ½å¯ä»¥åœ¨ç½‘ä¸Šå…è´¹è·å¾—ï¼è¯·ä¸è¦æ”¯æŒé‚£äº›å…œå”®EFIçš„æ— è‰¯å•†å®¶ï¼Œä»–ä»¬ä¹Ÿæ˜¯ä»ç½‘ä¸Šä¸‹è½½çš„ ä¸å»ºè®®å»æ·˜å®ä¸Šè´­ä¹°å®‰è£…é»‘è‹¹æœçš„æœåŠ¡ï¼Œå‡ºäº†é—®é¢˜åˆ°æœ€åè¿˜æ˜¯è¦ä½ è‡ªå·±è§£å†³ ä¸å»ºè®®æŠŠè‡ªå·±çš„æŠ˜è…¾æˆæœåœ¨ç½‘ç»œä¸Šæœ‰å¿æä¾›ï¼Œè¿™æ ·å¹¶ä¸åˆ©äºç¤¾åŒºçš„å‘å±• ç½‘å‹æ²¡æœ‰ä¹‰åŠ¡å»æ— å¿åœ°å¸®ä½ è§£å†³é—®é¢˜ï¼Œå¦å¤–ä¹Ÿè¯·å–„ç”¨æœç´¢å¼•æ“ é»‘è‹¹æœä¸€å¼€å§‹æ˜¯æå®¢çš„äº§ç‰©ï¼Œæ˜¯åå›ç²¾ç¥çš„è±¡å¾ã€‚ä»¤äººæ„æ–™ä¸åˆ°çš„æ˜¯ï¼Œç°åœ¨å®ƒå±…ç„¶å¯ä»¥ä¸ºæˆ‘ä»¬æ™®é€šäººæ‰€ç”¨ã€‚è€Œä»æå®¢åˆ°å¤§ä¼—çš„è¿‡æ¸¡ï¼Œé»‘è‹¹æœçš„å¼€æºç¤¾åŒºå¯¹æ­¤ä½œå‡ºäº†æå¤§è´¡çŒ®ã€‚å¯¹é‚£äº›å¯¹ç¤¾åŒºåšå‡ºè¿‡æå¤§è´¡çŒ®çš„æå®¢å’Œå·¥ç¨‹å¸ˆä»¬ï¼Œå¯¹ç¤¾åŒºå»ºè®¾è´¡çŒ®å‡ºè‡ªå·±çš„ä¸€ä»½åŠ›é‡ã€åŠªåŠ›ç»´æŠ¤ç¤¾åŒºå¥åº·å‘å±•çš„æˆå‘˜ï¼Œæˆ‘å‘ä½ ä»¬è¡¨è¾¾æœ€è¯šæŒšçš„æ„Ÿè°¢ã€‚æ²¡æœ‰ç¤¾åŒºï¼Œå°±æ²¡æœ‰é»‘è‹¹æœçš„ä»Šå¤©ã€‚ä½œä¸ºä»ç¤¾åŒºä¸­è·ç›Šçš„æ™®é€šæˆå‘˜ï¼Œä¹Ÿåº”è¯¥é€šè¿‡è‡ªå·±çš„åŠªåŠ›ï¼Œä»¥è‡ªå·±çš„æ–¹å¼å»å›é¦ˆè¿™ä¸ªç¤¾åŒºï¼Œå¸®åŠ©å®ƒæ›´å¥½åœ°å‘å±•ã€‚ åšä¸»åœ¨æ­¤è°¨å‘ä½ ä»¬è¡¨è¾¾æˆ‘çš„æ„Ÿè°¢ï¼šRehabManï¼ŒAcidantheraï¼Œé»‘æœå°å…µï¼ŒSlientSliverï¼ŒITå¯†ç ï¼Œä»¥åŠå…¶ä»–ç»™äºˆè¿‡æˆ‘å¸®åŠ©çš„ç½‘å‹æˆ–å¼€å‘è€…ä»¬ğŸ˜˜ã€‚ é™„ï¼šè½¯ä»¶åº¦ç›˜é“¾æ¥ ï¼Œå¯†ç ï¼š3lkxã€‚","link":"/2020/02/14/Introduction_to_hackintosh/"}],"tags":[{"name":"macOS","slug":"macOS","link":"/tags/macOS/"},{"name":"Hackintosh","slug":"Hackintosh","link":"/tags/Hackintosh/"},{"name":"Astrobear","slug":"Astrobear","link":"/tags/Astrobear/"},{"name":"Life","slug":"Life","link":"/tags/Life/"},{"name":"Others","slug":"Others","link":"/tags/Others/"},{"name":"AirSim","slug":"AirSim","link":"/tags/AirSim/"},{"name":"Research","slug":"Research","link":"/tags/Research/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Photos","slug":"Photos","link":"/tags/Photos/"},{"name":"Astrophotography","slug":"Astrophotography","link":"/tags/Astrophotography/"},{"name":"HP","slug":"HP","link":"/tags/HP/"},{"name":"Programming Language","slug":"Programming-Language","link":"/tags/Programming-Language/"},{"name":"RL","slug":"RL","link":"/tags/RL/"},{"name":"Nginx","slug":"Nginx","link":"/tags/Nginx/"},{"name":"Internet server","slug":"Internet-server","link":"/tags/Internet-server/"},{"name":"Network Technology","slug":"Network-Technology","link":"/tags/Network-Technology/"},{"name":"Experience","slug":"Experience","link":"/tags/Experience/"}],"categories":[{"name":"Hackintosh","slug":"Hackintosh","link":"/categories/Hackintosh/"},{"name":"Others","slug":"Others","link":"/categories/Others/"},{"name":"CS","slug":"CS","link":"/categories/CS/"}]}