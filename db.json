{"meta":{"version":1,"warehouse":"3.0.1"},"models":{"Asset":[{"_id":"themes/icarus/source/css/back-to-top.css","path":"css/back-to-top.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/progressbar.css","path":"css/progressbar.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/insight.css","path":"css/insight.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/search.css","path":"css/search.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/animation.js","path":"js/animation.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/insight.js","path":"js/insight.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/gallery.js","path":"js/gallery.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/back-to-top.js","path":"js/back-to-top.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/favicon.png","path":"images/favicon.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/logo.png","path":"images/logo.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/og_image.png","path":"images/og_image.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/thumbnail.svg","path":"images/thumbnail.svg","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/alipay.JPG","path":"images/alipay.JPG","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/avatar.jpg","path":"images/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/wechatpay.JPG","path":"images/wechatpay.JPG","modified":0,"renderable":1}],"Cache":[{"_id":"themes/icarus/.DS_Store","hash":"61a9b03ed2c71583e45a0c14fe11ec055a46b717","modified":1578128474241},{"_id":"themes/icarus/README.md","hash":"c351bc76d3b4a138989c50f57bd3c52fa95eaa56","modified":1578039467871},{"_id":"themes/icarus/LICENSE","hash":"41f72cd544612bc4589c924c776422b800a4eff7","modified":1577937081622},{"_id":"themes/icarus/_config.yml","hash":"86571ff21f7b02cfabea9a130c860a7c3469fcc4","modified":1578073534991},{"_id":"themes/icarus/package.json","hash":"4f362aa241ad5d5d9422cc8f1e825edfcd8cbb1a","modified":1577937081648},{"_id":"source/.DS_Store","hash":"637de53a8294d668770de6d972389f1d7d47ba6a","modified":1581000770337},{"_id":"themes/icarus/includes/.DS_Store","hash":"e6690115edc4626c57e09b6a6541c7af78c7b044","modified":1578069394351},{"_id":"themes/icarus/languages/en.yml","hash":"66d873bbe04894610c2d28b0d03e10143997b51b","modified":1577937081628},{"_id":"themes/icarus/languages/es.yml","hash":"2e59e579d393c881dcbb885516d93eeaf469cec2","modified":1577937081628},{"_id":"themes/icarus/languages/id.yml","hash":"92d2d19a62a17b6e99f82a014309bbf6c13c9ae8","modified":1577937081628},{"_id":"themes/icarus/languages/ja.yml","hash":"6eed7771de2353d71b720c6e605cceb3f230b12e","modified":1577937081628},{"_id":"themes/icarus/languages/fr.yml","hash":"0017f93a5d491a9c0e55911cdc35316762c5a94e","modified":1577937081628},{"_id":"themes/icarus/languages/ko.yml","hash":"e7ac736b604429adedd67f3ca6043201eff7b23b","modified":1577937081629},{"_id":"themes/icarus/languages/pt-BR.yml","hash":"bcf5bc81ca855d26bbc3b3bfabc7d84429e74b85","modified":1577937081629},{"_id":"themes/icarus/languages/pl.yml","hash":"43f5447c38c9be2e1f5ce6181a0f97eeb437b059","modified":1577937081629},{"_id":"themes/icarus/languages/ru.yml","hash":"ba8b4f7d77eb1d1e28aa1f9107bd0bbbdc4cba99","modified":1577937081629},{"_id":"themes/icarus/languages/tr.yml","hash":"eff1c0b3d5c4b328f6dd74a195ff378c898f4d29","modified":1577937081629},{"_id":"themes/icarus/languages/vn.yml","hash":"6d9f4fabca711a6cb0a0efd72aa75c3641beb4a6","modified":1577937081629},{"_id":"themes/icarus/languages/zh-CN.yml","hash":"804f6a1edee49bb6a5ecb8e9d14d3e93eaca37c0","modified":1577937081629},{"_id":"themes/icarus/languages/zh-TW.yml","hash":"6ff978a0c4c11e996925e1a912a1d805f4680a6c","modified":1577937081630},{"_id":"themes/icarus/layout/.DS_Store","hash":"a35945b9f253a9a05d95202ea4e9343f69531ce3","modified":1578061687314},{"_id":"themes/icarus/layout/archive.ejs","hash":"2527527eaf3e757ab476325f691d2e2e0ff9c2d5","modified":1577937081630},{"_id":"themes/icarus/layout/categories.ejs","hash":"29d304f2b95a04fbc5e7529f9bdce9648e3545ef","modified":1577937081630},{"_id":"themes/icarus/layout/category.ejs","hash":"58aa84f75193b978b2072f29dbb84ed8279574b9","modified":1577937081630},{"_id":"themes/icarus/layout/index.ejs","hash":"8ab440868f721bb7256ab9f2be96996850b0cf44","modified":1577937081637},{"_id":"themes/icarus/layout/layout.ejs","hash":"c2b47692e9db24db485265ca7d10f9ddbe10e49c","modified":1578048397463},{"_id":"themes/icarus/layout/page.ejs","hash":"ebf120d46074f67ea25a231d2f7a64fd1e751904","modified":1577937081637},{"_id":"themes/icarus/layout/post.ejs","hash":"ebf120d46074f67ea25a231d2f7a64fd1e751904","modified":1577937081641},{"_id":"themes/icarus/layout/tags.ejs","hash":"0c527c6b72386f11c18e8aa5249be8c601e69906","modified":1577937081644},{"_id":"themes/icarus/layout/tag.ejs","hash":"45eb077f2ac86f5c8090cb1a2361eed56a368e95","modified":1577937081644},{"_id":"themes/icarus/scripts/.DS_Store","hash":"a56011e9cbd55e51cd3897d1b33636876bb4225d","modified":1578048397464},{"_id":"themes/icarus/scripts/index.js","hash":"40839db58041e31eb06d428a91869b0789277e7e","modified":1577937081648},{"_id":"themes/icarus/source/.DS_Store","hash":"f5abc8eba3cba8f51cb421bf95d081510e193f34","modified":1578128480463},{"_id":"source/_drafts/template.md","hash":"b738a1718c9a344de4cef83e3289208d55ed6d64","modified":1578069394349},{"_id":"source/_drafts/.DS_Store","hash":"090feb3c86cfe00ec59c67c50c2ddc02cd2251c5","modified":1578464882985},{"_id":"source/_posts/About_2019-1-3.md","hash":"154ce21a19d1aac0bf4e2ff4a6b99c0e405b7cf7","modified":1578112205263},{"_id":"source/_posts/.DS_Store","hash":"a7eddd5b0051407c6b8d52187686a7d3a9555b9d","modified":1580909428030},{"_id":"source/_posts/Gallery.md","hash":"5adb43f4969c824078267c3af8523b1fefec4f9b","modified":1578301165714},{"_id":"themes/icarus/includes/common/ConfigGenerator.js","hash":"451397efc7808787419fa3eb6b043c0bd8bbdf30","modified":1577937081622},{"_id":"themes/icarus/includes/common/ConfigValidator.js","hash":"48cff5402e93b11d5266370e9c4b78ee21369cb9","modified":1577937081622},{"_id":"themes/icarus/includes/common/utils.js","hash":"c0aeaeb57a42bcc71a92da2249762f91abd83ffe","modified":1577937081622},{"_id":"themes/icarus/includes/generators/categories.js","hash":"7cb370ac53a05d6b1b9203579716c0ca83d35c36","modified":1577937081623},{"_id":"themes/icarus/includes/generators/category.js","hash":"313e170e55d74526c4e1be7181ef7a21439147c9","modified":1577937081623},{"_id":"themes/icarus/includes/generators/insight.js","hash":"c4b981443927b87cc14a3a583029e13f819d6d71","modified":1577937081623},{"_id":"themes/icarus/includes/generators/tags.js","hash":"8195322c208706427a1cf56361669dca4d86f6f1","modified":1577937081623},{"_id":"themes/icarus/includes/helpers/cdn.js","hash":"7d34ea6400cb3611c374c135304abcb65ef291b7","modified":1577937081623},{"_id":"themes/icarus/includes/helpers/config.js","hash":"2f76cfc1481cfb8ba2820e1611c3dfc340e5531b","modified":1578060620091},{"_id":"themes/icarus/includes/helpers/layout.js","hash":"c24589e283bc9c9cd47680eb93b9aeba1195ae84","modified":1578117322951},{"_id":"themes/icarus/includes/helpers/override.js","hash":"223771860caddffcbce7d84dfb07798f6aa0bdda","modified":1577937081624},{"_id":"themes/icarus/includes/helpers/page.js","hash":"d85844cda82512e5541db0e28bfe778bd117d207","modified":1578048397461},{"_id":"themes/icarus/includes/helpers/site.js","hash":"2f55818448fe83c73418dcf9751745c7918c10e3","modified":1577937081624},{"_id":"themes/icarus/includes/specs/article.spec.js","hash":"ce24279cd0cd39855216dab0cd5223c755757cdf","modified":1577937081624},{"_id":"themes/icarus/includes/specs/comment.spec.js","hash":"b0ef033e363b918134fb5a003143e9bd8fafa300","modified":1577937081624},{"_id":"themes/icarus/includes/specs/config.spec.js","hash":"7a9bac384a73cf9f39173fdb2dfc2813784d8891","modified":1577937081624},{"_id":"themes/icarus/includes/specs/donate.spec.js","hash":"722cb2662569957e8b1d1a467d9632b8cc6e69d6","modified":1577937081625},{"_id":"themes/icarus/includes/specs/footer.spec.js","hash":"8e6d7c5f9a13ce03241b6562259d210b389cb88e","modified":1577937081625},{"_id":"themes/icarus/includes/specs/icon_link.spec.js","hash":"7bce7e778a622ed3b3bccbe4d51d481454d78ba7","modified":1578048397462},{"_id":"themes/icarus/includes/specs/meta.spec.js","hash":"ed1b818b929d71930608291514a72ef5a639efee","modified":1577937081625},{"_id":"themes/icarus/includes/specs/navbar.spec.js","hash":"7de29c0031738a4de4d31ed4f7b0c43447c7961c","modified":1577937081625},{"_id":"themes/icarus/includes/specs/plugins.spec.js","hash":"2fb7a28fdde9a46f576e69b9967f24d66adffb57","modified":1577937081626},{"_id":"themes/icarus/includes/specs/providers.spec.js","hash":"820cc6936ba75e3104cc2e8641716ed65ada8b6f","modified":1577937081626},{"_id":"themes/icarus/includes/specs/search.spec.js","hash":"1e3995cdc471e6a2817cd45e2b6f0fd39b4540ec","modified":1577937081626},{"_id":"themes/icarus/includes/specs/share.spec.js","hash":"5ec65409a17ead13974140fc5ddc19e526586d9f","modified":1577937081626},{"_id":"themes/icarus/includes/specs/sidebar.spec.js","hash":"630c9701affe2549abc61cd4d1e5153af2224fb6","modified":1577937081626},{"_id":"themes/icarus/includes/specs/widgets.spec.js","hash":"c5cedfe1074c0566baf8aca248f0392a501d9a74","modified":1577937081627},{"_id":"themes/icarus/includes/utils/lru.js","hash":"35c0ede3553549758ff5e4ded2bc650778793377","modified":1577937081627},{"_id":"themes/icarus/includes/tasks/check_config.js","hash":"ce7626d643737c90dee6b75435ccdec26b89dacf","modified":1577937081627},{"_id":"themes/icarus/includes/tasks/check_deps.js","hash":"ab08051f785eab2a0685aa537270d2988bc13639","modified":1577937081627},{"_id":"themes/icarus/includes/tasks/welcome.js","hash":"00d1ef8c9609552b82e9a5140b838a9057c59508","modified":1577937081627},{"_id":"themes/icarus/layout/comment/changyan.ejs","hash":"73038ac4fdfdfa71d92edaa98cc194b3446586a3","modified":1577937081630},{"_id":"themes/icarus/layout/comment/changyan.locals.js","hash":"49bce2ee742c7224bda97092d6e0a1a09184ef34","modified":1577937081630},{"_id":"themes/icarus/layout/comment/disqus.ejs","hash":"7a8c656c8651d48e21ed24c469ea75898b2b12df","modified":1577937081631},{"_id":"themes/icarus/layout/comment/disqus.locals.js","hash":"a8d2cecaa82ec9e2e2e61cb73417d63d115335d6","modified":1577937081631},{"_id":"themes/icarus/layout/comment/facebook.ejs","hash":"1c3751f36f737527e352c65bb1ca7172ff792979","modified":1577937081631},{"_id":"themes/icarus/layout/comment/facebook.locals.js","hash":"77e3ef1d933660d980b26d15968aa1a5c8a93a56","modified":1577937081631},{"_id":"themes/icarus/layout/comment/gitalk.ejs","hash":"39686c7ffa1077fbaeea9e0bbc9402f9ca4a0d18","modified":1578069394351},{"_id":"themes/icarus/layout/comment/gitalk.locals.js","hash":"f920f130598148b4d9f213c82f2d7f88a796012f","modified":1577937081632},{"_id":"themes/icarus/layout/comment/gitment.ejs","hash":"d5e1a396e23df4e75e139d12846290bdb08ba01e","modified":1577937081632},{"_id":"themes/icarus/layout/comment/gitment.locals.js","hash":"f920f130598148b4d9f213c82f2d7f88a796012f","modified":1577937081632},{"_id":"themes/icarus/layout/comment/isso.ejs","hash":"cc6a43bd24be764086f88ad7c5c97ff04df87e0b","modified":1577937081632},{"_id":"themes/icarus/layout/comment/isso.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081632},{"_id":"themes/icarus/layout/comment/livere.ejs","hash":"12ff9a345f6bba2f732f592e39508c2afde89b00","modified":1577937081632},{"_id":"themes/icarus/layout/comment/livere.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081633},{"_id":"themes/icarus/layout/comment/valine.ejs","hash":"31471cd05018583249b4c09a78cf1d02e7987244","modified":1577937081633},{"_id":"themes/icarus/layout/comment/valine.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081633},{"_id":"themes/icarus/layout/common/article.ejs","hash":"6d513f3cf313b34349e125821408decdfd09ca66","modified":1578111563282},{"_id":"themes/icarus/layout/common/article.locals.js","hash":"1f108fa96e61a681d7b1ee390b4f0ff60d042720","modified":1577937081634},{"_id":"themes/icarus/layout/common/footer.ejs","hash":"8b4f636aa30ad588ce094528375336375a0585b6","modified":1578048397463},{"_id":"themes/icarus/layout/common/footer.locals.js","hash":"504ed92dc76723f19777463d690acfbe1d89e2ba","modified":1577937081634},{"_id":"themes/icarus/layout/common/head.ejs","hash":"0998072c9ece90e26339f2503633c4ca08da62e9","modified":1577937081634},{"_id":"themes/icarus/layout/common/navbar.ejs","hash":"0fe0c940fcb112828d465830713a0cdd42864951","modified":1577937081634},{"_id":"themes/icarus/layout/common/navbar.locals.js","hash":"7e523ba80667038f2e58cf4f9cb073e9afbc70e6","modified":1577937081635},{"_id":"themes/icarus/layout/common/paginator.ejs","hash":"7837d80b27f166161b3deeffb571680025c7d723","modified":1577937081635},{"_id":"themes/icarus/layout/common/scripts.ejs","hash":"99da87756d2bd234b058e90d2c6dabc1bc10f20e","modified":1577937081635},{"_id":"themes/icarus/layout/common/widget.ejs","hash":"0f2526f2d2696c598d829972e33f35748b8450c1","modified":1578048397463},{"_id":"themes/icarus/layout/donate/alipay.ejs","hash":"3290058879973e403a05472a0fe2ac0219d5b961","modified":1577937081635},{"_id":"themes/icarus/layout/donate/alipay.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081636},{"_id":"themes/icarus/layout/donate/patreon.ejs","hash":"fc19da9674649c035d133535078ff7e37d0f54c1","modified":1577937081636},{"_id":"themes/icarus/layout/donate/patreon.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081636},{"_id":"themes/icarus/layout/donate/paypal.ejs","hash":"dbb90fa9214d659ea6bbd5a92ea00888adf3761e","modified":1577937081636},{"_id":"themes/icarus/layout/donate/paypal.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081636},{"_id":"themes/icarus/layout/donate/wechat.ejs","hash":"051b873e1fc28c1d7c2d6443991b6a2f43813e6b","modified":1577937081637},{"_id":"themes/icarus/layout/donate/wechat.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081637},{"_id":"themes/icarus/layout/plugin/animejs.ejs","hash":"c17ea2cfe5cb239342166e2ba72cbfc663c8160f","modified":1577937081638},{"_id":"themes/icarus/layout/plugin/animejs.locals.js","hash":"3bf911060a222f00b03be708c37f20e36cb66ba9","modified":1577937081638},{"_id":"themes/icarus/layout/plugin/back-to-top.ejs","hash":"5936b5fd2f2444605a21c6c422623f07f02d5c9a","modified":1577937081638},{"_id":"themes/icarus/layout/plugin/back-to-top.locals.js","hash":"3bf911060a222f00b03be708c37f20e36cb66ba9","modified":1577937081638},{"_id":"themes/icarus/layout/plugin/baidu-analytics.ejs","hash":"7dbbea5722277e00a624c1796ec83d5f9c12d059","modified":1577937081638},{"_id":"themes/icarus/layout/plugin/baidu-analytics.locals.js","hash":"c02eb152e6aff05833006e6edd32b74c1c4258c3","modified":1577937081638},{"_id":"themes/icarus/layout/plugin/busuanzi.ejs","hash":"4285b0ae608c7c54e4ecbebb6d22d4cd1be28f70","modified":1577937081639},{"_id":"themes/icarus/layout/plugin/busuanzi.locals.js","hash":"ec80bcfa4c1302c04130a746df4b1298d117de0b","modified":1577937081639},{"_id":"themes/icarus/layout/plugin/gallery.ejs","hash":"7d19b7a5713d08a614578f079f1327a651c472ae","modified":1577937081639},{"_id":"themes/icarus/layout/plugin/gallery.locals.js","hash":"037fb56dffc128d3a91c1cb8852998d9539d3fac","modified":1577937081639},{"_id":"themes/icarus/layout/plugin/google-analytics.ejs","hash":"13b298b0026bfc7bcb6a47b6c795fe15cc4584fc","modified":1577937081639},{"_id":"themes/icarus/layout/plugin/google-analytics.locals.js","hash":"c02eb152e6aff05833006e6edd32b74c1c4258c3","modified":1577937081640},{"_id":"themes/icarus/layout/plugin/hotjar.ejs","hash":"6df0d8f77ed39e4d32c78177844115e31bf3a776","modified":1577937081640},{"_id":"themes/icarus/layout/plugin/hotjar.locals.js","hash":"9258fc2af057d2545a43fae54790743b63450378","modified":1577937081640},{"_id":"themes/icarus/layout/plugin/mathjax.ejs","hash":"dddb6f37487286fe2080118bcbb4a8d82dc84d5e","modified":1577937081640},{"_id":"themes/icarus/layout/plugin/mathjax.locals.js","hash":"7faa26fa6da6a93dc3f7fdcf5a784d1f8825b031","modified":1577937081640},{"_id":"themes/icarus/layout/plugin/outdated-browser.ejs","hash":"1437d1ac085a8110e61317254f6c0a034121bc39","modified":1577937081640},{"_id":"themes/icarus/layout/plugin/outdated-browser.locals.js","hash":"037fb56dffc128d3a91c1cb8852998d9539d3fac","modified":1577937081641},{"_id":"themes/icarus/layout/plugin/progressbar.ejs","hash":"34423f74787cc9d67b2598dd69b07c84d5bf2280","modified":1577937081641},{"_id":"themes/icarus/layout/plugin/progressbar.locals.js","hash":"ec80bcfa4c1302c04130a746df4b1298d117de0b","modified":1577937081641},{"_id":"themes/icarus/layout/search/baidu.ejs","hash":"850aa91778100d693a52b10eaa8586c8e3215ee6","modified":1577937081641},{"_id":"themes/icarus/layout/search/baidu.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081641},{"_id":"themes/icarus/layout/search/google-cse.ejs","hash":"4b881a99325a6a0cebf97ac53e09d8fc67f87d29","modified":1577937081642},{"_id":"themes/icarus/layout/search/google-cse.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081642},{"_id":"themes/icarus/layout/search/insight.ejs","hash":"9a27db2a007582ceee7ca4b1eebddbd456893568","modified":1577937081642},{"_id":"themes/icarus/layout/search/insight.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081642},{"_id":"themes/icarus/layout/share/addthis.ejs","hash":"f1c5f337333009d5f00dfbac4864a16ef8f9cb8d","modified":1577937081642},{"_id":"themes/icarus/layout/share/addthis.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081643},{"_id":"themes/icarus/layout/share/addtoany.ejs","hash":"95d3bc1a841bd934b1ae9209ad1af74e743ecb10","modified":1577937081643},{"_id":"themes/icarus/layout/share/addtoany.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081643},{"_id":"themes/icarus/layout/share/bdshare.ejs","hash":"f14c8084b7ee16a091f0bd2ae9039e3bfff7e7b7","modified":1577937081643},{"_id":"themes/icarus/layout/share/bdshare.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081643},{"_id":"themes/icarus/layout/share/sharejs.ejs","hash":"0f28a2bed23ba80014e7fd5e28a5eb807ec04ba2","modified":1578048397463},{"_id":"themes/icarus/layout/share/sharejs.locals.js","hash":"11976fd4cfed1044be29b476b34c33175c9b4308","modified":1577937081644},{"_id":"themes/icarus/layout/share/sharethis.ejs","hash":"4f2c40f790f3be0a4e79db04f02ea41ba2f4d4c0","modified":1577937081644},{"_id":"themes/icarus/layout/share/sharethis.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081644},{"_id":"themes/icarus/layout/widget/.DS_Store","hash":"d880bfca0110ea7f19179d834cdc916d16e72b72","modified":1578048397463},{"_id":"themes/icarus/layout/widget/archive.ejs","hash":"742952c7ace79b4d66db9a447bf5977597de841e","modified":1578048397463},{"_id":"themes/icarus/layout/widget/archive.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081645},{"_id":"themes/icarus/layout/widget/category.ejs","hash":"17e58e537645c4434a1140377ae3e7f43cca4927","modified":1577937081645},{"_id":"themes/icarus/layout/widget/category.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081645},{"_id":"themes/icarus/layout/widget/links.ejs","hash":"bb6510193632413e83227bdffad7c3b37629dcde","modified":1577937081645},{"_id":"themes/icarus/layout/widget/links.locals.js","hash":"872cf1a18e152361f5739c6d5fecc0bf46d59513","modified":1577937081645},{"_id":"themes/icarus/layout/widget/profile.ejs","hash":"436974e24e793d66beea44a97cf3fa6c5bdda867","modified":1578048397464},{"_id":"themes/icarus/layout/widget/recent_posts.ejs","hash":"14a2f4587831e017b93818c06dbe18a7e8a27c1e","modified":1577937081646},{"_id":"themes/icarus/layout/widget/profile.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081646},{"_id":"themes/icarus/layout/widget/recent_posts.locals.js","hash":"5065aca74ec2c98ec88994636fee8408f769c5f2","modified":1577937081646},{"_id":"themes/icarus/layout/widget/subscribe_email.ejs","hash":"391622e9c1d17bf79180faa617ed8c1ee1871a87","modified":1577937081646},{"_id":"themes/icarus/layout/widget/subscribe_email.locals.js","hash":"aae87fbdb7a1245a0fc0637225a935fc39836916","modified":1577937081647},{"_id":"themes/icarus/layout/widget/tag.ejs","hash":"e41aff420cc4ea1c454de49bd8af0e7a93f3db3f","modified":1577937081647},{"_id":"themes/icarus/layout/widget/tagcloud.ejs","hash":"8bb486e10b4e0e3900dcc5ffb46717777c460e44","modified":1578048397464},{"_id":"themes/icarus/layout/widget/tagcloud.locals.js","hash":"093f59d2f43e7ffa47bee79da15f98705300dfba","modified":1577937081647},{"_id":"themes/icarus/layout/widget/toc.ejs","hash":"b6e0a3d95e8660ceb9a94b6e671ad9a6e345acd1","modified":1578048397464},{"_id":"themes/icarus/layout/widget/toc.locals.js","hash":"e730a7fff2717f17741540e5ed77b89e289fdeab","modified":1577937081647},{"_id":"themes/icarus/source/css/back-to-top.css","hash":"5805bee2445e997d64dfe526b08b5fe0bce357eb","modified":1577937081648},{"_id":"themes/icarus/source/css/progressbar.css","hash":"bbc737b7a8feb19901e792c447a846273779d5c3","modified":1577937081649},{"_id":"themes/icarus/source/css/insight.css","hash":"22943a610d5cfffedfb823c692f4db2b1f37a4c9","modified":1577937081648},{"_id":"themes/icarus/source/css/search.css","hash":"d6a59894819e7431d42b249b6c2fc9ff3b99a488","modified":1577937081649},{"_id":"themes/icarus/source/css/style.styl","hash":"d9c612f4e67cdc4e787046dc9e4e64ce31264d40","modified":1578111041259},{"_id":"themes/icarus/source/js/animation.js","hash":"d744581909d2d092a584be07c39f9d3f0d009ec7","modified":1577937081651},{"_id":"themes/icarus/source/js/insight.js","hash":"8ba56fd5e4232a05ccef5f8b733c7ecca0814633","modified":1577937081652},{"_id":"themes/icarus/source/js/gallery.js","hash":"bb74e694457dc23b83ac80cf5aadcd26b60469fd","modified":1577937081652},{"_id":"themes/icarus/source/js/back-to-top.js","hash":"b1dcf30577cefe833dc6151757c0a05ea5b5a643","modified":1577937081652},{"_id":"themes/icarus/source/js/main.js","hash":"009991bd09cfc4f62ce1ce42d138709a76732a6f","modified":1578048316901},{"_id":"themes/icarus/source/images/.DS_Store","hash":"b2057735a0742688a1279c5cc099fbf91519b3f7","modified":1578128705520},{"_id":"themes/icarus/source/images/favicon.png","hash":"ced4774159b178d14dc3504bfcf6a2e928587883","modified":1577970272080},{"_id":"themes/icarus/source/images/logo.png","hash":"bf936514d39216192308530005354793cce73f4e","modified":1577969646704},{"_id":"themes/icarus/source/images/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1577937081651},{"_id":"themes/icarus/source/images/thumbnail.svg","hash":"b9c58ff09ed415e6cf08b42b35faa2bc000d5059","modified":1577937081651},{"_id":"themes/icarus/source/images/alipay.JPG","hash":"dbc4a7854afd6c2a7e42869b9b0ddb5b1a43866c","modified":1577972646000},{"_id":"themes/icarus/source/images/avatar.jpg","hash":"3c5113043990ad941b130bd4cbc4ef8fe1fcc7e6","modified":1547818171000},{"_id":"themes/icarus/source/images/wechatpay.JPG","hash":"1d840127f10a0a6dc0b0d6b10e1d6eeb30ad2321","modified":1577972646000},{"_id":"public/content.json","hash":"4207dbd179b4dcc7fdfbc13b11ec830a40e6cfce","modified":1581068346626},{"_id":"public/2020/01/03/Gallery/index.html","hash":"8f0706737e77312863dcdfcf448a806a24be824e","modified":1581004138459},{"_id":"public/2020/01/03/About_2019-1-3/index.html","hash":"1b19d8f9cac14013dc441ab4a1a798ff5c5dd086","modified":1578128559917},{"_id":"public/archives/2020/index.html","hash":"b76efe24a7a263b620e07d7d66edcb982d8de3dd","modified":1581004138459},{"_id":"public/archives/index.html","hash":"44eb4c19040bbfe89da2a82c32d7bfc0d2eb3732","modified":1581004138459},{"_id":"public/archives/2020/01/index.html","hash":"91d5b18934902a39a18d9fe1f2b95c8eb62d3a50","modified":1581004138459},{"_id":"public/tags/Astrobear/index.html","hash":"2f50ca6d59b8b74a93ea6c7475614f3c3975704c","modified":1581004138459},{"_id":"public/tags/Life/index.html","hash":"7cfb7a6c64f3d0a68df8c80ffe64010881f67f9c","modified":1581004138459},{"_id":"public/tags/Others/index.html","hash":"327614626ed9685f7b487f381a9b2ba69c5d1c6f","modified":1581004138459},{"_id":"public/tags/Photos/index.html","hash":"a2f9db414869d1922428e78e55ad9e43cf3a5409","modified":1581004138459},{"_id":"public/tags/Astrophotography/index.html","hash":"b2f210476610817c202b06a013a6f91a09ed3753","modified":1581004138459},{"_id":"public/index.html","hash":"be1c9d6e8fa86b75a6b1485c51d2e1ccfc32ef9e","modified":1581068346626},{"_id":"public/categories/Others/index.html","hash":"e30f3b4b79b984d131fca9d7ad9123a98a1145df","modified":1581004138459},{"_id":"public/categories/Gellary/index.html","hash":"747d76167826efaf0abfdd0e64593791b96cf1cd","modified":1578128559917},{"_id":"public/categories/index.html","hash":"f9c862485fe12d01ace73054c982fb6d1eaf46c1","modified":1581004138459},{"_id":"public/tags/index.html","hash":"66a3f02e381fab261fed072cb58e623913050402","modified":1581004138459},{"_id":"public/images/favicon.png","hash":"ced4774159b178d14dc3504bfcf6a2e928587883","modified":1578128559917},{"_id":"public/images/thumbnail.svg","hash":"b9c58ff09ed415e6cf08b42b35faa2bc000d5059","modified":1578128559917},{"_id":"public/images/logo.png","hash":"bf936514d39216192308530005354793cce73f4e","modified":1578128559917},{"_id":"public/images/alipay.JPG","hash":"dbc4a7854afd6c2a7e42869b9b0ddb5b1a43866c","modified":1578128559917},{"_id":"public/images/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1578128559917},{"_id":"public/css/search.css","hash":"d6a59894819e7431d42b249b6c2fc9ff3b99a488","modified":1578128559917},{"_id":"public/css/insight.css","hash":"22943a610d5cfffedfb823c692f4db2b1f37a4c9","modified":1578128559917},{"_id":"public/css/style.css","hash":"86c5814101a1a0f5fec222acd168ba26b98e3771","modified":1578128559917},{"_id":"public/css/back-to-top.css","hash":"5805bee2445e997d64dfe526b08b5fe0bce357eb","modified":1578128559917},{"_id":"public/js/animation.js","hash":"d744581909d2d092a584be07c39f9d3f0d009ec7","modified":1578128559917},{"_id":"public/js/gallery.js","hash":"bb74e694457dc23b83ac80cf5aadcd26b60469fd","modified":1578128559917},{"_id":"public/js/insight.js","hash":"8ba56fd5e4232a05ccef5f8b733c7ecca0814633","modified":1578128559917},{"_id":"public/js/back-to-top.js","hash":"b1dcf30577cefe833dc6151757c0a05ea5b5a643","modified":1578128559917},{"_id":"public/css/progressbar.css","hash":"bbc737b7a8feb19901e792c447a846273779d5c3","modified":1578128559917},{"_id":"public/js/main.js","hash":"009991bd09cfc4f62ce1ce42d138709a76732a6f","modified":1578128559917},{"_id":"public/images/avatar.jpg","hash":"3c5113043990ad941b130bd4cbc4ef8fe1fcc7e6","modified":1578128559917},{"_id":"public/images/wechatpay.JPG","hash":"1d840127f10a0a6dc0b0d6b10e1d6eeb30ad2321","modified":1578128559917},{"_id":"source/_posts/About.md","hash":"154ce21a19d1aac0bf4e2ff4a6b99c0e405b7cf7","modified":1578128618244},{"_id":"source/_posts/RLSummarize2.md","hash":"1bcb47a425729be5e1d29b8fd2cd82da8d15b5b3","modified":1581005495224},{"_id":"source/_posts/RLSummarize3.md","hash":"41d68356c0fc75613c590bc6c8634dadef67c5d0","modified":1581001067072},{"_id":"source/_posts/AirSimMultirotorAPIs.md","hash":"2a3de6bc11468cc1e1da5f26d7504e24634ea5c2","modified":1579276481295},{"_id":"source/_posts/Python学习笔记.md","hash":"61890a075ed055448a6aec975fa185a2db2b7396","modified":1578889105089},{"_id":"source/_posts/RLSummarize1.md","hash":"439438a529e22bd748390437811224fbc8d39b77","modified":1581068340831},{"_id":"source/_posts/华为云+nginx服务器搭建总结.md","hash":"8963423b10f0cae9e120af65b82295c4a7ac21a6","modified":1578645923632},{"_id":"public/2020/02/01/RLSummarize3/index.html","hash":"a129ae7e4264ad95b878957332ac9b235413a4e9","modified":1581004138459},{"_id":"public/2020/01/17/RLSummarize1/index.html","hash":"8635c0b313d887c6790cf8ea5ac814c239d883a0","modified":1581068346626},{"_id":"public/2020/01/18/RLSummarize2/index.html","hash":"fecd0970311d233bcfdb97781b252cd412707816","modified":1581005503476},{"_id":"public/2020/01/15/AirSimMultirotorAPIs/index.html","hash":"ee2908c9a3f2a24c479e8384eb7cb34591875a27","modified":1581004138459},{"_id":"public/2020/01/08/华为云+nginx服务器搭建总结/index.html","hash":"306774d9af87571cdbef364bc498c1c8c2c71a02","modified":1581004138459},{"_id":"public/2020/01/06/Python学习笔记/index.html","hash":"b84ff504223bee175c0a24c116f63b983dd0fed4","modified":1581004138459},{"_id":"public/2020/01/03/About/index.html","hash":"93928eb334850033de9cd3e5b00974c241fec7b2","modified":1581004138459},{"_id":"public/tags/AirSim/index.html","hash":"7e26e82c9a78ea949e30962da97b94e902b284e2","modified":1581004138459},{"_id":"public/tags/Python/index.html","hash":"19552d0ef6c8ca674cd679b974ae9fd71e9084ec","modified":1581068346626},{"_id":"public/tags/RL/index.html","hash":"82c28a8e9a424d471432cbc31517e37116f2ebad","modified":1581068346626},{"_id":"public/tags/Research/index.html","hash":"b2d4425ae2f89193b77dadb6592693b7e7497c13","modified":1581068346626},{"_id":"public/tags/Programming-Language/index.html","hash":"2fc30c84c4e83445bace013fc2ba8d83007c28c2","modified":1581004138459},{"_id":"public/tags/Nginx/index.html","hash":"0276478d6c9ce496ba35827459f24e1217abdbfd","modified":1581004138459},{"_id":"public/tags/Internet-server/index.html","hash":"438e30addb85f944476aff9783f395ca439f25e7","modified":1581004138459},{"_id":"public/tags/Network-Technology/index.html","hash":"4d1fc46a302deeb300a5ff727592fe3a26a49987","modified":1581004138459},{"_id":"public/tags/Experience/index.html","hash":"a2b755837b85fbd2e677b94813bbecdefcfedd24","modified":1581004138459},{"_id":"public/archives/2020/02/index.html","hash":"d8d9994fda77ec236fb9cb6e3076a50645613cba","modified":1581004138459},{"_id":"public/categories/CS/index.html","hash":"c9b8046cf4383acb2f77999b89fb4f35b9030464","modified":1581068346626}],"Category":[{"name":"Others","_id":"ck4zczyf50004vq39a3nk34a3"},{"name":"Gellary","_id":"ck4zczyf70006vq39g17adi5e"},{"name":"CS","_id":"ck6ax1lgi0007j1p20c54bs9g"}],"Data":[],"Page":[],"Post":[{"title":"template","date":"2020-01-03T16:36:34.349Z","_content":"\n","source":"_drafts/template.md","raw":"---\ntitle: #title\ndate: #yyyy-mm-dd hh:mm:ss\ncategories: \n\t#- [cate1]\n\t#- [cate2]\n\t#...\ntags: \n\t#- tag1\n\t#- tag2\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\n#thumbnail: /thumbnail/xxx.xxx\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\n#excerpt: ...\n\n#You can begin to input your article below now.\n\n---\n\n","slug":"template","published":0,"updated":"2020-01-03T16:36:34.349Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck4zczyeu0000vq39ec769wap","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Gallery","date":"2020-01-03T15:25:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t1.jpg","excerpt":"Welcome to my gallery!","widgets":[],"_content":"\n> Photos will continue to update...\n\n<div class=\"justified-gallery\">\n\n\n\n\n![Seattle Space Needle Tower](https://astrobear.top/resource/astroblog/gallery/g1.jpg)\n\n![SF Golden Gate Bridge](https://astrobear.top/resource/astroblog/gallery/g2.jpg)\n\n![Stanford University](https://astrobear.top/resource/astroblog/gallery/g3.jpg)\n\n![Fengyun Hill](https://astrobear.top/resource/astroblog/gallery/g4.jpg)\n\n![Beyond the Clouds](https://astrobear.top/resource/astroblog/gallery/g5.jpg)\n\n![Temple](https://astrobear.top/resource/astroblog/gallery/g6.jpg)\n\n![Chaka Salt Lake](https://astrobear.top/resource/astroblog/gallery/g7.jpg)\n\n![Lizard](https://astrobear.top/resource/astroblog/gallery/g8.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g9.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g10.jpg)\n\n![Host's Cat](https://astrobear.top/resource/astroblog/gallery/g11.jpg)\n\n![Changbai Mountain](https://astrobear.top/resource/astroblog/gallery/g12.jpg)\n\n![Forbidden City](https://astrobear.top/resource/astroblog/gallery/g13.jpg)\n\n![Signal Hill, Tsingtao](https://astrobear.top/resource/astroblog/gallery/g14.jpg)\n\n![The Milky Way and Sunflower](https://astrobear.top/resource/astroblog/gallery/g15.jpg)\n\n![NGC7000 The North America Nebula](https://astrobear.top/resource/astroblog/gallery/g16.jpg)\n\n![The North Lake](https://astrobear.top/resource/astroblog/gallery/g17.jpg)\n\n![Shanghai Bund](https://astrobear.top/resource/astroblog/gallery/g18.jpg)\n\n![Xinjiekou, Nanjing](https://astrobear.top/resource/astroblog/gallery/g19.jpg)\n\n![Huangpu River](https://astrobear.top/resource/astroblog/gallery/g20.jpg)\n\n![Art Show](https://astrobear.top/resource/astroblog/gallery/g21.jpg)\n\n![Milky Way and Car](https://astrobear.top/resource/astroblog/gallery/g22.jpg)\n\n![Milky Way and Camera](https://astrobear.top/resource/astroblog/gallery/g23.jpg)\n\n![Teradacho Park](https://astrobear.top/resource/astroblog/gallery/g24.jpg)\n\n![The Jellyfish in Osaka Aquarium](https://astrobear.top/resource/astroblog/gallery/g25.jpg)\n\n![Osaka City](https://astrobear.top/resource/astroblog/gallery/g26.jpg)\n\n![Wakakusa Yama](https://astrobear.top/resource/astroblog/gallery/g27.jpg)\n\n![Kasuga Taisha](https://astrobear.top/resource/astroblog/gallery/g28.jpg)\n\n![Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g29.jpg)\n\n![Gate of Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g30.jpg)\n\n![Kyoto Tower](https://astrobear.top/resource/astroblog/gallery/g31.jpg)\n\n![Torii Gate](https://astrobear.top/resource/astroblog/gallery/g32.jpg)\n\n![M45 Pleiades](https://astrobear.top/resource/astroblog/gallery/g33.jpg)\n\n![The Milky Way](https://astrobear.top/resource/astroblog/gallery/g34.jpg)\n\n\n\n</div>","source":"_posts/Gallery.md","raw":"---\ntitle: Gallery\ndate: 2020-1-3 23:25:00\ncategories: \n\t- [Others]\n\t#- [cate2]\n\t#...\ntags: \n\t- Photos\n\t- Astrophotography\n\t- Life\n\t- Others\n\t- Astrobear\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t1.jpg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Welcome to my gallery!\n\nwidgets: []\n\n#You can begin to input your article below now.\n---\n\n> Photos will continue to update...\n\n<div class=\"justified-gallery\">\n\n\n\n\n![Seattle Space Needle Tower](https://astrobear.top/resource/astroblog/gallery/g1.jpg)\n\n![SF Golden Gate Bridge](https://astrobear.top/resource/astroblog/gallery/g2.jpg)\n\n![Stanford University](https://astrobear.top/resource/astroblog/gallery/g3.jpg)\n\n![Fengyun Hill](https://astrobear.top/resource/astroblog/gallery/g4.jpg)\n\n![Beyond the Clouds](https://astrobear.top/resource/astroblog/gallery/g5.jpg)\n\n![Temple](https://astrobear.top/resource/astroblog/gallery/g6.jpg)\n\n![Chaka Salt Lake](https://astrobear.top/resource/astroblog/gallery/g7.jpg)\n\n![Lizard](https://astrobear.top/resource/astroblog/gallery/g8.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g9.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g10.jpg)\n\n![Host's Cat](https://astrobear.top/resource/astroblog/gallery/g11.jpg)\n\n![Changbai Mountain](https://astrobear.top/resource/astroblog/gallery/g12.jpg)\n\n![Forbidden City](https://astrobear.top/resource/astroblog/gallery/g13.jpg)\n\n![Signal Hill, Tsingtao](https://astrobear.top/resource/astroblog/gallery/g14.jpg)\n\n![The Milky Way and Sunflower](https://astrobear.top/resource/astroblog/gallery/g15.jpg)\n\n![NGC7000 The North America Nebula](https://astrobear.top/resource/astroblog/gallery/g16.jpg)\n\n![The North Lake](https://astrobear.top/resource/astroblog/gallery/g17.jpg)\n\n![Shanghai Bund](https://astrobear.top/resource/astroblog/gallery/g18.jpg)\n\n![Xinjiekou, Nanjing](https://astrobear.top/resource/astroblog/gallery/g19.jpg)\n\n![Huangpu River](https://astrobear.top/resource/astroblog/gallery/g20.jpg)\n\n![Art Show](https://astrobear.top/resource/astroblog/gallery/g21.jpg)\n\n![Milky Way and Car](https://astrobear.top/resource/astroblog/gallery/g22.jpg)\n\n![Milky Way and Camera](https://astrobear.top/resource/astroblog/gallery/g23.jpg)\n\n![Teradacho Park](https://astrobear.top/resource/astroblog/gallery/g24.jpg)\n\n![The Jellyfish in Osaka Aquarium](https://astrobear.top/resource/astroblog/gallery/g25.jpg)\n\n![Osaka City](https://astrobear.top/resource/astroblog/gallery/g26.jpg)\n\n![Wakakusa Yama](https://astrobear.top/resource/astroblog/gallery/g27.jpg)\n\n![Kasuga Taisha](https://astrobear.top/resource/astroblog/gallery/g28.jpg)\n\n![Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g29.jpg)\n\n![Gate of Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g30.jpg)\n\n![Kyoto Tower](https://astrobear.top/resource/astroblog/gallery/g31.jpg)\n\n![Torii Gate](https://astrobear.top/resource/astroblog/gallery/g32.jpg)\n\n![M45 Pleiades](https://astrobear.top/resource/astroblog/gallery/g33.jpg)\n\n![The Milky Way](https://astrobear.top/resource/astroblog/gallery/g34.jpg)\n\n\n\n</div>","slug":"Gallery","published":1,"updated":"2020-01-06T08:59:25.714Z","_id":"ck4zczyf20002vq391oi1h22d","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>Photos will continue to update…</p>\n</blockquote>\n<div class=\"justified-gallery\">\n\n\n\n\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g1.jpg\" alt=\"Seattle Space Needle Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g2.jpg\" alt=\"SF Golden Gate Bridge\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g3.jpg\" alt=\"Stanford University\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g4.jpg\" alt=\"Fengyun Hill\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g5.jpg\" alt=\"Beyond the Clouds\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g6.jpg\" alt=\"Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g7.jpg\" alt=\"Chaka Salt Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g8.jpg\" alt=\"Lizard\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g9.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g10.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g11.jpg\" alt=\"Host&#39;s Cat\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g12.jpg\" alt=\"Changbai Mountain\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g13.jpg\" alt=\"Forbidden City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g14.jpg\" alt=\"Signal Hill, Tsingtao\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g15.jpg\" alt=\"The Milky Way and Sunflower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g16.jpg\" alt=\"NGC7000 The North America Nebula\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g17.jpg\" alt=\"The North Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g18.jpg\" alt=\"Shanghai Bund\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g19.jpg\" alt=\"Xinjiekou, Nanjing\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g20.jpg\" alt=\"Huangpu River\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g21.jpg\" alt=\"Art Show\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g22.jpg\" alt=\"Milky Way and Car\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g23.jpg\" alt=\"Milky Way and Camera\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g24.jpg\" alt=\"Teradacho Park\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g25.jpg\" alt=\"The Jellyfish in Osaka Aquarium\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g26.jpg\" alt=\"Osaka City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g27.jpg\" alt=\"Wakakusa Yama\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g28.jpg\" alt=\"Kasuga Taisha\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g29.jpg\" alt=\"Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g30.jpg\" alt=\"Gate of Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g31.jpg\" alt=\"Kyoto Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g32.jpg\" alt=\"Torii Gate\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g33.jpg\" alt=\"M45 Pleiades\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g34.jpg\" alt=\"The Milky Way\"></p>\n</div>","site":{"data":{}},"more":"<blockquote>\n<p>Photos will continue to update…</p>\n</blockquote>\n<div class=\"justified-gallery\">\n\n\n\n\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g1.jpg\" alt=\"Seattle Space Needle Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g2.jpg\" alt=\"SF Golden Gate Bridge\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g3.jpg\" alt=\"Stanford University\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g4.jpg\" alt=\"Fengyun Hill\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g5.jpg\" alt=\"Beyond the Clouds\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g6.jpg\" alt=\"Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g7.jpg\" alt=\"Chaka Salt Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g8.jpg\" alt=\"Lizard\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g9.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g10.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g11.jpg\" alt=\"Host&#39;s Cat\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g12.jpg\" alt=\"Changbai Mountain\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g13.jpg\" alt=\"Forbidden City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g14.jpg\" alt=\"Signal Hill, Tsingtao\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g15.jpg\" alt=\"The Milky Way and Sunflower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g16.jpg\" alt=\"NGC7000 The North America Nebula\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g17.jpg\" alt=\"The North Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g18.jpg\" alt=\"Shanghai Bund\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g19.jpg\" alt=\"Xinjiekou, Nanjing\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g20.jpg\" alt=\"Huangpu River\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g21.jpg\" alt=\"Art Show\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g22.jpg\" alt=\"Milky Way and Car\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g23.jpg\" alt=\"Milky Way and Camera\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g24.jpg\" alt=\"Teradacho Park\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g25.jpg\" alt=\"The Jellyfish in Osaka Aquarium\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g26.jpg\" alt=\"Osaka City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g27.jpg\" alt=\"Wakakusa Yama\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g28.jpg\" alt=\"Kasuga Taisha\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g29.jpg\" alt=\"Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g30.jpg\" alt=\"Gate of Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g31.jpg\" alt=\"Kyoto Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g32.jpg\" alt=\"Torii Gate\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g33.jpg\" alt=\"M45 Pleiades\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g34.jpg\" alt=\"The Milky Way\"></p>\n</div>"},{"title":"欢迎来到Astroblog！","date":"2020-01-03T12:47:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t2.jpg","excerpt":"Astroblog是Astrobear的基地！这里有知识，方法，还有更多！","_content":"\n## 渊源\n\n本人2019年4月在华为云购买了一台云服务器。本来打算是为了给自己“未来要做的“微信小程序提供后端服务的，结果一直拖到8月份才购买了域名并完成了备案。在这之后一段时间内又没有新的小程序要做，于是这个服务器和域名便一直荒废了快大半年。由于大三上学期结束的非常之早，我人生中第一次拥有了将近两个月的寒假。趁此机会，我决定将这个服务器先利用起来，于是就有了Astroblog。\n\n## 简介\n\nAstroblog上主要将包括以下内容：\n\n- 在学校的课程总结，目前计划将总结的资料全部电子化（尽量，看心情😂）\n- 与计算机技术相关的技术总结，比如一些教程或方法等，用作备忘，大致分为以下几类：\n  - 编程语言知识总结\n  - 计算机网络技术\n  - 程序开发\n  - 黑苹果\n- 个人摄影作品以及其他优秀摄影作品的展览\n- 一些民航知识\n- 其他内容\n\n## 关于Astrobear\n\n站长现在（2020年1月）是一个大三学生，专业是探测制导与控制技术。从小的梦想是成为一名飞行员，但是高三体检被刷了😭。于是迷迷糊糊就到了现在的学校，进了现在的专业。高中的时候加入了学校天文社，从那时起喜欢上了天文（摄影）。本渣在大一上学期学了C语言以后对计算机技术产生了兴趣，然后在大一暑假几乎独立开发了~~（合伙人沉迷acm太忙了）~~一个给自己在的学生组织用的管理类微信小程序，之后在“编程”的道路上越走越远。\n\n总结一下，Astrobear是：学控制的梦想成为飞行员的天文和计算机爱好者。\n\n由于我并不是计算机专业的，所以关于计算机技术这一块是本着“拿来主义”的态度去学的——会用就行。因此在这方面难免会有疏漏错误之处，也请大家海涵。\n\n多亏有了互联网的发展，知识的传播可以如此地迅速。在此，对之前对我有过帮助的博主表示衷心的感谢！","source":"_posts/About.md","raw":"---\ntitle: 欢迎来到Astroblog！\ndate: 2020-1-3 20:47\ncategories: \n\t- [Others]\n\t#- [cate2]\n\t#...\ntags: \n\t- Astrobear\n\t- Life\n\t- Others\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t2.jpg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Astroblog是Astrobear的基地！这里有知识，方法，还有更多！\n\n#You can begin to input your article below now.\n\n---\n\n## 渊源\n\n本人2019年4月在华为云购买了一台云服务器。本来打算是为了给自己“未来要做的“微信小程序提供后端服务的，结果一直拖到8月份才购买了域名并完成了备案。在这之后一段时间内又没有新的小程序要做，于是这个服务器和域名便一直荒废了快大半年。由于大三上学期结束的非常之早，我人生中第一次拥有了将近两个月的寒假。趁此机会，我决定将这个服务器先利用起来，于是就有了Astroblog。\n\n## 简介\n\nAstroblog上主要将包括以下内容：\n\n- 在学校的课程总结，目前计划将总结的资料全部电子化（尽量，看心情😂）\n- 与计算机技术相关的技术总结，比如一些教程或方法等，用作备忘，大致分为以下几类：\n  - 编程语言知识总结\n  - 计算机网络技术\n  - 程序开发\n  - 黑苹果\n- 个人摄影作品以及其他优秀摄影作品的展览\n- 一些民航知识\n- 其他内容\n\n## 关于Astrobear\n\n站长现在（2020年1月）是一个大三学生，专业是探测制导与控制技术。从小的梦想是成为一名飞行员，但是高三体检被刷了😭。于是迷迷糊糊就到了现在的学校，进了现在的专业。高中的时候加入了学校天文社，从那时起喜欢上了天文（摄影）。本渣在大一上学期学了C语言以后对计算机技术产生了兴趣，然后在大一暑假几乎独立开发了~~（合伙人沉迷acm太忙了）~~一个给自己在的学生组织用的管理类微信小程序，之后在“编程”的道路上越走越远。\n\n总结一下，Astrobear是：学控制的梦想成为飞行员的天文和计算机爱好者。\n\n由于我并不是计算机专业的，所以关于计算机技术这一块是本着“拿来主义”的态度去学的——会用就行。因此在这方面难免会有疏漏错误之处，也请大家海涵。\n\n多亏有了互联网的发展，知识的传播可以如此地迅速。在此，对之前对我有过帮助的博主表示衷心的感谢！","slug":"About","published":1,"updated":"2020-01-04T09:03:38.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck6ax1lfy0000j1p2e2ht8nn6","content":"<h2 id=\"渊源\"><a href=\"#渊源\" class=\"headerlink\" title=\"渊源\"></a>渊源</h2><p>本人2019年4月在华为云购买了一台云服务器。本来打算是为了给自己“未来要做的“微信小程序提供后端服务的，结果一直拖到8月份才购买了域名并完成了备案。在这之后一段时间内又没有新的小程序要做，于是这个服务器和域名便一直荒废了快大半年。由于大三上学期结束的非常之早，我人生中第一次拥有了将近两个月的寒假。趁此机会，我决定将这个服务器先利用起来，于是就有了Astroblog。</p>\n<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>Astroblog上主要将包括以下内容：</p>\n<ul>\n<li>在学校的课程总结，目前计划将总结的资料全部电子化（尽量，看心情😂）</li>\n<li>与计算机技术相关的技术总结，比如一些教程或方法等，用作备忘，大致分为以下几类：<ul>\n<li>编程语言知识总结</li>\n<li>计算机网络技术</li>\n<li>程序开发</li>\n<li>黑苹果</li>\n</ul>\n</li>\n<li>个人摄影作品以及其他优秀摄影作品的展览</li>\n<li>一些民航知识</li>\n<li>其他内容</li>\n</ul>\n<h2 id=\"关于Astrobear\"><a href=\"#关于Astrobear\" class=\"headerlink\" title=\"关于Astrobear\"></a>关于Astrobear</h2><p>站长现在（2020年1月）是一个大三学生，专业是探测制导与控制技术。从小的梦想是成为一名飞行员，但是高三体检被刷了😭。于是迷迷糊糊就到了现在的学校，进了现在的专业。高中的时候加入了学校天文社，从那时起喜欢上了天文（摄影）。本渣在大一上学期学了C语言以后对计算机技术产生了兴趣，然后在大一暑假几乎独立开发了<del>（合伙人沉迷acm太忙了）</del>一个给自己在的学生组织用的管理类微信小程序，之后在“编程”的道路上越走越远。</p>\n<p>总结一下，Astrobear是：学控制的梦想成为飞行员的天文和计算机爱好者。</p>\n<p>由于我并不是计算机专业的，所以关于计算机技术这一块是本着“拿来主义”的态度去学的——会用就行。因此在这方面难免会有疏漏错误之处，也请大家海涵。</p>\n<p>多亏有了互联网的发展，知识的传播可以如此地迅速。在此，对之前对我有过帮助的博主表示衷心的感谢！</p>\n","site":{"data":{}},"more":"<h2 id=\"渊源\"><a href=\"#渊源\" class=\"headerlink\" title=\"渊源\"></a>渊源</h2><p>本人2019年4月在华为云购买了一台云服务器。本来打算是为了给自己“未来要做的“微信小程序提供后端服务的，结果一直拖到8月份才购买了域名并完成了备案。在这之后一段时间内又没有新的小程序要做，于是这个服务器和域名便一直荒废了快大半年。由于大三上学期结束的非常之早，我人生中第一次拥有了将近两个月的寒假。趁此机会，我决定将这个服务器先利用起来，于是就有了Astroblog。</p>\n<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>Astroblog上主要将包括以下内容：</p>\n<ul>\n<li>在学校的课程总结，目前计划将总结的资料全部电子化（尽量，看心情😂）</li>\n<li>与计算机技术相关的技术总结，比如一些教程或方法等，用作备忘，大致分为以下几类：<ul>\n<li>编程语言知识总结</li>\n<li>计算机网络技术</li>\n<li>程序开发</li>\n<li>黑苹果</li>\n</ul>\n</li>\n<li>个人摄影作品以及其他优秀摄影作品的展览</li>\n<li>一些民航知识</li>\n<li>其他内容</li>\n</ul>\n<h2 id=\"关于Astrobear\"><a href=\"#关于Astrobear\" class=\"headerlink\" title=\"关于Astrobear\"></a>关于Astrobear</h2><p>站长现在（2020年1月）是一个大三学生，专业是探测制导与控制技术。从小的梦想是成为一名飞行员，但是高三体检被刷了😭。于是迷迷糊糊就到了现在的学校，进了现在的专业。高中的时候加入了学校天文社，从那时起喜欢上了天文（摄影）。本渣在大一上学期学了C语言以后对计算机技术产生了兴趣，然后在大一暑假几乎独立开发了<del>（合伙人沉迷acm太忙了）</del>一个给自己在的学生组织用的管理类微信小程序，之后在“编程”的道路上越走越远。</p>\n<p>总结一下，Astrobear是：学控制的梦想成为飞行员的天文和计算机爱好者。</p>\n<p>由于我并不是计算机专业的，所以关于计算机技术这一块是本着“拿来主义”的态度去学的——会用就行。因此在这方面难免会有疏漏错误之处，也请大家海涵。</p>\n<p>多亏有了互联网的发展，知识的传播可以如此地迅速。在此，对之前对我有过帮助的博主表示衷心的感谢！</p>\n"},{"title":"APIs of Multirotor in Airsim","date":"2020-01-15T15:40:00.000Z","thumbnail":"https://cn.bing.com/th?id=OIP.o6vbAWXSs3ffmE8NXNaZ4QHaEM&pid=Api&rs=1","_content":"\n### APIs of Multirotor in Airsim\n\nby Astrobear\n\n#### Preface\n\n- All APIs listed below need to add the suffix `.join()`. Actually, `.join()` is a call on Python's main process to wait for the thread to complete.\n- All APIs listed below has a hidden parameter, which is `vehicle_name`. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.\n- This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.\n\n#### Control APIs\n\n**`takeoffAsync(timeout_sec)`**: the multirotor will take off when this command is being executed. \n\n- `timeout_sec`: take off time, second. Better to greater than 3s but less than 10s.\n\n`hoverAsync()`: the multirotor will maintain its attitude when executed.\n\n**`landAsync(timeout_sec)`**: the multirotor will land when executed.\n\n- `timeout_sec`: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.\n\n**`goHomeAsync(timeout_sec)`**: the multirotor will fly back to its starting point automatically.\n\n- `timeout_sec`: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n\n**`moveByAngleZAsync(pitch, roll, z, yaw, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `z`: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.\n- `yaw`: angle of yaw, radian.\n- `duration`: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.\n\n**`moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `throttle`: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.\n- `yaw_rate`: angular velocity at yaw axis, radian per second.\n- `duration`: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.\n\n**`moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)`**: change the velocity of the multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `vz`: velocity projected at z axis, meter per second.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)`**: change the velocity at horizontal plane and the altitude of multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `z`: flight altitude, meter.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly according to several given coordinates.\n\n- `path`: a `Vector3r` array, which provides the route coordinates, meter. The form of it is `[airsim.Vector3r(x, y, z), ...]`.\n- `velocity`: flight velocity when traveling, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. \n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.\n\n- `x`: distance projected at x axis, meter.\n- `y`: distance projected at y axis, meter.\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.\n\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`rotateByYawRateAsync(yaw_rate, duration)`**: the multirotor will yaw at the given yaw rate.\n\n- `yaw_rate`: yawing angular velocity, degree per second. \n- `duration`: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.\n","source":"_posts/AirSimMultirotorAPIs.md","raw":"---\ntitle: APIs of Multirotor in Airsim\ndate: 2020-1-15 23:40:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- AirSim\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://cn.bing.com/th?id=OIP.o6vbAWXSs3ffmE8NXNaZ4QHaEM&pid=Api&rs=1\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\n#excerpt: ...\n\n#You can begin to input your article below now.\n\n---\n\n### APIs of Multirotor in Airsim\n\nby Astrobear\n\n#### Preface\n\n- All APIs listed below need to add the suffix `.join()`. Actually, `.join()` is a call on Python's main process to wait for the thread to complete.\n- All APIs listed below has a hidden parameter, which is `vehicle_name`. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.\n- This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.\n\n#### Control APIs\n\n**`takeoffAsync(timeout_sec)`**: the multirotor will take off when this command is being executed. \n\n- `timeout_sec`: take off time, second. Better to greater than 3s but less than 10s.\n\n`hoverAsync()`: the multirotor will maintain its attitude when executed.\n\n**`landAsync(timeout_sec)`**: the multirotor will land when executed.\n\n- `timeout_sec`: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.\n\n**`goHomeAsync(timeout_sec)`**: the multirotor will fly back to its starting point automatically.\n\n- `timeout_sec`: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n\n**`moveByAngleZAsync(pitch, roll, z, yaw, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `z`: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.\n- `yaw`: angle of yaw, radian.\n- `duration`: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.\n\n**`moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `throttle`: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.\n- `yaw_rate`: angular velocity at yaw axis, radian per second.\n- `duration`: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.\n\n**`moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)`**: change the velocity of the multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `vz`: velocity projected at z axis, meter per second.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)`**: change the velocity at horizontal plane and the altitude of multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `z`: flight altitude, meter.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly according to several given coordinates.\n\n- `path`: a `Vector3r` array, which provides the route coordinates, meter. The form of it is `[airsim.Vector3r(x, y, z), ...]`.\n- `velocity`: flight velocity when traveling, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. \n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.\n\n- `x`: distance projected at x axis, meter.\n- `y`: distance projected at y axis, meter.\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.\n\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`rotateByYawRateAsync(yaw_rate, duration)`**: the multirotor will yaw at the given yaw rate.\n\n- `yaw_rate`: yawing angular velocity, degree per second. \n- `duration`: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.\n","slug":"AirSimMultirotorAPIs","published":1,"updated":"2020-01-17T15:54:41.295Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck6ax1lg50001j1p27ayd7j78","content":"<h3 id=\"APIs-of-Multirotor-in-Airsim\"><a href=\"#APIs-of-Multirotor-in-Airsim\" class=\"headerlink\" title=\"APIs of Multirotor in Airsim\"></a>APIs of Multirotor in Airsim</h3><p>by Astrobear</p>\n<h4 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h4><ul>\n<li>All APIs listed below need to add the suffix <code>.join()</code>. Actually, <code>.join()</code> is a call on Python’s main process to wait for the thread to complete.</li>\n<li>All APIs listed below has a hidden parameter, which is <code>vehicle_name</code>. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.</li>\n<li>This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.</li>\n</ul>\n<h4 id=\"Control-APIs\"><a href=\"#Control-APIs\" class=\"headerlink\" title=\"Control APIs\"></a>Control APIs</h4><p><strong><code>takeoffAsync(timeout_sec)</code></strong>: the multirotor will take off when this command is being executed. </p>\n<ul>\n<li><code>timeout_sec</code>: take off time, second. Better to greater than 3s but less than 10s.</li>\n</ul>\n<p><code>hoverAsync()</code>: the multirotor will maintain its attitude when executed.</p>\n<p><strong><code>landAsync(timeout_sec)</code></strong>: the multirotor will land when executed.</p>\n<ul>\n<li><code>timeout_sec</code>: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.</li>\n</ul>\n<p><strong><code>goHomeAsync(timeout_sec)</code></strong>: the multirotor will fly back to its starting point automatically.</p>\n<ul>\n<li><code>timeout_sec</code>: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n</ul>\n<p><strong><code>moveByAngleZAsync(pitch, roll, z, yaw, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>z</code>: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.</li>\n<li><code>yaw</code>: angle of yaw, radian.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.</li>\n</ul>\n<p><strong><code>moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>throttle</code>: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.</li>\n<li><code>yaw_rate</code>: angular velocity at yaw axis, radian per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.</li>\n</ul>\n<p><strong><code>moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)</code></strong>: change the velocity of the multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>vz</code>: velocity projected at z axis, meter per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)</code></strong>: change the velocity at horizontal plane and the altitude of multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly according to several given coordinates.</p>\n<ul>\n<li><code>path</code>: a <code>Vector3r</code> array, which provides the route coordinates, meter. The form of it is <code>[airsim.Vector3r(x, y, z), ...]</code>.</li>\n<li><code>velocity</code>: flight velocity when traveling, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. </li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.</p>\n<ul>\n<li><code>x</code>: distance projected at x axis, meter.</li>\n<li><code>y</code>: distance projected at y axis, meter.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.</p>\n<ul>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>rotateByYawRateAsync(yaw_rate, duration)</code></strong>: the multirotor will yaw at the given yaw rate.</p>\n<ul>\n<li><code>yaw_rate</code>: yawing angular velocity, degree per second. </li>\n<li><code>duration</code>: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"APIs-of-Multirotor-in-Airsim\"><a href=\"#APIs-of-Multirotor-in-Airsim\" class=\"headerlink\" title=\"APIs of Multirotor in Airsim\"></a>APIs of Multirotor in Airsim</h3><p>by Astrobear</p>\n<h4 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h4><ul>\n<li>All APIs listed below need to add the suffix <code>.join()</code>. Actually, <code>.join()</code> is a call on Python’s main process to wait for the thread to complete.</li>\n<li>All APIs listed below has a hidden parameter, which is <code>vehicle_name</code>. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.</li>\n<li>This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.</li>\n</ul>\n<h4 id=\"Control-APIs\"><a href=\"#Control-APIs\" class=\"headerlink\" title=\"Control APIs\"></a>Control APIs</h4><p><strong><code>takeoffAsync(timeout_sec)</code></strong>: the multirotor will take off when this command is being executed. </p>\n<ul>\n<li><code>timeout_sec</code>: take off time, second. Better to greater than 3s but less than 10s.</li>\n</ul>\n<p><code>hoverAsync()</code>: the multirotor will maintain its attitude when executed.</p>\n<p><strong><code>landAsync(timeout_sec)</code></strong>: the multirotor will land when executed.</p>\n<ul>\n<li><code>timeout_sec</code>: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.</li>\n</ul>\n<p><strong><code>goHomeAsync(timeout_sec)</code></strong>: the multirotor will fly back to its starting point automatically.</p>\n<ul>\n<li><code>timeout_sec</code>: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n</ul>\n<p><strong><code>moveByAngleZAsync(pitch, roll, z, yaw, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>z</code>: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.</li>\n<li><code>yaw</code>: angle of yaw, radian.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.</li>\n</ul>\n<p><strong><code>moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>throttle</code>: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.</li>\n<li><code>yaw_rate</code>: angular velocity at yaw axis, radian per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.</li>\n</ul>\n<p><strong><code>moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)</code></strong>: change the velocity of the multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>vz</code>: velocity projected at z axis, meter per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)</code></strong>: change the velocity at horizontal plane and the altitude of multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly according to several given coordinates.</p>\n<ul>\n<li><code>path</code>: a <code>Vector3r</code> array, which provides the route coordinates, meter. The form of it is <code>[airsim.Vector3r(x, y, z), ...]</code>.</li>\n<li><code>velocity</code>: flight velocity when traveling, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. </li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.</p>\n<ul>\n<li><code>x</code>: distance projected at x axis, meter.</li>\n<li><code>y</code>: distance projected at y axis, meter.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.</p>\n<ul>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>rotateByYawRateAsync(yaw_rate, duration)</code></strong>: the multirotor will yaw at the given yaw rate.</p>\n<ul>\n<li><code>yaw_rate</code>: yawing angular velocity, degree per second. </li>\n<li><code>duration</code>: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.</li>\n</ul>\n"},{"title":"Summarize of Reinforcement Learning 3","date":"2020-02-01T09:12:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg","excerpt":"Introduction to MC and TD.","_content":"\n### Introduction\n\nIn the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss *model-free algorithms* in this article. \n\nThroughout this article, we will assume an *infinite horizon* as well as *stationary rewards, transition probabilities and policies*.\n\nFirst comes the definition of *history*: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: \n\n$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},...,s_{j,L_j})$, \n\nwhere $L_j$ is the length of the interaction (interaction between agent and environment). \n\nIn the article *Summarize of Reinforcement Learning 2* I introduced the *iterative solution* of value function, which is\n\n$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$\n\n​          $=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$.\n\nThis ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. \n\n### Monte Carlo on policy evaluation\n\nIn general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. \n\nIn reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: \n\n- Execute a rollout of policy until termination many times\n- Record the returns $G_t$ that we observe when starting at state $s$\n- Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. \n\nFigure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.\n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg)\n\n#### How to evaluate the good and bad of an algorithm\n\nWe use three quntities to evaluate the good and bad of an algorithm.\n\nConsider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and it's a function of observed data $x$. Then we have these quantities of the estimator: \n\nBias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, \n\nVariance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, \n\nMean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. \n\n#### First-Visit Monte Carlo\n\nHere is the algorithm of First-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $i$ `do`\n\n​\t `while` **first time $t$** that the state $s$ is visited in episode $h_i$ `do`\n\n​\t\t$N(s)=N(s)+1$\n\n​\t\t$G(s)=G(s)+G_{i,t}$\n\n​\t\t$V(s)=G(s)/N(s)$ \n\n`return` $V(s)$\n\nFirst-Visit Monte Carlo estimator is an unbised estimator.\n\n#### Every-Visit Monte Carlo\n\nHere is the algorithm of Every-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $h_i$ `do`\n\n​\t `while` **every time $t$** that the state $s$ is visited in episode $i$ `do`\n\n​\t\t$N(s)=N(s)+1$\n\n​\t\t$G(s)=G(s)+G_{i,t}$\n\n​\t\t$V(s)=G(s)/N(s)$ \n\n`return` $V(s)$\n\nEvery-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. \n\n#### Increment First-Visit/Every-Visit Monte Carlo\n\nWe can replace $V(s)=G(s)/N(s)$ in both two algorithms by \n\n$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nBecause\n\n${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nReplacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general *Incremental Monte Carlo on policy evaluation*. Setting $\\alpha > {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. \n\n### Temporal Difference (TD) Learning\n\nTD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. \n\nIn dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have \n\n$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, \n\nand this is the TD learning update. \n\nIn TD learning update, there are two concepts which are *TD error* and *TD target*. TD error is written as below: \n\n$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. \n\nAnd here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: \n\n$r_t+\\gamma V^\\pi(s_{t+1})$. \n\nThe algorithm of TD learning is shown below.\n\nInitialize $V^\\pi(s)=0,\\ s\\in S$\n\n`while` True `do`\n\n​\tSample tuple $(s_t,a_t,r_t,s_{t+1})$ \n\n​\t$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ \n\nIt is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that's why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.\n\nIn reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. \n\nFigure 2 shows a diagram expressing TD learning. \n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS3F2.png)\n\n### Summary\n\nTable below gives some fundamental properties of these three algorithms (DP, MC, TD). \n\n| Properties                                                   | DP   | MC                   | TD   |\n| ------------------------------------------------------------ | ---- | -------------------- | ---- |\n| Useble when no models of current domain                      | No   | Yes                  | Yes  |\n| Handles continuing domains (episodes will never terminate)   | Yes  | No                   | Yes  |\n| Handles Non-Markovian domains                                | No   | Yes                  | No   |\n| Coverges to true value in limit (satisfying some conditions) | Yes  | Yes                  | Yes  |\n| Unbised estimate of value                                    | N/A  | Yes (First-Visit MC) | No   |\n| Variance                                                     | N/A  | High                 | Low  |\n\nFigure 3 shows some other properties that may help us to choose the algorithm. \n\n![Figure 3](https://astrobear.top/resource/astroblog/content/RLS3F3.png)\n\n### Batch Monte Carlo and Temporal Difference\n\nThe batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. \n\nIn the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where\n\n![Figure 4](https://astrobear.top/resource/astroblog/content/RLS3F4.png). \n\nThe value function derived from the maximum likehood MDP model is known as the *certainty equivalence estimate*. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.","source":"_posts/RLSummarize3.md","raw":"---\ntitle: Summarize of Reinforcement Learning 3\ndate: 2020-2-1 17:12:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Introduction to MC and TD.\n\n#You can begin to input your article below now.\n\n---\n\n### Introduction\n\nIn the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss *model-free algorithms* in this article. \n\nThroughout this article, we will assume an *infinite horizon* as well as *stationary rewards, transition probabilities and policies*.\n\nFirst comes the definition of *history*: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: \n\n$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},...,s_{j,L_j})$, \n\nwhere $L_j$ is the length of the interaction (interaction between agent and environment). \n\nIn the article *Summarize of Reinforcement Learning 2* I introduced the *iterative solution* of value function, which is\n\n$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$\n\n​          $=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$.\n\nThis ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. \n\n### Monte Carlo on policy evaluation\n\nIn general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. \n\nIn reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: \n\n- Execute a rollout of policy until termination many times\n- Record the returns $G_t$ that we observe when starting at state $s$\n- Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. \n\nFigure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.\n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg)\n\n#### How to evaluate the good and bad of an algorithm\n\nWe use three quntities to evaluate the good and bad of an algorithm.\n\nConsider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and it's a function of observed data $x$. Then we have these quantities of the estimator: \n\nBias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, \n\nVariance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, \n\nMean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. \n\n#### First-Visit Monte Carlo\n\nHere is the algorithm of First-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $i$ `do`\n\n​\t `while` **first time $t$** that the state $s$ is visited in episode $h_i$ `do`\n\n​\t\t$N(s)=N(s)+1$\n\n​\t\t$G(s)=G(s)+G_{i,t}$\n\n​\t\t$V(s)=G(s)/N(s)$ \n\n`return` $V(s)$\n\nFirst-Visit Monte Carlo estimator is an unbised estimator.\n\n#### Every-Visit Monte Carlo\n\nHere is the algorithm of Every-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $h_i$ `do`\n\n​\t `while` **every time $t$** that the state $s$ is visited in episode $i$ `do`\n\n​\t\t$N(s)=N(s)+1$\n\n​\t\t$G(s)=G(s)+G_{i,t}$\n\n​\t\t$V(s)=G(s)/N(s)$ \n\n`return` $V(s)$\n\nEvery-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. \n\n#### Increment First-Visit/Every-Visit Monte Carlo\n\nWe can replace $V(s)=G(s)/N(s)$ in both two algorithms by \n\n$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nBecause\n\n${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nReplacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general *Incremental Monte Carlo on policy evaluation*. Setting $\\alpha > {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. \n\n### Temporal Difference (TD) Learning\n\nTD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. \n\nIn dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have \n\n$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, \n\nand this is the TD learning update. \n\nIn TD learning update, there are two concepts which are *TD error* and *TD target*. TD error is written as below: \n\n$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. \n\nAnd here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: \n\n$r_t+\\gamma V^\\pi(s_{t+1})$. \n\nThe algorithm of TD learning is shown below.\n\nInitialize $V^\\pi(s)=0,\\ s\\in S$\n\n`while` True `do`\n\n​\tSample tuple $(s_t,a_t,r_t,s_{t+1})$ \n\n​\t$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ \n\nIt is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that's why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.\n\nIn reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. \n\nFigure 2 shows a diagram expressing TD learning. \n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS3F2.png)\n\n### Summary\n\nTable below gives some fundamental properties of these three algorithms (DP, MC, TD). \n\n| Properties                                                   | DP   | MC                   | TD   |\n| ------------------------------------------------------------ | ---- | -------------------- | ---- |\n| Useble when no models of current domain                      | No   | Yes                  | Yes  |\n| Handles continuing domains (episodes will never terminate)   | Yes  | No                   | Yes  |\n| Handles Non-Markovian domains                                | No   | Yes                  | No   |\n| Coverges to true value in limit (satisfying some conditions) | Yes  | Yes                  | Yes  |\n| Unbised estimate of value                                    | N/A  | Yes (First-Visit MC) | No   |\n| Variance                                                     | N/A  | High                 | Low  |\n\nFigure 3 shows some other properties that may help us to choose the algorithm. \n\n![Figure 3](https://astrobear.top/resource/astroblog/content/RLS3F3.png)\n\n### Batch Monte Carlo and Temporal Difference\n\nThe batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. \n\nIn the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where\n\n![Figure 4](https://astrobear.top/resource/astroblog/content/RLS3F4.png). \n\nThe value function derived from the maximum likehood MDP model is known as the *certainty equivalence estimate*. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.","slug":"RLSummarize3","published":1,"updated":"2020-02-06T14:57:47.072Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck6ax1lg80003j1p26y1j8hbi","content":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss <em>model-free algorithms</em> in this article. </p>\n<p>Throughout this article, we will assume an <em>infinite horizon</em> as well as <em>stationary rewards, transition probabilities and policies</em>.</p>\n<p>First comes the definition of <em>history</em>: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: </p>\n<p>$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},…,s_{j,L_j})$, </p>\n<p>where $L_j$ is the length of the interaction (interaction between agent and environment). </p>\n<p>In the article <em>Summarize of Reinforcement Learning 2</em> I introduced the <em>iterative solution</em> of value function, which is</p>\n<p>$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$</p>\n<p>​          $=R(s)+\\gamma \\sum P(s’|s)V_{t+1}(s’), \\forall t=0,…,H-1,V_H(s)=0$.</p>\n<p>This ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. </p>\n<h3 id=\"Monte-Carlo-on-policy-evaluation\"><a href=\"#Monte-Carlo-on-policy-evaluation\" class=\"headerlink\" title=\"Monte Carlo on policy evaluation\"></a>Monte Carlo on policy evaluation</h3><p>In general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. </p>\n<p>In reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: </p>\n<ul>\n<li>Execute a rollout of policy until termination many times</li>\n<li>Record the returns $G_t$ that we observe when starting at state $s$</li>\n<li>Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. </li>\n</ul>\n<p>Figure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg\" alt=\"Figure 1\"></p>\n<h4 id=\"How-to-evaluate-the-good-and-bad-of-an-algorithm\"><a href=\"#How-to-evaluate-the-good-and-bad-of-an-algorithm\" class=\"headerlink\" title=\"How to evaluate the good and bad of an algorithm\"></a>How to evaluate the good and bad of an algorithm</h4><p>We use three quntities to evaluate the good and bad of an algorithm.</p>\n<p>Consider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and it’s a function of observed data $x$. Then we have these quantities of the estimator: </p>\n<p>Bias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, </p>\n<p>Variance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, </p>\n<p>Mean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. </p>\n<h4 id=\"First-Visit-Monte-Carlo\"><a href=\"#First-Visit-Monte-Carlo\" class=\"headerlink\" title=\"First-Visit Monte Carlo\"></a>First-Visit Monte Carlo</h4><p>Here is the algorithm of First-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>\n<p>​     <code>while</code> <strong>first time $t$</strong> that the state $s$ is visited in episode $h_i$ <code>do</code></p>\n<p>​        $N(s)=N(s)+1$</p>\n<p>​        $G(s)=G(s)+G_{i,t}$</p>\n<p>​        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$</p>\n<p>First-Visit Monte Carlo estimator is an unbised estimator.</p>\n<h4 id=\"Every-Visit-Monte-Carlo\"><a href=\"#Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Every-Visit Monte Carlo\"></a>Every-Visit Monte Carlo</h4><p>Here is the algorithm of Every-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $h_i$ <code>do</code></p>\n<p>​     <code>while</code> <strong>every time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>\n<p>​        $N(s)=N(s)+1$</p>\n<p>​        $G(s)=G(s)+G_{i,t}$</p>\n<p>​        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$</p>\n<p>Every-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. </p>\n<h4 id=\"Increment-First-Visit-Every-Visit-Monte-Carlo\"><a href=\"#Increment-First-Visit-Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Increment First-Visit/Every-Visit Monte Carlo\"></a>Increment First-Visit/Every-Visit Monte Carlo</h4><p>We can replace $V(s)=G(s)/N(s)$ in both two algorithms by </p>\n<p>$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Because</p>\n<p>${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Replacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general <em>Incremental Monte Carlo on policy evaluation</em>. Setting $\\alpha &gt; {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. </p>\n<h3 id=\"Temporal-Difference-TD-Learning\"><a href=\"#Temporal-Difference-TD-Learning\" class=\"headerlink\" title=\"Temporal Difference (TD) Learning\"></a>Temporal Difference (TD) Learning</h3><p>TD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. </p>\n<p>In dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have </p>\n<p>$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, </p>\n<p>and this is the TD learning update. </p>\n<p>In TD learning update, there are two concepts which are <em>TD error</em> and <em>TD target</em>. TD error is written as below: </p>\n<p>$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. </p>\n<p>And here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: </p>\n<p>$r_t+\\gamma V^\\pi(s_{t+1})$. </p>\n<p>The algorithm of TD learning is shown below.</p>\n<p>Initialize $V^\\pi(s)=0,\\ s\\in S$</p>\n<p><code>while</code> True <code>do</code></p>\n<p>​    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ </p>\n<p>​    $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ </p>\n<p>It is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that’s why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.</p>\n<p>In reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. </p>\n<p>Figure 2 shows a diagram expressing TD learning. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F2.png\" alt=\"Figure 2\"></p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>Table below gives some fundamental properties of these three algorithms (DP, MC, TD). </p>\n<table>\n<thead>\n<tr>\n<th>Properties</th>\n<th>DP</th>\n<th>MC</th>\n<th>TD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Useble when no models of current domain</td>\n<td>No</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles continuing domains (episodes will never terminate)</td>\n<td>Yes</td>\n<td>No</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles Non-Markovian domains</td>\n<td>No</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Coverges to true value in limit (satisfying some conditions)</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Unbised estimate of value</td>\n<td>N/A</td>\n<td>Yes (First-Visit MC)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Variance</td>\n<td>N/A</td>\n<td>High</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>Figure 3 shows some other properties that may help us to choose the algorithm. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F3.png\" alt=\"Figure 3\"></p>\n<h3 id=\"Batch-Monte-Carlo-and-Temporal-Difference\"><a href=\"#Batch-Monte-Carlo-and-Temporal-Difference\" class=\"headerlink\" title=\"Batch Monte Carlo and Temporal Difference\"></a>Batch Monte Carlo and Temporal Difference</h3><p>The batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. </p>\n<p>In the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F4.png\" alt=\"Figure 4\">. </p>\n<p>The value function derived from the maximum likehood MDP model is known as the <em>certainty equivalence estimate</em>. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.</p>\n","site":{"data":{}},"more":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss <em>model-free algorithms</em> in this article. </p>\n<p>Throughout this article, we will assume an <em>infinite horizon</em> as well as <em>stationary rewards, transition probabilities and policies</em>.</p>\n<p>First comes the definition of <em>history</em>: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: </p>\n<p>$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},…,s_{j,L_j})$, </p>\n<p>where $L_j$ is the length of the interaction (interaction between agent and environment). </p>\n<p>In the article <em>Summarize of Reinforcement Learning 2</em> I introduced the <em>iterative solution</em> of value function, which is</p>\n<p>$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$</p>\n<p>​          $=R(s)+\\gamma \\sum P(s’|s)V_{t+1}(s’), \\forall t=0,…,H-1,V_H(s)=0$.</p>\n<p>This ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. </p>\n<h3 id=\"Monte-Carlo-on-policy-evaluation\"><a href=\"#Monte-Carlo-on-policy-evaluation\" class=\"headerlink\" title=\"Monte Carlo on policy evaluation\"></a>Monte Carlo on policy evaluation</h3><p>In general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. </p>\n<p>In reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: </p>\n<ul>\n<li>Execute a rollout of policy until termination many times</li>\n<li>Record the returns $G_t$ that we observe when starting at state $s$</li>\n<li>Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. </li>\n</ul>\n<p>Figure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg\" alt=\"Figure 1\"></p>\n<h4 id=\"How-to-evaluate-the-good-and-bad-of-an-algorithm\"><a href=\"#How-to-evaluate-the-good-and-bad-of-an-algorithm\" class=\"headerlink\" title=\"How to evaluate the good and bad of an algorithm\"></a>How to evaluate the good and bad of an algorithm</h4><p>We use three quntities to evaluate the good and bad of an algorithm.</p>\n<p>Consider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and it’s a function of observed data $x$. Then we have these quantities of the estimator: </p>\n<p>Bias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, </p>\n<p>Variance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, </p>\n<p>Mean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. </p>\n<h4 id=\"First-Visit-Monte-Carlo\"><a href=\"#First-Visit-Monte-Carlo\" class=\"headerlink\" title=\"First-Visit Monte Carlo\"></a>First-Visit Monte Carlo</h4><p>Here is the algorithm of First-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>\n<p>​     <code>while</code> <strong>first time $t$</strong> that the state $s$ is visited in episode $h_i$ <code>do</code></p>\n<p>​        $N(s)=N(s)+1$</p>\n<p>​        $G(s)=G(s)+G_{i,t}$</p>\n<p>​        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$</p>\n<p>First-Visit Monte Carlo estimator is an unbised estimator.</p>\n<h4 id=\"Every-Visit-Monte-Carlo\"><a href=\"#Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Every-Visit Monte Carlo\"></a>Every-Visit Monte Carlo</h4><p>Here is the algorithm of Every-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $h_i$ <code>do</code></p>\n<p>​     <code>while</code> <strong>every time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>\n<p>​        $N(s)=N(s)+1$</p>\n<p>​        $G(s)=G(s)+G_{i,t}$</p>\n<p>​        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$</p>\n<p>Every-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. </p>\n<h4 id=\"Increment-First-Visit-Every-Visit-Monte-Carlo\"><a href=\"#Increment-First-Visit-Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Increment First-Visit/Every-Visit Monte Carlo\"></a>Increment First-Visit/Every-Visit Monte Carlo</h4><p>We can replace $V(s)=G(s)/N(s)$ in both two algorithms by </p>\n<p>$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Because</p>\n<p>${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Replacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general <em>Incremental Monte Carlo on policy evaluation</em>. Setting $\\alpha &gt; {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. </p>\n<h3 id=\"Temporal-Difference-TD-Learning\"><a href=\"#Temporal-Difference-TD-Learning\" class=\"headerlink\" title=\"Temporal Difference (TD) Learning\"></a>Temporal Difference (TD) Learning</h3><p>TD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. </p>\n<p>In dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have </p>\n<p>$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, </p>\n<p>and this is the TD learning update. </p>\n<p>In TD learning update, there are two concepts which are <em>TD error</em> and <em>TD target</em>. TD error is written as below: </p>\n<p>$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. </p>\n<p>And here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: </p>\n<p>$r_t+\\gamma V^\\pi(s_{t+1})$. </p>\n<p>The algorithm of TD learning is shown below.</p>\n<p>Initialize $V^\\pi(s)=0,\\ s\\in S$</p>\n<p><code>while</code> True <code>do</code></p>\n<p>​    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ </p>\n<p>​    $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ </p>\n<p>It is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that’s why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.</p>\n<p>In reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. </p>\n<p>Figure 2 shows a diagram expressing TD learning. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F2.png\" alt=\"Figure 2\"></p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>Table below gives some fundamental properties of these three algorithms (DP, MC, TD). </p>\n<table>\n<thead>\n<tr>\n<th>Properties</th>\n<th>DP</th>\n<th>MC</th>\n<th>TD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Useble when no models of current domain</td>\n<td>No</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles continuing domains (episodes will never terminate)</td>\n<td>Yes</td>\n<td>No</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles Non-Markovian domains</td>\n<td>No</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Coverges to true value in limit (satisfying some conditions)</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Unbised estimate of value</td>\n<td>N/A</td>\n<td>Yes (First-Visit MC)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Variance</td>\n<td>N/A</td>\n<td>High</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>Figure 3 shows some other properties that may help us to choose the algorithm. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F3.png\" alt=\"Figure 3\"></p>\n<h3 id=\"Batch-Monte-Carlo-and-Temporal-Difference\"><a href=\"#Batch-Monte-Carlo-and-Temporal-Difference\" class=\"headerlink\" title=\"Batch Monte Carlo and Temporal Difference\"></a>Batch Monte Carlo and Temporal Difference</h3><p>The batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. </p>\n<p>In the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F4.png\" alt=\"Figure 4\">. </p>\n<p>The value function derived from the maximum likehood MDP model is known as the <em>certainty equivalence estimate</em>. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.</p>\n"},{"title":"Summarize of Reinforcement Learning 2","date":"2020-01-18T13:06:00.000Z","thumbnail":"https://pic1.zhimg.com/80/v2-e1e894383536e4ff019f63e5507c2a18_hd.png","excerpt":"Introduction to MP, MRP, and MDP.","_content":"\n### Markov process (MP)\n\nMarkov process is a stochastic process that satisfies the Markov property, which means it is \"memoryless\" and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. \n\nWe need to make two assumptions before we define the Markov process. The first assumption is that *the state of MP is finite*, and we have $s_i\\in S, i\\in1,2,...$ , where $|S|<\\infty$. The second assumption is that *the transition probabilities are time independent*. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,...$.\n\nBase on these two assumption, we can define a *transition transform matrix*:\n\n![](https://astrobear.top/resource/astroblog/content/RLS2F0.png)\n\nThe size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.\n\nHenceforth, we can define a Markov process using a tuple $(S,\\bf P)$.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n\nBy calculating $S\\bf P$ we can get the distribution of the new state.\n\nFigure 1 shows a student MP example.\n\n![Figure 1](https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png)\n\n### Markov reward process (MRP)\n\nMRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explaintions.\n\n#### Reward function\n\nWhen we are moving from the current state $s$ to a *successor state* $s'$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s'$ ). For a state $s\\in S$, we define the expected reward by\n\n$R(s)=\\Bbb E[r_t|s_t=s]$. \n\nHere we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.\n\n#### Horizon\n\nIt is defined as the number of time steps in each episode of the process. An *episode* is the whole process of a round of training. The horizon can be finite or infinite.\n\n#### Return\n\nThe return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon *H*. We can calculate the return using\n\n$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.\n\n#### State value function\n\nThe state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression\n\n$V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\nIf the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.\n\n#### Discount factor\n\nWe design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma <1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.\n\nFigure 2 and 3 shows an example of how to calculate the return.\n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS2F2.png)\n\n![Figure 3](https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg)\n\nIt is significant to find out a value function while many problems of RL is how to get a value function essentially.\n\n#### Computing the value function\n\nWe have three ways to compute the value function.\n\n- Simulation. Through simulation, we can get the value function by averaing many returns of episodes.\n\n- Analytic solution. We have defined the state value function \n\n  $V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\n  Then, make a little transformation, see Figure 4 in detail. \n\n  ![Figure 4](https://astrobear.top/resource/astroblog/content/RLS2F4.png)\n\n  Then, we have\n\n  $V(s)=R(s)+\\gamma \\sum P(s'|s)V(s')$, \n\n  \n\n  $V=R+\\gamma\\bf P\\mit V$. \n\n  Therefore we have\n\n  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. \n\n  If $0<\\gamma<1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.\n\n  Notice that $s'$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.\n\n  ![Figure 5](https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png)\n\n- Iterative solution. \n\n  $V_t(s)=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$. \n\n  We can iterate it again and again and use $|V_t-V_{t-1}|<\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. \n\n### Markov decision process (MDP)\n\nMDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. \n\n- $S$: A finite state space.\n- $A$: A finite set of actions which are available from each state $s$.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explanations.\n\n#### Notifications\n\n- Both $S$ and $A$ are finite.\n\n- In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as\n\n  $P(s_{t+1}|s_t,a_t)$.\n\n- In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as\n\n  $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n- Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.\n\n#### Policy\n\nBefore we mention the state value function, we need to talk about the policy for the MDP first. \n\nA policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be *varying with time*, especially when the horizon is finite. A policy can be written as\n\n$\\bf \\pi\\mit(a|s)=P(a_t=a|s_t=s)$. \n\nIf given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: \n\n- $P^\\pi(s'|s)=\\sum_{a\\in A}\\pi(a|s) P(s'|s,a)$\n\n  When we have a policy $\\pi$, the probability of the state transforms from $s$ to $s'$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s'$ when executing an action $a$.\n\n- $R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$\n\n  When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.\n\n#### Value functions in MDP (Bellman expectation equations)\n\nGiven a policy $\\pi$ can define two quantities: *the state value function* and *the state-action value function*. These two value functions are both *Bellman expectation equations*.\n\n- State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression\n\n  $V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. \n\n  Frequently we will drop the subscript $\\pi$ in the expectation. \n\n- State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form\n\n  $Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. \n\n  It evaluates the value of acting the action $a$ under current state $s$. \n\nNow let's talk about the relationships between these two value functions.\n\nFigure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.\n\n![Figure 6](https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png)\n\nWe can discover that the value of a state can be denoted as\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.\n\nIn a similar way, Figure 7 shows what states that an action can lead to.\n\n![Figure 7](https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png)\n\nWe can also find that \n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^\\pi(s')$. \n\nOn the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s'$ and the probability of getting into that new state. \n\nIf we combine the two Bellman equation with each other, we can get\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^\\pi(s')]$\n\n​            $=R(s',\\pi(s'))+\\gamma\\sum_{s'\\in S}P(s'|s,\\pi(s)) V^\\pi(s')$, \n\nand\n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)\\sum_{a\\in A}\\pi(a'|s')Q_\\pi(s',a')$. \n\nThe example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.\n\n![Figure 8](https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png)\n\n#### Optimality value function (Bellman optimality equation)\n\n- Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. \n- Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.\n\nOptimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. \n\n### Find the best policy\n\nThe best policy is defined precisely as *optimal policy*  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.\n\nFor an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.\n\nWe can compute the optimal policy by\n\n$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,\n\nWhich means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. \n\nIf an optimal policy exists then its value function must be a fixed point of the operator $B^*$. \n\n#### Bellman optimality backup operator\n\nBellman optimality backup operator is written as $B^*$ with a value function behind it \n\n$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V(s')$. \n\nIf $\\gamma<1$, $B^*$ is a strict contraction and has a unique fixed point. This means \n\n$B^*V(s)\\geq V^\\pi(s)$.\n\nBellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.\n\nNext I'll briefly introduce some algorithms to compute the optimal value function and an optimal policy.\n\n#### Policy search\n\nThis algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a $V^*(s)$  and  $\\pi^*(s)$. \n\n#### Policy iteration\n\nThe algorithm of policy iteration is shown below: \n\n`while` True `do`\n\n​\t$V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)\n\n​\t$\\pi^*$ = Policy improvement $(M,V^\\pi)$\n\n`if` $\\pi^*(s)=\\pi(s)$ `then`\n\n​\t`break`\n\n`else`\n\n​\t$\\pi$ = $\\pi^*$\n\n$V^*$ = $V^\\pi$ . \n\nPolicy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute\n\n$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^{\\pi i}(s')$ \n\nfor all the $a$ and $s$ and then take the max\n\n`return` $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.\n\nNotice that there is a relationship\n\n$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.\n\nThis means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.\n\n#### Value iteration\n\nThe algorithm of value iteration is shown below:\n\n$V'(s)=0, V(s)=\\infty$, for all $s\\in S$\n\n`while` $||V-V'||_\\infty>\\epsilon$ `do`\n\n​\t$V=V'$\n\n​\t$V'(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V'(s)$, for all states $s\\in S$ \n\n$V^*=V$, for all $s\\in S$ \n\n$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^ *(s'),\\ \\forall s\\in S$ . \n\nThe idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.\n\n","source":"_posts/RLSummarize2.md","raw":"---\ntitle: Summarize of Reinforcement Learning 2\ndate: 2020-1-18 21:06:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://pic1.zhimg.com/80/v2-e1e894383536e4ff019f63e5507c2a18_hd.png\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Introduction to MP, MRP, and MDP.\n\n#You can begin to input your article below now.\n\n---\n\n### Markov process (MP)\n\nMarkov process is a stochastic process that satisfies the Markov property, which means it is \"memoryless\" and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. \n\nWe need to make two assumptions before we define the Markov process. The first assumption is that *the state of MP is finite*, and we have $s_i\\in S, i\\in1,2,...$ , where $|S|<\\infty$. The second assumption is that *the transition probabilities are time independent*. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,...$.\n\nBase on these two assumption, we can define a *transition transform matrix*:\n\n![](https://astrobear.top/resource/astroblog/content/RLS2F0.png)\n\nThe size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.\n\nHenceforth, we can define a Markov process using a tuple $(S,\\bf P)$.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n\nBy calculating $S\\bf P$ we can get the distribution of the new state.\n\nFigure 1 shows a student MP example.\n\n![Figure 1](https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png)\n\n### Markov reward process (MRP)\n\nMRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explaintions.\n\n#### Reward function\n\nWhen we are moving from the current state $s$ to a *successor state* $s'$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s'$ ). For a state $s\\in S$, we define the expected reward by\n\n$R(s)=\\Bbb E[r_t|s_t=s]$. \n\nHere we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.\n\n#### Horizon\n\nIt is defined as the number of time steps in each episode of the process. An *episode* is the whole process of a round of training. The horizon can be finite or infinite.\n\n#### Return\n\nThe return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon *H*. We can calculate the return using\n\n$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.\n\n#### State value function\n\nThe state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression\n\n$V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\nIf the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.\n\n#### Discount factor\n\nWe design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma <1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.\n\nFigure 2 and 3 shows an example of how to calculate the return.\n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS2F2.png)\n\n![Figure 3](https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg)\n\nIt is significant to find out a value function while many problems of RL is how to get a value function essentially.\n\n#### Computing the value function\n\nWe have three ways to compute the value function.\n\n- Simulation. Through simulation, we can get the value function by averaing many returns of episodes.\n\n- Analytic solution. We have defined the state value function \n\n  $V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\n  Then, make a little transformation, see Figure 4 in detail. \n\n  ![Figure 4](https://astrobear.top/resource/astroblog/content/RLS2F4.png)\n\n  Then, we have\n\n  $V(s)=R(s)+\\gamma \\sum P(s'|s)V(s')$, \n\n  \n\n  $V=R+\\gamma\\bf P\\mit V$. \n\n  Therefore we have\n\n  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. \n\n  If $0<\\gamma<1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.\n\n  Notice that $s'$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.\n\n  ![Figure 5](https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png)\n\n- Iterative solution. \n\n  $V_t(s)=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$. \n\n  We can iterate it again and again and use $|V_t-V_{t-1}|<\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. \n\n### Markov decision process (MDP)\n\nMDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. \n\n- $S$: A finite state space.\n- $A$: A finite set of actions which are available from each state $s$.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explanations.\n\n#### Notifications\n\n- Both $S$ and $A$ are finite.\n\n- In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as\n\n  $P(s_{t+1}|s_t,a_t)$.\n\n- In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as\n\n  $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n- Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.\n\n#### Policy\n\nBefore we mention the state value function, we need to talk about the policy for the MDP first. \n\nA policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be *varying with time*, especially when the horizon is finite. A policy can be written as\n\n$\\bf \\pi\\mit(a|s)=P(a_t=a|s_t=s)$. \n\nIf given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: \n\n- $P^\\pi(s'|s)=\\sum_{a\\in A}\\pi(a|s) P(s'|s,a)$\n\n  When we have a policy $\\pi$, the probability of the state transforms from $s$ to $s'$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s'$ when executing an action $a$.\n\n- $R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$\n\n  When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.\n\n#### Value functions in MDP (Bellman expectation equations)\n\nGiven a policy $\\pi$ can define two quantities: *the state value function* and *the state-action value function*. These two value functions are both *Bellman expectation equations*.\n\n- State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression\n\n  $V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. \n\n  Frequently we will drop the subscript $\\pi$ in the expectation. \n\n- State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form\n\n  $Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. \n\n  It evaluates the value of acting the action $a$ under current state $s$. \n\nNow let's talk about the relationships between these two value functions.\n\nFigure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.\n\n![Figure 6](https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png)\n\nWe can discover that the value of a state can be denoted as\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.\n\nIn a similar way, Figure 7 shows what states that an action can lead to.\n\n![Figure 7](https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png)\n\nWe can also find that \n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^\\pi(s')$. \n\nOn the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s'$ and the probability of getting into that new state. \n\nIf we combine the two Bellman equation with each other, we can get\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^\\pi(s')]$\n\n​            $=R(s',\\pi(s'))+\\gamma\\sum_{s'\\in S}P(s'|s,\\pi(s)) V^\\pi(s')$, \n\nand\n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)\\sum_{a\\in A}\\pi(a'|s')Q_\\pi(s',a')$. \n\nThe example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.\n\n![Figure 8](https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png)\n\n#### Optimality value function (Bellman optimality equation)\n\n- Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. \n- Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.\n\nOptimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. \n\n### Find the best policy\n\nThe best policy is defined precisely as *optimal policy*  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.\n\nFor an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.\n\nWe can compute the optimal policy by\n\n$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,\n\nWhich means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. \n\nIf an optimal policy exists then its value function must be a fixed point of the operator $B^*$. \n\n#### Bellman optimality backup operator\n\nBellman optimality backup operator is written as $B^*$ with a value function behind it \n\n$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V(s')$. \n\nIf $\\gamma<1$, $B^*$ is a strict contraction and has a unique fixed point. This means \n\n$B^*V(s)\\geq V^\\pi(s)$.\n\nBellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.\n\nNext I'll briefly introduce some algorithms to compute the optimal value function and an optimal policy.\n\n#### Policy search\n\nThis algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a $V^*(s)$  and  $\\pi^*(s)$. \n\n#### Policy iteration\n\nThe algorithm of policy iteration is shown below: \n\n`while` True `do`\n\n​\t$V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)\n\n​\t$\\pi^*$ = Policy improvement $(M,V^\\pi)$\n\n`if` $\\pi^*(s)=\\pi(s)$ `then`\n\n​\t`break`\n\n`else`\n\n​\t$\\pi$ = $\\pi^*$\n\n$V^*$ = $V^\\pi$ . \n\nPolicy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute\n\n$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^{\\pi i}(s')$ \n\nfor all the $a$ and $s$ and then take the max\n\n`return` $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.\n\nNotice that there is a relationship\n\n$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.\n\nThis means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.\n\n#### Value iteration\n\nThe algorithm of value iteration is shown below:\n\n$V'(s)=0, V(s)=\\infty$, for all $s\\in S$\n\n`while` $||V-V'||_\\infty>\\epsilon$ `do`\n\n​\t$V=V'$\n\n​\t$V'(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V'(s)$, for all states $s\\in S$ \n\n$V^*=V$, for all $s\\in S$ \n\n$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^ *(s'),\\ \\forall s\\in S$ . \n\nThe idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.\n\n","slug":"RLSummarize2","published":1,"updated":"2020-02-06T16:11:35.224Z","_id":"ck6ax1lgh0006j1p22s0p8u3s","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"Markov-process-MP\"><a href=\"#Markov-process-MP\" class=\"headerlink\" title=\"Markov process (MP)\"></a>Markov process (MP)</h3><p>Markov process is a stochastic process that satisfies the Markov property, which means it is “memoryless” and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. </p>\n<p>We need to make two assumptions before we define the Markov process. The first assumption is that <em>the state of MP is finite</em>, and we have $s_i\\in S, i\\in1,2,…$ , where $|S|&lt;\\infty$. The second assumption is that <em>the transition probabilities are time independent</em>. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,…$.</p>\n<p>Base on these two assumption, we can define a <em>transition transform matrix</em>:</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F0.png\" alt=\"\"></p>\n<p>The size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.</p>\n<p>Henceforth, we can define a Markov process using a tuple $(S,\\bf P)$.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n</ul>\n<p>By calculating $S\\bf P$ we can get the distribution of the new state.</p>\n<p>Figure 1 shows a student MP example.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png\" alt=\"Figure 1\"></p>\n<h3 id=\"Markov-reward-process-MRP\"><a href=\"#Markov-reward-process-MRP\" class=\"headerlink\" title=\"Markov reward process (MRP)\"></a>Markov reward process (MRP)</h3><p>MRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explaintions.</p>\n<h4 id=\"Reward-function\"><a href=\"#Reward-function\" class=\"headerlink\" title=\"Reward function\"></a>Reward function</h4><p>When we are moving from the current state $s$ to a <em>successor state</em> $s’$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s’$ ). For a state $s\\in S$, we define the expected reward by</p>\n<p>$R(s)=\\Bbb E[r_t|s_t=s]$. </p>\n<p>Here we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.</p>\n<h4 id=\"Horizon\"><a href=\"#Horizon\" class=\"headerlink\" title=\"Horizon\"></a>Horizon</h4><p>It is defined as the number of time steps in each episode of the process. An <em>episode</em> is the whole process of a round of training. The horizon can be finite or infinite.</p>\n<h4 id=\"Return\"><a href=\"#Return\" class=\"headerlink\" title=\"Return\"></a>Return</h4><p>The return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon <em>H</em>. We can calculate the return using</p>\n<p>$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.</p>\n<h4 id=\"State-value-function\"><a href=\"#State-value-function\" class=\"headerlink\" title=\"State value function\"></a>State value function</h4><p>The state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression</p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>If the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.</p>\n<h4 id=\"Discount-factor\"><a href=\"#Discount-factor\" class=\"headerlink\" title=\"Discount factor\"></a>Discount factor</h4><p>We design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma &lt;1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.</p>\n<p>Figure 2 and 3 shows an example of how to calculate the return.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F2.png\" alt=\"Figure 2\"></p>\n<p><img src=\"https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg\" alt=\"Figure 3\"></p>\n<p>It is significant to find out a value function while many problems of RL is how to get a value function essentially.</p>\n<h4 id=\"Computing-the-value-function\"><a href=\"#Computing-the-value-function\" class=\"headerlink\" title=\"Computing the value function\"></a>Computing the value function</h4><p>We have three ways to compute the value function.</p>\n<ul>\n<li><p>Simulation. Through simulation, we can get the value function by averaing many returns of episodes.</p>\n</li>\n<li><p>Analytic solution. We have defined the state value function </p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>Then, make a little transformation, see Figure 4 in detail. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F4.png\" alt=\"Figure 4\"></p>\n<p>Then, we have</p>\n<p>$V(s)=R(s)+\\gamma \\sum P(s’|s)V(s’)$, </p>\n</li>\n</ul>\n<p>  $V=R+\\gamma\\bf P\\mit V$. </p>\n<p>  Therefore we have</p>\n<p>  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. </p>\n<p>  If $0&lt;\\gamma&lt;1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.</p>\n<p>  Notice that $s’$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.</p>\n<p>  <img src=\"https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png\" alt=\"Figure 5\"></p>\n<ul>\n<li><p>Iterative solution. </p>\n<p>$V_t(s)=R(s)+\\gamma \\sum P(s’|s)V_{t+1}(s’), \\forall t=0,…,H-1,V_H(s)=0$. </p>\n<p>We can iterate it again and again and use $|V_t-V_{t-1}|&lt;\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. </p>\n</li>\n</ul>\n<h3 id=\"Markov-decision-process-MDP\"><a href=\"#Markov-decision-process-MDP\" class=\"headerlink\" title=\"Markov decision process (MDP)\"></a>Markov decision process (MDP)</h3><p>MDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. </p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$A$: A finite set of actions which are available from each state $s$.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explanations.</p>\n<h4 id=\"Notifications\"><a href=\"#Notifications\" class=\"headerlink\" title=\"Notifications\"></a>Notifications</h4><ul>\n<li><p>Both $S$ and $A$ are finite.</p>\n</li>\n<li><p>In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as</p>\n<p>$P(s_{t+1}|s_t,a_t)$.</p>\n</li>\n<li><p>In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as</p>\n<p>$R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n</li>\n<li><p>Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.</p>\n</li>\n</ul>\n<h4 id=\"Policy\"><a href=\"#Policy\" class=\"headerlink\" title=\"Policy\"></a>Policy</h4><p>Before we mention the state value function, we need to talk about the policy for the MDP first. </p>\n<p>A policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be <em>varying with time</em>, especially when the horizon is finite. A policy can be written as</p>\n<p>$\\bf \\pi\\mit(a|s)=P(a_t=a|s_t=s)$. </p>\n<p>If given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: </p>\n<ul>\n<li><p>$P^\\pi(s’|s)=\\sum_{a\\in A}\\pi(a|s) P(s’|s,a)$</p>\n<p>When we have a policy $\\pi$, the probability of the state transforms from $s$ to $s’$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s’$ when executing an action $a$.</p>\n</li>\n<li><p>$R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$</p>\n<p>When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.</p>\n</li>\n</ul>\n<h4 id=\"Value-functions-in-MDP-Bellman-expectation-equations\"><a href=\"#Value-functions-in-MDP-Bellman-expectation-equations\" class=\"headerlink\" title=\"Value functions in MDP (Bellman expectation equations)\"></a>Value functions in MDP (Bellman expectation equations)</h4><p>Given a policy $\\pi$ can define two quantities: <em>the state value function</em> and <em>the state-action value function</em>. These two value functions are both <em>Bellman expectation equations</em>.</p>\n<ul>\n<li><p>State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression</p>\n<p>$V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. </p>\n<p>Frequently we will drop the subscript $\\pi$ in the expectation. </p>\n</li>\n<li><p>State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form</p>\n<p>$Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. </p>\n<p>It evaluates the value of acting the action $a$ under current state $s$. </p>\n</li>\n</ul>\n<p>Now let’s talk about the relationships between these two value functions.</p>\n<p>Figure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png\" alt=\"Figure 6\"></p>\n<p>We can discover that the value of a state can be denoted as</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.</p>\n<p>In a similar way, Figure 7 shows what states that an action can lead to.</p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png\" alt=\"Figure 7\"></p>\n<p>We can also find that </p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)V^\\pi(s’)$. </p>\n<p>On the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s’$ and the probability of getting into that new state. </p>\n<p>If we combine the two Bellman equation with each other, we can get</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V^\\pi(s’)]$</p>\n<p>​            $=R(s’,\\pi(s’))+\\gamma\\sum_{s’\\in S}P(s’|s,\\pi(s)) V^\\pi(s’)$, </p>\n<p>and</p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)\\sum_{a\\in A}\\pi(a’|s’)Q_\\pi(s’,a’)$. </p>\n<p>The example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png\" alt=\"Figure 8\"></p>\n<h4 id=\"Optimality-value-function-Bellman-optimality-equation\"><a href=\"#Optimality-value-function-Bellman-optimality-equation\" class=\"headerlink\" title=\"Optimality value function (Bellman optimality equation)\"></a>Optimality value function (Bellman optimality equation)</h4><ul>\n<li>Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. </li>\n<li>Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.</li>\n</ul>\n<p>Optimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. </p>\n<h3 id=\"Find-the-best-policy\"><a href=\"#Find-the-best-policy\" class=\"headerlink\" title=\"Find the best policy\"></a>Find the best policy</h3><p>The best policy is defined precisely as <em>optimal policy</em>  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.</p>\n<p>For an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.</p>\n<p>We can compute the optimal policy by</p>\n<p>$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,</p>\n<p>Which means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. </p>\n<p>If an optimal policy exists then its value function must be a fixed point of the operator $B^*$. </p>\n<h4 id=\"Bellman-optimality-backup-operator\"><a href=\"#Bellman-optimality-backup-operator\" class=\"headerlink\" title=\"Bellman optimality backup operator\"></a>Bellman optimality backup operator</h4><p>Bellman optimality backup operator is written as $B^*$ with a value function behind it </p>\n<p>$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V(s’)$. </p>\n<p>If $\\gamma&lt;1$, $B^*$ is a strict contraction and has a unique fixed point. This means </p>\n<p>$B^*V(s)\\geq V^\\pi(s)$.</p>\n<p>Bellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.</p>\n<p>Next I’ll briefly introduce some algorithms to compute the optimal value function and an optimal policy.</p>\n<h4 id=\"Policy-search\"><a href=\"#Policy-search\" class=\"headerlink\" title=\"Policy search\"></a>Policy search</h4><p>This algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a $V^<em>(s)$  and  $\\pi^</em>(s)$. </p>\n<h4 id=\"Policy-iteration\"><a href=\"#Policy-iteration\" class=\"headerlink\" title=\"Policy iteration\"></a>Policy iteration</h4><p>The algorithm of policy iteration is shown below: </p>\n<p><code>while</code> True <code>do</code></p>\n<p>​    $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)</p>\n<p>​    $\\pi^*$ = Policy improvement $(M,V^\\pi)$</p>\n<p><code>if</code> $\\pi^*(s)=\\pi(s)$ <code>then</code></p>\n<p>​    <code>break</code></p>\n<p><code>else</code></p>\n<p>​    $\\pi$ = $\\pi^*$</p>\n<p>$V^*$ = $V^\\pi$ . </p>\n<p>Policy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute</p>\n<p>$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)V^{\\pi i}(s’)$ </p>\n<p>for all the $a$ and $s$ and then take the max</p>\n<p><code>return</code> $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.</p>\n<p>Notice that there is a relationship</p>\n<p>$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.</p>\n<p>This means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.</p>\n<h4 id=\"Value-iteration\"><a href=\"#Value-iteration\" class=\"headerlink\" title=\"Value iteration\"></a>Value iteration</h4><p>The algorithm of value iteration is shown below:</p>\n<p>$V’(s)=0, V(s)=\\infty$, for all $s\\in S$</p>\n<p><code>while</code> $||V-V’||_\\infty&gt;\\epsilon$ <code>do</code></p>\n<p>​    $V=V’$</p>\n<p>​    $V’(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V’(s)$, for all states $s\\in S$ </p>\n<p>$V^*=V$, for all $s\\in S$ </p>\n<p>$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V^ *(s’),\\ \\forall s\\in S$ . </p>\n<p>The idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.</p>\n","site":{"data":{}},"more":"<h3 id=\"Markov-process-MP\"><a href=\"#Markov-process-MP\" class=\"headerlink\" title=\"Markov process (MP)\"></a>Markov process (MP)</h3><p>Markov process is a stochastic process that satisfies the Markov property, which means it is “memoryless” and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. </p>\n<p>We need to make two assumptions before we define the Markov process. The first assumption is that <em>the state of MP is finite</em>, and we have $s_i\\in S, i\\in1,2,…$ , where $|S|&lt;\\infty$. The second assumption is that <em>the transition probabilities are time independent</em>. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,…$.</p>\n<p>Base on these two assumption, we can define a <em>transition transform matrix</em>:</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F0.png\" alt=\"\"></p>\n<p>The size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.</p>\n<p>Henceforth, we can define a Markov process using a tuple $(S,\\bf P)$.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n</ul>\n<p>By calculating $S\\bf P$ we can get the distribution of the new state.</p>\n<p>Figure 1 shows a student MP example.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png\" alt=\"Figure 1\"></p>\n<h3 id=\"Markov-reward-process-MRP\"><a href=\"#Markov-reward-process-MRP\" class=\"headerlink\" title=\"Markov reward process (MRP)\"></a>Markov reward process (MRP)</h3><p>MRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explaintions.</p>\n<h4 id=\"Reward-function\"><a href=\"#Reward-function\" class=\"headerlink\" title=\"Reward function\"></a>Reward function</h4><p>When we are moving from the current state $s$ to a <em>successor state</em> $s’$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s’$ ). For a state $s\\in S$, we define the expected reward by</p>\n<p>$R(s)=\\Bbb E[r_t|s_t=s]$. </p>\n<p>Here we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.</p>\n<h4 id=\"Horizon\"><a href=\"#Horizon\" class=\"headerlink\" title=\"Horizon\"></a>Horizon</h4><p>It is defined as the number of time steps in each episode of the process. An <em>episode</em> is the whole process of a round of training. The horizon can be finite or infinite.</p>\n<h4 id=\"Return\"><a href=\"#Return\" class=\"headerlink\" title=\"Return\"></a>Return</h4><p>The return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon <em>H</em>. We can calculate the return using</p>\n<p>$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.</p>\n<h4 id=\"State-value-function\"><a href=\"#State-value-function\" class=\"headerlink\" title=\"State value function\"></a>State value function</h4><p>The state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression</p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>If the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.</p>\n<h4 id=\"Discount-factor\"><a href=\"#Discount-factor\" class=\"headerlink\" title=\"Discount factor\"></a>Discount factor</h4><p>We design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma &lt;1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.</p>\n<p>Figure 2 and 3 shows an example of how to calculate the return.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F2.png\" alt=\"Figure 2\"></p>\n<p><img src=\"https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg\" alt=\"Figure 3\"></p>\n<p>It is significant to find out a value function while many problems of RL is how to get a value function essentially.</p>\n<h4 id=\"Computing-the-value-function\"><a href=\"#Computing-the-value-function\" class=\"headerlink\" title=\"Computing the value function\"></a>Computing the value function</h4><p>We have three ways to compute the value function.</p>\n<ul>\n<li><p>Simulation. Through simulation, we can get the value function by averaing many returns of episodes.</p>\n</li>\n<li><p>Analytic solution. We have defined the state value function </p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>Then, make a little transformation, see Figure 4 in detail. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F4.png\" alt=\"Figure 4\"></p>\n<p>Then, we have</p>\n<p>$V(s)=R(s)+\\gamma \\sum P(s’|s)V(s’)$, </p>\n</li>\n</ul>\n<p>  $V=R+\\gamma\\bf P\\mit V$. </p>\n<p>  Therefore we have</p>\n<p>  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. </p>\n<p>  If $0&lt;\\gamma&lt;1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.</p>\n<p>  Notice that $s’$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.</p>\n<p>  <img src=\"https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png\" alt=\"Figure 5\"></p>\n<ul>\n<li><p>Iterative solution. </p>\n<p>$V_t(s)=R(s)+\\gamma \\sum P(s’|s)V_{t+1}(s’), \\forall t=0,…,H-1,V_H(s)=0$. </p>\n<p>We can iterate it again and again and use $|V_t-V_{t-1}|&lt;\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. </p>\n</li>\n</ul>\n<h3 id=\"Markov-decision-process-MDP\"><a href=\"#Markov-decision-process-MDP\" class=\"headerlink\" title=\"Markov decision process (MDP)\"></a>Markov decision process (MDP)</h3><p>MDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. </p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$A$: A finite set of actions which are available from each state $s$.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explanations.</p>\n<h4 id=\"Notifications\"><a href=\"#Notifications\" class=\"headerlink\" title=\"Notifications\"></a>Notifications</h4><ul>\n<li><p>Both $S$ and $A$ are finite.</p>\n</li>\n<li><p>In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as</p>\n<p>$P(s_{t+1}|s_t,a_t)$.</p>\n</li>\n<li><p>In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as</p>\n<p>$R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n</li>\n<li><p>Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.</p>\n</li>\n</ul>\n<h4 id=\"Policy\"><a href=\"#Policy\" class=\"headerlink\" title=\"Policy\"></a>Policy</h4><p>Before we mention the state value function, we need to talk about the policy for the MDP first. </p>\n<p>A policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be <em>varying with time</em>, especially when the horizon is finite. A policy can be written as</p>\n<p>$\\bf \\pi\\mit(a|s)=P(a_t=a|s_t=s)$. </p>\n<p>If given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: </p>\n<ul>\n<li><p>$P^\\pi(s’|s)=\\sum_{a\\in A}\\pi(a|s) P(s’|s,a)$</p>\n<p>When we have a policy $\\pi$, the probability of the state transforms from $s$ to $s’$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s’$ when executing an action $a$.</p>\n</li>\n<li><p>$R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$</p>\n<p>When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.</p>\n</li>\n</ul>\n<h4 id=\"Value-functions-in-MDP-Bellman-expectation-equations\"><a href=\"#Value-functions-in-MDP-Bellman-expectation-equations\" class=\"headerlink\" title=\"Value functions in MDP (Bellman expectation equations)\"></a>Value functions in MDP (Bellman expectation equations)</h4><p>Given a policy $\\pi$ can define two quantities: <em>the state value function</em> and <em>the state-action value function</em>. These two value functions are both <em>Bellman expectation equations</em>.</p>\n<ul>\n<li><p>State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression</p>\n<p>$V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. </p>\n<p>Frequently we will drop the subscript $\\pi$ in the expectation. </p>\n</li>\n<li><p>State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form</p>\n<p>$Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. </p>\n<p>It evaluates the value of acting the action $a$ under current state $s$. </p>\n</li>\n</ul>\n<p>Now let’s talk about the relationships between these two value functions.</p>\n<p>Figure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png\" alt=\"Figure 6\"></p>\n<p>We can discover that the value of a state can be denoted as</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.</p>\n<p>In a similar way, Figure 7 shows what states that an action can lead to.</p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png\" alt=\"Figure 7\"></p>\n<p>We can also find that </p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)V^\\pi(s’)$. </p>\n<p>On the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s’$ and the probability of getting into that new state. </p>\n<p>If we combine the two Bellman equation with each other, we can get</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V^\\pi(s’)]$</p>\n<p>​            $=R(s’,\\pi(s’))+\\gamma\\sum_{s’\\in S}P(s’|s,\\pi(s)) V^\\pi(s’)$, </p>\n<p>and</p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)\\sum_{a\\in A}\\pi(a’|s’)Q_\\pi(s’,a’)$. </p>\n<p>The example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png\" alt=\"Figure 8\"></p>\n<h4 id=\"Optimality-value-function-Bellman-optimality-equation\"><a href=\"#Optimality-value-function-Bellman-optimality-equation\" class=\"headerlink\" title=\"Optimality value function (Bellman optimality equation)\"></a>Optimality value function (Bellman optimality equation)</h4><ul>\n<li>Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. </li>\n<li>Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.</li>\n</ul>\n<p>Optimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. </p>\n<h3 id=\"Find-the-best-policy\"><a href=\"#Find-the-best-policy\" class=\"headerlink\" title=\"Find the best policy\"></a>Find the best policy</h3><p>The best policy is defined precisely as <em>optimal policy</em>  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.</p>\n<p>For an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.</p>\n<p>We can compute the optimal policy by</p>\n<p>$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,</p>\n<p>Which means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. </p>\n<p>If an optimal policy exists then its value function must be a fixed point of the operator $B^*$. </p>\n<h4 id=\"Bellman-optimality-backup-operator\"><a href=\"#Bellman-optimality-backup-operator\" class=\"headerlink\" title=\"Bellman optimality backup operator\"></a>Bellman optimality backup operator</h4><p>Bellman optimality backup operator is written as $B^*$ with a value function behind it </p>\n<p>$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V(s’)$. </p>\n<p>If $\\gamma&lt;1$, $B^*$ is a strict contraction and has a unique fixed point. This means </p>\n<p>$B^*V(s)\\geq V^\\pi(s)$.</p>\n<p>Bellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.</p>\n<p>Next I’ll briefly introduce some algorithms to compute the optimal value function and an optimal policy.</p>\n<h4 id=\"Policy-search\"><a href=\"#Policy-search\" class=\"headerlink\" title=\"Policy search\"></a>Policy search</h4><p>This algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a $V^<em>(s)$  and  $\\pi^</em>(s)$. </p>\n<h4 id=\"Policy-iteration\"><a href=\"#Policy-iteration\" class=\"headerlink\" title=\"Policy iteration\"></a>Policy iteration</h4><p>The algorithm of policy iteration is shown below: </p>\n<p><code>while</code> True <code>do</code></p>\n<p>​    $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)</p>\n<p>​    $\\pi^*$ = Policy improvement $(M,V^\\pi)$</p>\n<p><code>if</code> $\\pi^*(s)=\\pi(s)$ <code>then</code></p>\n<p>​    <code>break</code></p>\n<p><code>else</code></p>\n<p>​    $\\pi$ = $\\pi^*$</p>\n<p>$V^*$ = $V^\\pi$ . </p>\n<p>Policy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute</p>\n<p>$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)V^{\\pi i}(s’)$ </p>\n<p>for all the $a$ and $s$ and then take the max</p>\n<p><code>return</code> $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.</p>\n<p>Notice that there is a relationship</p>\n<p>$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.</p>\n<p>This means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.</p>\n<h4 id=\"Value-iteration\"><a href=\"#Value-iteration\" class=\"headerlink\" title=\"Value iteration\"></a>Value iteration</h4><p>The algorithm of value iteration is shown below:</p>\n<p>$V’(s)=0, V(s)=\\infty$, for all $s\\in S$</p>\n<p><code>while</code> $||V-V’||_\\infty&gt;\\epsilon$ <code>do</code></p>\n<p>​    $V=V’$</p>\n<p>​    $V’(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V’(s)$, for all states $s\\in S$ </p>\n<p>$V^*=V$, for all $s\\in S$ </p>\n<p>$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V^ *(s’),\\ \\forall s\\in S$ . </p>\n<p>The idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.</p>\n"},{"title":"Python学习笔记","date":"2020-01-06T09:00:00.000Z","thumbnail":"https://netmaxtech.com/wp-content/uploads/2017/05/Python-Logo-PNG-Image.png","excerpt":"记录本人在学习Python时遇到的坑以及这门语言的特性。","_content":"\n> 这篇文章主要记录本人在学习Python时遇到的坑以及这个语言的一些特性，内容以时间顺序整理，比较零散杂乱。对于从零开始的同学，请参考官方文档[Python 3.8.1 中文文档](https://docs.python.org/zh-cn/3/)或其他网络上的教程。本文章将持续更新。\n\n### 19/9/14\n\n- 注释方法：`#（一行注释）`，`“”“ ”“”（多行注释）`\n- for循环：`for （变量） in （范围）`，范围可以用`range`函数\n- `Input`函数的输入是`char`类型的\n- `// `是整除运算\n- 逗号不可以用来分隔语句\n- 使用缩进（4个空格）来代替C/C++中的大括号\n\n### 19/9/15\n\n- `for...in`循环中，`_ `可以作为循环变量，这时候仅循环指定次数，而不需要关心循环变量的值；事实上，`_ `是一个合法的标识符，如果不关心这个变量，就可以将其定义成这个值，它是一个垃圾桶\n- 定义函数时，使用`函数名(*参数名)`的定义方式， `*` 代表函数的参数是可变参数，可以有0到多个参数\n- 一个文件代表一个模块(module)，若在不同的模块中含有同名函数，那么可以通过`import`导入指定的模块中的函数，如`from 模块 import 函数`，或者`import 模块 as 自定义模块名称`，再通过`自定义模块名称.函数`的方式调用\n- `__name__`是Python中一个隐含的变量，代表了模块的名字，只用直接执行的模块的名字才是`__main__`\n- 可以使用`global`指定使用的是一个全局变量，如果全局变量中没有找到对应的，那么会定义一个新的全局变量\n- 嵌套作用域：对于函数a内部的函数b而言，a中定义的变量对b来说是在嵌套作用域中的，若要指定修改嵌套作用域中的变量，可以使用`nonlocal`指示变量来自嵌套作用域\n- `pass`是一个空语句，只起到占位作用\n- 可以定义一个`main`函数（或者与模块名字相同的函数），再按照`if __name__ = '__main__'`的格式使脚本执行\n\n### 19/9/17\n\n- 与字符串有关的函数的调用方式为：`字符串名称.字符串操作函数()`，在此时字符串是一个对象，字符串操作函数的作用是向字符串对象发送一个消息\n- 字符串实质上是一个数组，可以进行下标运算\n- 字符串切片可以在下标运算中使用冒号进行运算，`[起始字符:结束字符:间隔]`，若不定义起始与终止字符，则默认为整个字符串，当间隔为负值时，以为着切片操作反向\n- 字符串的索引为负值时，意味着索引从右到左数\n- 列表可以理解为一个数组，其操作与字符串类似\n- 可使用`sorted`函数对列表进行排序\n- 可以使用生成式语法创建列表：`f = [x for x in range(1, 10)]`（此方法在创建列表后元素已经准备就绪，耗费较多内存），或`f = (x for x in range(1, 10))`（此方法创建的是一个生成器对象，需要数据时列表通过生成器产生，节省内存但是耗费较多时间）\n- 可以使用`yield`关键字来实现迭代，使用`yield`就是产生了一个生成器，每次遇到` yield `时函数会暂停并保存当前所有的运行信息，返回` yield `的值，并在下一次执行此方法是从当前位置开始运行\n- 可以定义元组，其相当于不能修改的数组，一个元组中的元素数据类型可以不同，定义元组使用`t = ()`\n- 列表和元组可以互相转换\n- 可以定义集合，定义集合可以使用`set = {}`，元组可以转换为集合\n- 字典类似于数组，但是它是由多组键值对组成的\n\n### 19/9/19\n\n- 使用class关键字定义类，再在类中定义函数，如：`class 类名(object)`\n- `__init__`函数是用于在创建对象时进行的初始化操作\n- self是类的本身，是它的实例变量，在类中所有函数的第一个参数就是self，在类中修改属性值需使用`self.属性值 = x`的语法\n- 实例化类的方法：`对象名 = 类名(初始化函数参数)`\n- 对象中方法的引用可以采用`对象.方法（也即函数）`的语句，通过此方式向对象发送消息\n- Python中，属性和方法的访问权限只有`public`和`private`，若希望属性或方法是私有的，在给它们命名的时候要使用`__`开头，但是不建议将属性设置为私有的\n- 使用`_`开头暗示属性或方法是受保护(protected)的，访问它们建议通过特定的方法，但实际上它们还是可以直接被外部访问\n- 可以通过在类中定义方法以访问对象受保护的属性，在定义这些方法（函数）时，要在上一行使用`@property`包装这些方法\n- 对于被保护的属性，在访问它们时采用`getter`方法，需添加`@property`，在修改它们时采用`setter`方法，需添加`@函数（即方法）名.setter`\n- Python可以对对象动态绑定新的属性或方法\n- 可以使用`__slots__`限定对象只能绑定某些属性，但是它只对当前类的对象生效，对子类不起作用\n- 可以通过给类发送消息，在类的对象被创建出来之前直接使用其中的方法，此种方法被称为静态方法，需要在定义时添加`@staticmethod`，此类方法的参数不含有`self`\n- 通过类方法可以获取类相关的信息并且可以__创建出类的对象__，需要在定义时添加`@classmethod`，类方法的第一个参数是`cls`，这个`cls`相当于就是在外部实例化类时定义的对象名，只不过它是放在类的内部使用了，其功能就是可以像在外部调用对象的属性和方法一样在类的内部使用对象（类）的属性和方法\n\n### 19/9/20\n\n- 类之间的关系：\n  - is-a：继承或者泛化，如：__student__ is a __human being__，__cell phone__ is a __electronic device__\n  - has-a：关联，如 __department__ has an __employee__\n  - use-a：依赖，如 __driver__ use a __car__ \n- 类与类之间可以继承，提供继承信息的成为父类（超类或者基类），得到继承的称为子类（派生类或者衍生类）\n- Python中继承的写法：`class 子类名(基类名)`\n- 在编程中一般使用子类去替代基类\n- 在子类中，通过重新定义父类中的方法，可以让同一种方法在不同的子类中有不同的行为，这称为重写\n\n### 20/1/11\n\n- Python中提供两个重要的功能：异常处理和断言（Assertions）来处理运行中出现的异常和错误，他们的功能是用于调试Python程序\n- 异常：无法正常处理程序时会发生异常，是一个对象，如果不捕获异常，程序会终止执行\n- Python中异常处理的写法：\n\n```python\ntry: \n\t#operation1\nexcept exception_type, argument:\n\t#if error occurs in operation1, execute operation2\n  #operation2\nelse: \n\t#if no error occurs in operation1, execute operation3\n  #operation3\n```\n\n- 使用`except`可以不带异常类型，但是会让`try-except`语句捕获所有的异常，不建议这样写\n- 可以使用`expect(exception1[, expection2[, expection3]])`来添加多个异常类型\n- `argument`为异常的参数，可以用于输出异常信息的异常值\n- 也可以使用如下方法，但是与`try-except`有所不同：\n\n```python\ntry:\n\t#operation1\nfinally:\n\t#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished\n```\n\n- `finally`和`except`不可以同时使用\n\n- 可以使用`raise`触发异常\n- `append()`方法用于在列表末尾添加新的对象，对于一个数组`list`，可以这样使用：`list.append()`\n- 多线程用于同时执行多个不同的程序，可以把占据长时间的程序中的任务放到后台处理\n- 线程与进程：独立的线程有自己的程序入口、执行序列、程序出口，但是线程不可以独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制\n- 在Python中使用线程：`thread.start_new_thread(function, args[, kwargs])`，其中`function`为线程函数，这个函数需要提前定义好，`args`为传递给线程函数的参数，是一个元组，`kwargs`为可选参数，此种方式称为函数式，线程的结束一般靠函数的自然结束\n- 此外还可以使用Python所提供的`threading`模块，直接从`threading.Thread`继承：`class myThread(threading.Thread)`，然后重写`__init__`和`run`方法，把需要执行的代码写到`run`方法里面，`__init__`的重写方法如下：\n\n```python\ndef __init__(self, threadID, name, counter):\n\tthreading.Thread.__init__(self)\n\tself.threadID = threadID\n\tself.name = name\n\tself.counter = counter\n```\n\n- 上述`thread`类提供了以下方法：\n  - `run()`：表示线程活动的方法\n  - `start`：启动线程\n  - `join()`：等待直到线程终止\n  - `isAlive()`：查询线程是否活动\n  - `getName()`：返回线程名\n  - `setName()`：设置线程名\n- 为了避免两个或多个线程同时运行，产生冲突，可以使用线程锁来控制线程执行的优先顺序，被锁定的线程优先执行，其他进程必须停止\n- 可以使用`threading.Lock().acquire()`和`threading.Lock().release()`来锁定和释放线程\n- 可以建立一个空数组用于存放线程，再通过`append`方法将线程添加至该数组中，通过遍历数组可以对其中的线程做一样的操作\n\n### 20/1/13\n\n- 在继承的时候，","source":"_posts/Python学习笔记.md","raw":"---\ntitle: Python学习笔记\ndate: 2020-1-6 17:00:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- Python\n\t- Programming Language\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://netmaxtech.com/wp-content/uploads/2017/05/Python-Logo-PNG-Image.png\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: 记录本人在学习Python时遇到的坑以及这门语言的特性。\n\n#You can begin to input your article below now.\n\n---\n\n> 这篇文章主要记录本人在学习Python时遇到的坑以及这个语言的一些特性，内容以时间顺序整理，比较零散杂乱。对于从零开始的同学，请参考官方文档[Python 3.8.1 中文文档](https://docs.python.org/zh-cn/3/)或其他网络上的教程。本文章将持续更新。\n\n### 19/9/14\n\n- 注释方法：`#（一行注释）`，`“”“ ”“”（多行注释）`\n- for循环：`for （变量） in （范围）`，范围可以用`range`函数\n- `Input`函数的输入是`char`类型的\n- `// `是整除运算\n- 逗号不可以用来分隔语句\n- 使用缩进（4个空格）来代替C/C++中的大括号\n\n### 19/9/15\n\n- `for...in`循环中，`_ `可以作为循环变量，这时候仅循环指定次数，而不需要关心循环变量的值；事实上，`_ `是一个合法的标识符，如果不关心这个变量，就可以将其定义成这个值，它是一个垃圾桶\n- 定义函数时，使用`函数名(*参数名)`的定义方式， `*` 代表函数的参数是可变参数，可以有0到多个参数\n- 一个文件代表一个模块(module)，若在不同的模块中含有同名函数，那么可以通过`import`导入指定的模块中的函数，如`from 模块 import 函数`，或者`import 模块 as 自定义模块名称`，再通过`自定义模块名称.函数`的方式调用\n- `__name__`是Python中一个隐含的变量，代表了模块的名字，只用直接执行的模块的名字才是`__main__`\n- 可以使用`global`指定使用的是一个全局变量，如果全局变量中没有找到对应的，那么会定义一个新的全局变量\n- 嵌套作用域：对于函数a内部的函数b而言，a中定义的变量对b来说是在嵌套作用域中的，若要指定修改嵌套作用域中的变量，可以使用`nonlocal`指示变量来自嵌套作用域\n- `pass`是一个空语句，只起到占位作用\n- 可以定义一个`main`函数（或者与模块名字相同的函数），再按照`if __name__ = '__main__'`的格式使脚本执行\n\n### 19/9/17\n\n- 与字符串有关的函数的调用方式为：`字符串名称.字符串操作函数()`，在此时字符串是一个对象，字符串操作函数的作用是向字符串对象发送一个消息\n- 字符串实质上是一个数组，可以进行下标运算\n- 字符串切片可以在下标运算中使用冒号进行运算，`[起始字符:结束字符:间隔]`，若不定义起始与终止字符，则默认为整个字符串，当间隔为负值时，以为着切片操作反向\n- 字符串的索引为负值时，意味着索引从右到左数\n- 列表可以理解为一个数组，其操作与字符串类似\n- 可使用`sorted`函数对列表进行排序\n- 可以使用生成式语法创建列表：`f = [x for x in range(1, 10)]`（此方法在创建列表后元素已经准备就绪，耗费较多内存），或`f = (x for x in range(1, 10))`（此方法创建的是一个生成器对象，需要数据时列表通过生成器产生，节省内存但是耗费较多时间）\n- 可以使用`yield`关键字来实现迭代，使用`yield`就是产生了一个生成器，每次遇到` yield `时函数会暂停并保存当前所有的运行信息，返回` yield `的值，并在下一次执行此方法是从当前位置开始运行\n- 可以定义元组，其相当于不能修改的数组，一个元组中的元素数据类型可以不同，定义元组使用`t = ()`\n- 列表和元组可以互相转换\n- 可以定义集合，定义集合可以使用`set = {}`，元组可以转换为集合\n- 字典类似于数组，但是它是由多组键值对组成的\n\n### 19/9/19\n\n- 使用class关键字定义类，再在类中定义函数，如：`class 类名(object)`\n- `__init__`函数是用于在创建对象时进行的初始化操作\n- self是类的本身，是它的实例变量，在类中所有函数的第一个参数就是self，在类中修改属性值需使用`self.属性值 = x`的语法\n- 实例化类的方法：`对象名 = 类名(初始化函数参数)`\n- 对象中方法的引用可以采用`对象.方法（也即函数）`的语句，通过此方式向对象发送消息\n- Python中，属性和方法的访问权限只有`public`和`private`，若希望属性或方法是私有的，在给它们命名的时候要使用`__`开头，但是不建议将属性设置为私有的\n- 使用`_`开头暗示属性或方法是受保护(protected)的，访问它们建议通过特定的方法，但实际上它们还是可以直接被外部访问\n- 可以通过在类中定义方法以访问对象受保护的属性，在定义这些方法（函数）时，要在上一行使用`@property`包装这些方法\n- 对于被保护的属性，在访问它们时采用`getter`方法，需添加`@property`，在修改它们时采用`setter`方法，需添加`@函数（即方法）名.setter`\n- Python可以对对象动态绑定新的属性或方法\n- 可以使用`__slots__`限定对象只能绑定某些属性，但是它只对当前类的对象生效，对子类不起作用\n- 可以通过给类发送消息，在类的对象被创建出来之前直接使用其中的方法，此种方法被称为静态方法，需要在定义时添加`@staticmethod`，此类方法的参数不含有`self`\n- 通过类方法可以获取类相关的信息并且可以__创建出类的对象__，需要在定义时添加`@classmethod`，类方法的第一个参数是`cls`，这个`cls`相当于就是在外部实例化类时定义的对象名，只不过它是放在类的内部使用了，其功能就是可以像在外部调用对象的属性和方法一样在类的内部使用对象（类）的属性和方法\n\n### 19/9/20\n\n- 类之间的关系：\n  - is-a：继承或者泛化，如：__student__ is a __human being__，__cell phone__ is a __electronic device__\n  - has-a：关联，如 __department__ has an __employee__\n  - use-a：依赖，如 __driver__ use a __car__ \n- 类与类之间可以继承，提供继承信息的成为父类（超类或者基类），得到继承的称为子类（派生类或者衍生类）\n- Python中继承的写法：`class 子类名(基类名)`\n- 在编程中一般使用子类去替代基类\n- 在子类中，通过重新定义父类中的方法，可以让同一种方法在不同的子类中有不同的行为，这称为重写\n\n### 20/1/11\n\n- Python中提供两个重要的功能：异常处理和断言（Assertions）来处理运行中出现的异常和错误，他们的功能是用于调试Python程序\n- 异常：无法正常处理程序时会发生异常，是一个对象，如果不捕获异常，程序会终止执行\n- Python中异常处理的写法：\n\n```python\ntry: \n\t#operation1\nexcept exception_type, argument:\n\t#if error occurs in operation1, execute operation2\n  #operation2\nelse: \n\t#if no error occurs in operation1, execute operation3\n  #operation3\n```\n\n- 使用`except`可以不带异常类型，但是会让`try-except`语句捕获所有的异常，不建议这样写\n- 可以使用`expect(exception1[, expection2[, expection3]])`来添加多个异常类型\n- `argument`为异常的参数，可以用于输出异常信息的异常值\n- 也可以使用如下方法，但是与`try-except`有所不同：\n\n```python\ntry:\n\t#operation1\nfinally:\n\t#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished\n```\n\n- `finally`和`except`不可以同时使用\n\n- 可以使用`raise`触发异常\n- `append()`方法用于在列表末尾添加新的对象，对于一个数组`list`，可以这样使用：`list.append()`\n- 多线程用于同时执行多个不同的程序，可以把占据长时间的程序中的任务放到后台处理\n- 线程与进程：独立的线程有自己的程序入口、执行序列、程序出口，但是线程不可以独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制\n- 在Python中使用线程：`thread.start_new_thread(function, args[, kwargs])`，其中`function`为线程函数，这个函数需要提前定义好，`args`为传递给线程函数的参数，是一个元组，`kwargs`为可选参数，此种方式称为函数式，线程的结束一般靠函数的自然结束\n- 此外还可以使用Python所提供的`threading`模块，直接从`threading.Thread`继承：`class myThread(threading.Thread)`，然后重写`__init__`和`run`方法，把需要执行的代码写到`run`方法里面，`__init__`的重写方法如下：\n\n```python\ndef __init__(self, threadID, name, counter):\n\tthreading.Thread.__init__(self)\n\tself.threadID = threadID\n\tself.name = name\n\tself.counter = counter\n```\n\n- 上述`thread`类提供了以下方法：\n  - `run()`：表示线程活动的方法\n  - `start`：启动线程\n  - `join()`：等待直到线程终止\n  - `isAlive()`：查询线程是否活动\n  - `getName()`：返回线程名\n  - `setName()`：设置线程名\n- 为了避免两个或多个线程同时运行，产生冲突，可以使用线程锁来控制线程执行的优先顺序，被锁定的线程优先执行，其他进程必须停止\n- 可以使用`threading.Lock().acquire()`和`threading.Lock().release()`来锁定和释放线程\n- 可以建立一个空数组用于存放线程，再通过`append`方法将线程添加至该数组中，通过遍历数组可以对其中的线程做一样的操作\n\n### 20/1/13\n\n- 在继承的时候，","slug":"Python学习笔记","published":1,"updated":"2020-01-13T04:18:25.089Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck6ax1lgj0009j1p22bs352jd","content":"<blockquote>\n<p>这篇文章主要记录本人在学习Python时遇到的坑以及这个语言的一些特性，内容以时间顺序整理，比较零散杂乱。对于从零开始的同学，请参考官方文档<a href=\"https://docs.python.org/zh-cn/3/\">Python 3.8.1 中文文档</a>或其他网络上的教程。本文章将持续更新。</p>\n</blockquote>\n<h3 id=\"19-9-14\"><a href=\"#19-9-14\" class=\"headerlink\" title=\"19/9/14\"></a>19/9/14</h3><ul>\n<li>注释方法：<code>#（一行注释）</code>，<code>“”“ ”“”（多行注释）</code></li>\n<li>for循环：<code>for （变量） in （范围）</code>，范围可以用<code>range</code>函数</li>\n<li><code>Input</code>函数的输入是<code>char</code>类型的</li>\n<li><code>//</code>是整除运算</li>\n<li>逗号不可以用来分隔语句</li>\n<li>使用缩进（4个空格）来代替C/C++中的大括号</li>\n</ul>\n<h3 id=\"19-9-15\"><a href=\"#19-9-15\" class=\"headerlink\" title=\"19/9/15\"></a>19/9/15</h3><ul>\n<li><code>for...in</code>循环中，<code>_</code>可以作为循环变量，这时候仅循环指定次数，而不需要关心循环变量的值；事实上，<code>_</code>是一个合法的标识符，如果不关心这个变量，就可以将其定义成这个值，它是一个垃圾桶</li>\n<li>定义函数时，使用<code>函数名(*参数名)</code>的定义方式， <code>*</code> 代表函数的参数是可变参数，可以有0到多个参数</li>\n<li>一个文件代表一个模块(module)，若在不同的模块中含有同名函数，那么可以通过<code>import</code>导入指定的模块中的函数，如<code>from 模块 import 函数</code>，或者<code>import 模块 as 自定义模块名称</code>，再通过<code>自定义模块名称.函数</code>的方式调用</li>\n<li><code>__name__</code>是Python中一个隐含的变量，代表了模块的名字，只用直接执行的模块的名字才是<code>__main__</code></li>\n<li>可以使用<code>global</code>指定使用的是一个全局变量，如果全局变量中没有找到对应的，那么会定义一个新的全局变量</li>\n<li>嵌套作用域：对于函数a内部的函数b而言，a中定义的变量对b来说是在嵌套作用域中的，若要指定修改嵌套作用域中的变量，可以使用<code>nonlocal</code>指示变量来自嵌套作用域</li>\n<li><code>pass</code>是一个空语句，只起到占位作用</li>\n<li>可以定义一个<code>main</code>函数（或者与模块名字相同的函数），再按照<code>if __name__ = &#39;__main__&#39;</code>的格式使脚本执行</li>\n</ul>\n<h3 id=\"19-9-17\"><a href=\"#19-9-17\" class=\"headerlink\" title=\"19/9/17\"></a>19/9/17</h3><ul>\n<li>与字符串有关的函数的调用方式为：<code>字符串名称.字符串操作函数()</code>，在此时字符串是一个对象，字符串操作函数的作用是向字符串对象发送一个消息</li>\n<li>字符串实质上是一个数组，可以进行下标运算</li>\n<li>字符串切片可以在下标运算中使用冒号进行运算，<code>[起始字符:结束字符:间隔]</code>，若不定义起始与终止字符，则默认为整个字符串，当间隔为负值时，以为着切片操作反向</li>\n<li>字符串的索引为负值时，意味着索引从右到左数</li>\n<li>列表可以理解为一个数组，其操作与字符串类似</li>\n<li>可使用<code>sorted</code>函数对列表进行排序</li>\n<li>可以使用生成式语法创建列表：<code>f = [x for x in range(1, 10)]</code>（此方法在创建列表后元素已经准备就绪，耗费较多内存），或<code>f = (x for x in range(1, 10))</code>（此方法创建的是一个生成器对象，需要数据时列表通过生成器产生，节省内存但是耗费较多时间）</li>\n<li>可以使用<code>yield</code>关键字来实现迭代，使用<code>yield</code>就是产生了一个生成器，每次遇到<code>yield</code>时函数会暂停并保存当前所有的运行信息，返回<code>yield</code>的值，并在下一次执行此方法是从当前位置开始运行</li>\n<li>可以定义元组，其相当于不能修改的数组，一个元组中的元素数据类型可以不同，定义元组使用<code>t = ()</code></li>\n<li>列表和元组可以互相转换</li>\n<li>可以定义集合，定义集合可以使用<code>set = {}</code>，元组可以转换为集合</li>\n<li>字典类似于数组，但是它是由多组键值对组成的</li>\n</ul>\n<h3 id=\"19-9-19\"><a href=\"#19-9-19\" class=\"headerlink\" title=\"19/9/19\"></a>19/9/19</h3><ul>\n<li>使用class关键字定义类，再在类中定义函数，如：<code>class 类名(object)</code></li>\n<li><code>__init__</code>函数是用于在创建对象时进行的初始化操作</li>\n<li>self是类的本身，是它的实例变量，在类中所有函数的第一个参数就是self，在类中修改属性值需使用<code>self.属性值 = x</code>的语法</li>\n<li>实例化类的方法：<code>对象名 = 类名(初始化函数参数)</code></li>\n<li>对象中方法的引用可以采用<code>对象.方法（也即函数）</code>的语句，通过此方式向对象发送消息</li>\n<li>Python中，属性和方法的访问权限只有<code>public</code>和<code>private</code>，若希望属性或方法是私有的，在给它们命名的时候要使用<code>__</code>开头，但是不建议将属性设置为私有的</li>\n<li>使用<code>_</code>开头暗示属性或方法是受保护(protected)的，访问它们建议通过特定的方法，但实际上它们还是可以直接被外部访问</li>\n<li>可以通过在类中定义方法以访问对象受保护的属性，在定义这些方法（函数）时，要在上一行使用<code>@property</code>包装这些方法</li>\n<li>对于被保护的属性，在访问它们时采用<code>getter</code>方法，需添加<code>@property</code>，在修改它们时采用<code>setter</code>方法，需添加<code>@函数（即方法）名.setter</code></li>\n<li>Python可以对对象动态绑定新的属性或方法</li>\n<li>可以使用<code>__slots__</code>限定对象只能绑定某些属性，但是它只对当前类的对象生效，对子类不起作用</li>\n<li>可以通过给类发送消息，在类的对象被创建出来之前直接使用其中的方法，此种方法被称为静态方法，需要在定义时添加<code>@staticmethod</code>，此类方法的参数不含有<code>self</code></li>\n<li>通过类方法可以获取类相关的信息并且可以<strong>创建出类的对象</strong>，需要在定义时添加<code>@classmethod</code>，类方法的第一个参数是<code>cls</code>，这个<code>cls</code>相当于就是在外部实例化类时定义的对象名，只不过它是放在类的内部使用了，其功能就是可以像在外部调用对象的属性和方法一样在类的内部使用对象（类）的属性和方法</li>\n</ul>\n<h3 id=\"19-9-20\"><a href=\"#19-9-20\" class=\"headerlink\" title=\"19/9/20\"></a>19/9/20</h3><ul>\n<li>类之间的关系：<ul>\n<li>is-a：继承或者泛化，如：<strong>student</strong> is a <strong>human being</strong>，<strong>cell phone</strong> is a <strong>electronic device</strong></li>\n<li>has-a：关联，如 <strong>department</strong> has an <strong>employee</strong></li>\n<li>use-a：依赖，如 <strong>driver</strong> use a <strong>car</strong> </li>\n</ul>\n</li>\n<li>类与类之间可以继承，提供继承信息的成为父类（超类或者基类），得到继承的称为子类（派生类或者衍生类）</li>\n<li>Python中继承的写法：<code>class 子类名(基类名)</code></li>\n<li>在编程中一般使用子类去替代基类</li>\n<li>在子类中，通过重新定义父类中的方法，可以让同一种方法在不同的子类中有不同的行为，这称为重写</li>\n</ul>\n<h3 id=\"20-1-11\"><a href=\"#20-1-11\" class=\"headerlink\" title=\"20/1/11\"></a>20/1/11</h3><ul>\n<li>Python中提供两个重要的功能：异常处理和断言（Assertions）来处理运行中出现的异常和错误，他们的功能是用于调试Python程序</li>\n<li>异常：无法正常处理程序时会发生异常，是一个对象，如果不捕获异常，程序会终止执行</li>\n<li>Python中异常处理的写法：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">except</span> exception_type, argument:</span><br><span class=\"line\">\t<span class=\"comment\">#if error occurs in operation1, execute operation2</span></span><br><span class=\"line\">  <span class=\"comment\">#operation2</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#if no error occurs in operation1, execute operation3</span></span><br><span class=\"line\">  <span class=\"comment\">#operation3</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>使用<code>except</code>可以不带异常类型，但是会让<code>try-except</code>语句捕获所有的异常，不建议这样写</li>\n<li>可以使用<code>expect(exception1[, expection2[, expection3]])</code>来添加多个异常类型</li>\n<li><code>argument</code>为异常的参数，可以用于输出异常信息的异常值</li>\n<li>也可以使用如下方法，但是与<code>try-except</code>有所不同：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">finally</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p><code>finally</code>和<code>except</code>不可以同时使用</p>\n</li>\n<li><p>可以使用<code>raise</code>触发异常</p>\n</li>\n<li><p><code>append()</code>方法用于在列表末尾添加新的对象，对于一个数组<code>list</code>，可以这样使用：<code>list.append()</code></p>\n</li>\n<li><p>多线程用于同时执行多个不同的程序，可以把占据长时间的程序中的任务放到后台处理</p>\n</li>\n<li><p>线程与进程：独立的线程有自己的程序入口、执行序列、程序出口，但是线程不可以独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制</p>\n</li>\n<li><p>在Python中使用线程：<code>thread.start_new_thread(function, args[, kwargs])</code>，其中<code>function</code>为线程函数，这个函数需要提前定义好，<code>args</code>为传递给线程函数的参数，是一个元组，<code>kwargs</code>为可选参数，此种方式称为函数式，线程的结束一般靠函数的自然结束</p>\n</li>\n<li><p>此外还可以使用Python所提供的<code>threading</code>模块，直接从<code>threading.Thread</code>继承：<code>class myThread(threading.Thread)</code>，然后重写<code>__init__</code>和<code>run</code>方法，把需要执行的代码写到<code>run</code>方法里面，<code>__init__</code>的重写方法如下：</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, threadID, name, counter)</span>:</span></span><br><span class=\"line\">\tthreading.Thread.__init__(self)</span><br><span class=\"line\">\tself.threadID = threadID</span><br><span class=\"line\">\tself.name = name</span><br><span class=\"line\">\tself.counter = counter</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>上述<code>thread</code>类提供了以下方法：<ul>\n<li><code>run()</code>：表示线程活动的方法</li>\n<li><code>start</code>：启动线程</li>\n<li><code>join()</code>：等待直到线程终止</li>\n<li><code>isAlive()</code>：查询线程是否活动</li>\n<li><code>getName()</code>：返回线程名</li>\n<li><code>setName()</code>：设置线程名</li>\n</ul>\n</li>\n<li>为了避免两个或多个线程同时运行，产生冲突，可以使用线程锁来控制线程执行的优先顺序，被锁定的线程优先执行，其他进程必须停止</li>\n<li>可以使用<code>threading.Lock().acquire()</code>和<code>threading.Lock().release()</code>来锁定和释放线程</li>\n<li>可以建立一个空数组用于存放线程，再通过<code>append</code>方法将线程添加至该数组中，通过遍历数组可以对其中的线程做一样的操作</li>\n</ul>\n<h3 id=\"20-1-13\"><a href=\"#20-1-13\" class=\"headerlink\" title=\"20/1/13\"></a>20/1/13</h3><ul>\n<li>在继承的时候，</li>\n</ul>\n","site":{"data":{}},"more":"<blockquote>\n<p>这篇文章主要记录本人在学习Python时遇到的坑以及这个语言的一些特性，内容以时间顺序整理，比较零散杂乱。对于从零开始的同学，请参考官方文档<a href=\"https://docs.python.org/zh-cn/3/\">Python 3.8.1 中文文档</a>或其他网络上的教程。本文章将持续更新。</p>\n</blockquote>\n<h3 id=\"19-9-14\"><a href=\"#19-9-14\" class=\"headerlink\" title=\"19/9/14\"></a>19/9/14</h3><ul>\n<li>注释方法：<code>#（一行注释）</code>，<code>“”“ ”“”（多行注释）</code></li>\n<li>for循环：<code>for （变量） in （范围）</code>，范围可以用<code>range</code>函数</li>\n<li><code>Input</code>函数的输入是<code>char</code>类型的</li>\n<li><code>//</code>是整除运算</li>\n<li>逗号不可以用来分隔语句</li>\n<li>使用缩进（4个空格）来代替C/C++中的大括号</li>\n</ul>\n<h3 id=\"19-9-15\"><a href=\"#19-9-15\" class=\"headerlink\" title=\"19/9/15\"></a>19/9/15</h3><ul>\n<li><code>for...in</code>循环中，<code>_</code>可以作为循环变量，这时候仅循环指定次数，而不需要关心循环变量的值；事实上，<code>_</code>是一个合法的标识符，如果不关心这个变量，就可以将其定义成这个值，它是一个垃圾桶</li>\n<li>定义函数时，使用<code>函数名(*参数名)</code>的定义方式， <code>*</code> 代表函数的参数是可变参数，可以有0到多个参数</li>\n<li>一个文件代表一个模块(module)，若在不同的模块中含有同名函数，那么可以通过<code>import</code>导入指定的模块中的函数，如<code>from 模块 import 函数</code>，或者<code>import 模块 as 自定义模块名称</code>，再通过<code>自定义模块名称.函数</code>的方式调用</li>\n<li><code>__name__</code>是Python中一个隐含的变量，代表了模块的名字，只用直接执行的模块的名字才是<code>__main__</code></li>\n<li>可以使用<code>global</code>指定使用的是一个全局变量，如果全局变量中没有找到对应的，那么会定义一个新的全局变量</li>\n<li>嵌套作用域：对于函数a内部的函数b而言，a中定义的变量对b来说是在嵌套作用域中的，若要指定修改嵌套作用域中的变量，可以使用<code>nonlocal</code>指示变量来自嵌套作用域</li>\n<li><code>pass</code>是一个空语句，只起到占位作用</li>\n<li>可以定义一个<code>main</code>函数（或者与模块名字相同的函数），再按照<code>if __name__ = &#39;__main__&#39;</code>的格式使脚本执行</li>\n</ul>\n<h3 id=\"19-9-17\"><a href=\"#19-9-17\" class=\"headerlink\" title=\"19/9/17\"></a>19/9/17</h3><ul>\n<li>与字符串有关的函数的调用方式为：<code>字符串名称.字符串操作函数()</code>，在此时字符串是一个对象，字符串操作函数的作用是向字符串对象发送一个消息</li>\n<li>字符串实质上是一个数组，可以进行下标运算</li>\n<li>字符串切片可以在下标运算中使用冒号进行运算，<code>[起始字符:结束字符:间隔]</code>，若不定义起始与终止字符，则默认为整个字符串，当间隔为负值时，以为着切片操作反向</li>\n<li>字符串的索引为负值时，意味着索引从右到左数</li>\n<li>列表可以理解为一个数组，其操作与字符串类似</li>\n<li>可使用<code>sorted</code>函数对列表进行排序</li>\n<li>可以使用生成式语法创建列表：<code>f = [x for x in range(1, 10)]</code>（此方法在创建列表后元素已经准备就绪，耗费较多内存），或<code>f = (x for x in range(1, 10))</code>（此方法创建的是一个生成器对象，需要数据时列表通过生成器产生，节省内存但是耗费较多时间）</li>\n<li>可以使用<code>yield</code>关键字来实现迭代，使用<code>yield</code>就是产生了一个生成器，每次遇到<code>yield</code>时函数会暂停并保存当前所有的运行信息，返回<code>yield</code>的值，并在下一次执行此方法是从当前位置开始运行</li>\n<li>可以定义元组，其相当于不能修改的数组，一个元组中的元素数据类型可以不同，定义元组使用<code>t = ()</code></li>\n<li>列表和元组可以互相转换</li>\n<li>可以定义集合，定义集合可以使用<code>set = {}</code>，元组可以转换为集合</li>\n<li>字典类似于数组，但是它是由多组键值对组成的</li>\n</ul>\n<h3 id=\"19-9-19\"><a href=\"#19-9-19\" class=\"headerlink\" title=\"19/9/19\"></a>19/9/19</h3><ul>\n<li>使用class关键字定义类，再在类中定义函数，如：<code>class 类名(object)</code></li>\n<li><code>__init__</code>函数是用于在创建对象时进行的初始化操作</li>\n<li>self是类的本身，是它的实例变量，在类中所有函数的第一个参数就是self，在类中修改属性值需使用<code>self.属性值 = x</code>的语法</li>\n<li>实例化类的方法：<code>对象名 = 类名(初始化函数参数)</code></li>\n<li>对象中方法的引用可以采用<code>对象.方法（也即函数）</code>的语句，通过此方式向对象发送消息</li>\n<li>Python中，属性和方法的访问权限只有<code>public</code>和<code>private</code>，若希望属性或方法是私有的，在给它们命名的时候要使用<code>__</code>开头，但是不建议将属性设置为私有的</li>\n<li>使用<code>_</code>开头暗示属性或方法是受保护(protected)的，访问它们建议通过特定的方法，但实际上它们还是可以直接被外部访问</li>\n<li>可以通过在类中定义方法以访问对象受保护的属性，在定义这些方法（函数）时，要在上一行使用<code>@property</code>包装这些方法</li>\n<li>对于被保护的属性，在访问它们时采用<code>getter</code>方法，需添加<code>@property</code>，在修改它们时采用<code>setter</code>方法，需添加<code>@函数（即方法）名.setter</code></li>\n<li>Python可以对对象动态绑定新的属性或方法</li>\n<li>可以使用<code>__slots__</code>限定对象只能绑定某些属性，但是它只对当前类的对象生效，对子类不起作用</li>\n<li>可以通过给类发送消息，在类的对象被创建出来之前直接使用其中的方法，此种方法被称为静态方法，需要在定义时添加<code>@staticmethod</code>，此类方法的参数不含有<code>self</code></li>\n<li>通过类方法可以获取类相关的信息并且可以<strong>创建出类的对象</strong>，需要在定义时添加<code>@classmethod</code>，类方法的第一个参数是<code>cls</code>，这个<code>cls</code>相当于就是在外部实例化类时定义的对象名，只不过它是放在类的内部使用了，其功能就是可以像在外部调用对象的属性和方法一样在类的内部使用对象（类）的属性和方法</li>\n</ul>\n<h3 id=\"19-9-20\"><a href=\"#19-9-20\" class=\"headerlink\" title=\"19/9/20\"></a>19/9/20</h3><ul>\n<li>类之间的关系：<ul>\n<li>is-a：继承或者泛化，如：<strong>student</strong> is a <strong>human being</strong>，<strong>cell phone</strong> is a <strong>electronic device</strong></li>\n<li>has-a：关联，如 <strong>department</strong> has an <strong>employee</strong></li>\n<li>use-a：依赖，如 <strong>driver</strong> use a <strong>car</strong> </li>\n</ul>\n</li>\n<li>类与类之间可以继承，提供继承信息的成为父类（超类或者基类），得到继承的称为子类（派生类或者衍生类）</li>\n<li>Python中继承的写法：<code>class 子类名(基类名)</code></li>\n<li>在编程中一般使用子类去替代基类</li>\n<li>在子类中，通过重新定义父类中的方法，可以让同一种方法在不同的子类中有不同的行为，这称为重写</li>\n</ul>\n<h3 id=\"20-1-11\"><a href=\"#20-1-11\" class=\"headerlink\" title=\"20/1/11\"></a>20/1/11</h3><ul>\n<li>Python中提供两个重要的功能：异常处理和断言（Assertions）来处理运行中出现的异常和错误，他们的功能是用于调试Python程序</li>\n<li>异常：无法正常处理程序时会发生异常，是一个对象，如果不捕获异常，程序会终止执行</li>\n<li>Python中异常处理的写法：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">except</span> exception_type, argument:</span><br><span class=\"line\">\t<span class=\"comment\">#if error occurs in operation1, execute operation2</span></span><br><span class=\"line\">  <span class=\"comment\">#operation2</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#if no error occurs in operation1, execute operation3</span></span><br><span class=\"line\">  <span class=\"comment\">#operation3</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>使用<code>except</code>可以不带异常类型，但是会让<code>try-except</code>语句捕获所有的异常，不建议这样写</li>\n<li>可以使用<code>expect(exception1[, expection2[, expection3]])</code>来添加多个异常类型</li>\n<li><code>argument</code>为异常的参数，可以用于输出异常信息的异常值</li>\n<li>也可以使用如下方法，但是与<code>try-except</code>有所不同：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">finally</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p><code>finally</code>和<code>except</code>不可以同时使用</p>\n</li>\n<li><p>可以使用<code>raise</code>触发异常</p>\n</li>\n<li><p><code>append()</code>方法用于在列表末尾添加新的对象，对于一个数组<code>list</code>，可以这样使用：<code>list.append()</code></p>\n</li>\n<li><p>多线程用于同时执行多个不同的程序，可以把占据长时间的程序中的任务放到后台处理</p>\n</li>\n<li><p>线程与进程：独立的线程有自己的程序入口、执行序列、程序出口，但是线程不可以独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制</p>\n</li>\n<li><p>在Python中使用线程：<code>thread.start_new_thread(function, args[, kwargs])</code>，其中<code>function</code>为线程函数，这个函数需要提前定义好，<code>args</code>为传递给线程函数的参数，是一个元组，<code>kwargs</code>为可选参数，此种方式称为函数式，线程的结束一般靠函数的自然结束</p>\n</li>\n<li><p>此外还可以使用Python所提供的<code>threading</code>模块，直接从<code>threading.Thread</code>继承：<code>class myThread(threading.Thread)</code>，然后重写<code>__init__</code>和<code>run</code>方法，把需要执行的代码写到<code>run</code>方法里面，<code>__init__</code>的重写方法如下：</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, threadID, name, counter)</span>:</span></span><br><span class=\"line\">\tthreading.Thread.__init__(self)</span><br><span class=\"line\">\tself.threadID = threadID</span><br><span class=\"line\">\tself.name = name</span><br><span class=\"line\">\tself.counter = counter</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>上述<code>thread</code>类提供了以下方法：<ul>\n<li><code>run()</code>：表示线程活动的方法</li>\n<li><code>start</code>：启动线程</li>\n<li><code>join()</code>：等待直到线程终止</li>\n<li><code>isAlive()</code>：查询线程是否活动</li>\n<li><code>getName()</code>：返回线程名</li>\n<li><code>setName()</code>：设置线程名</li>\n</ul>\n</li>\n<li>为了避免两个或多个线程同时运行，产生冲突，可以使用线程锁来控制线程执行的优先顺序，被锁定的线程优先执行，其他进程必须停止</li>\n<li>可以使用<code>threading.Lock().acquire()</code>和<code>threading.Lock().release()</code>来锁定和释放线程</li>\n<li>可以建立一个空数组用于存放线程，再通过<code>append</code>方法将线程添加至该数组中，通过遍历数组可以对其中的线程做一样的操作</li>\n</ul>\n<h3 id=\"20-1-13\"><a href=\"#20-1-13\" class=\"headerlink\" title=\"20/1/13\"></a>20/1/13</h3><ul>\n<li>在继承的时候，</li>\n</ul>\n"},{"title":"Summarize of Reinforcement Learning 1","date":"2020-01-17T13:14:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t3.jpeg","excerpt":"A brief introduction to reinforcement learning.","_content":"\n### Preface\n\nThis blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. \n\nThese series of blogs of mine are mostly based on the following works and I'm really grateful to the contributors: \n\n- Online courses of [Stanford University CS234: Reinforcement Learning, Emma Brunskill](https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) and the [notes](https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn).\n- [Blogs of 从流域到海域](https://blog.csdn.net/solo95/category_9298323.html).\n- [Blogs of 叶强](https://zhuanlan.zhihu.com/reinforce).\n\nIf you find any mistake in my articles, please feel free to tell me in comments.\n\n### What is reinforcement learning (RL)?\n\nRL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.\n\nA RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent's desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more \"smarter\" and has a better performance.\n\n### Some basic notions of RL\n\nBecause in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce \"time\" to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript \"t\" means time it is in a time sequence. \n\n- **Agent**: The subject of RL, it is agent that interact with the world.\n- **Model**: The world, the environment, the *agent* stays in the *model*.\n- **Reward**: $ \\{r_t\\} $ , the feedback signal from the *model*, *agent* recieves the *reward*. The *reward* can have different values according to the different **states** of the *agent*.\n- **State**: $\\{s_t\\}$ , the *state* of the *agent*. The *state* can be either finite or infinite, and it is set by people.\n- **Action**: $\\{a_t\\}$ , the movement of the *agent* in the *model*, *actions* are different under different *states*.\n- **Observation**: $\\{o_t\\}$ , the *agent* need to observe its *state* and determine the *reward*.\n- **History**: a sequence of *action*, *reward*, *observation*, which is: $h_t=(a_1,o_1,r_1,...,a_t,o_t,r_t)$.\n- **Sequential Decision Making**: make decision base on the *history*, that is: $a_{t+1}=f(h_t)$.\n\nFigure 1.1 shows how an agent interact with its world.\n\n![Figure 1.1](https://astrobear.top/resource/astroblog/content/rl1.1.jpeg)\n\n### How to model the world?\n\n#### Markov Property\n\n$P(s_{t+1}|s_t,a_t,...,s_1,a_1)=P(s_{t+1}|s_t,a_t)$\n\nLeft-hand side is called the *transition dynamics* of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. \n\nA model consists of the two elements below. \n\n#### Transition dynamics $P(s_{t+1}|s_t,a_t)$\n\nThe probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. \n\n#### Reward function $R(s,a)$\n\nUsually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n### How to make a RL agent?\n\nLet the agent state be a function of the history, $s_t^a=g(h_t)$.\n\nAn agent often consists the three elements below.\n\n#### Policy $\\pi(a_t|s_a^t)$\n\nPolicy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability\n\n$P(a_t=a)=\\pi(a|s_t^a)$.\n\n#### Value function $V^\\pi$\n\nIf we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards\n\n$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+...|s_t=s]$.\n\n#### Model\n\nThe agent in RL may have a model. I have introduced how to make a model in section 3.\n\n### Three questions we are facing\n\n#### Do we need exploration or exploitation?\n\nIn RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.\n\n#### Can the agent generalize its experience?\n\nIn actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?\n\n#### Delayed consequences\n\nThe action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?\n\n### What's next?\n\nNow we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what's the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.","source":"_posts/RLSummarize1.md","raw":"---\ntitle: Summarize of Reinforcement Learning 1\ndate: 2020-1-17 21:14:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t3.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: A brief introduction to reinforcement learning.\n\n#You can begin to input your article below now.\n\n---\n\n### Preface\n\nThis blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. \n\nThese series of blogs of mine are mostly based on the following works and I'm really grateful to the contributors: \n\n- Online courses of [Stanford University CS234: Reinforcement Learning, Emma Brunskill](https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) and the [notes](https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn).\n- [Blogs of 从流域到海域](https://blog.csdn.net/solo95/category_9298323.html).\n- [Blogs of 叶强](https://zhuanlan.zhihu.com/reinforce).\n\nIf you find any mistake in my articles, please feel free to tell me in comments.\n\n### What is reinforcement learning (RL)?\n\nRL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.\n\nA RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent's desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more \"smarter\" and has a better performance.\n\n### Some basic notions of RL\n\nBecause in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce \"time\" to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript \"t\" means time it is in a time sequence. \n\n- **Agent**: The subject of RL, it is agent that interact with the world.\n- **Model**: The world, the environment, the *agent* stays in the *model*.\n- **Reward**: $ \\{r_t\\} $ , the feedback signal from the *model*, *agent* recieves the *reward*. The *reward* can have different values according to the different **states** of the *agent*.\n- **State**: $\\{s_t\\}$ , the *state* of the *agent*. The *state* can be either finite or infinite, and it is set by people.\n- **Action**: $\\{a_t\\}$ , the movement of the *agent* in the *model*, *actions* are different under different *states*.\n- **Observation**: $\\{o_t\\}$ , the *agent* need to observe its *state* and determine the *reward*.\n- **History**: a sequence of *action*, *reward*, *observation*, which is: $h_t=(a_1,o_1,r_1,...,a_t,o_t,r_t)$.\n- **Sequential Decision Making**: make decision base on the *history*, that is: $a_{t+1}=f(h_t)$.\n\nFigure 1.1 shows how an agent interact with its world.\n\n![Figure 1.1](https://astrobear.top/resource/astroblog/content/rl1.1.jpeg)\n\n### How to model the world?\n\n#### Markov Property\n\n$P(s_{t+1}|s_t,a_t,...,s_1,a_1)=P(s_{t+1}|s_t,a_t)$\n\nLeft-hand side is called the *transition dynamics* of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. \n\nA model consists of the two elements below. \n\n#### Transition dynamics $P(s_{t+1}|s_t,a_t)$\n\nThe probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. \n\n#### Reward function $R(s,a)$\n\nUsually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n### How to make a RL agent?\n\nLet the agent state be a function of the history, $s_t^a=g(h_t)$.\n\nAn agent often consists the three elements below.\n\n#### Policy $\\pi(a_t|s_a^t)$\n\nPolicy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability\n\n$P(a_t=a)=\\pi(a|s_t^a)$.\n\n#### Value function $V^\\pi$\n\nIf we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards\n\n$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+...|s_t=s]$.\n\n#### Model\n\nThe agent in RL may have a model. I have introduced how to make a model in section 3.\n\n### Three questions we are facing\n\n#### Do we need exploration or exploitation?\n\nIn RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.\n\n#### Can the agent generalize its experience?\n\nIn actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?\n\n#### Delayed consequences\n\nThe action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?\n\n### What's next?\n\nNow we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what's the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.","slug":"RLSummarize1","published":1,"updated":"2020-02-07T09:39:00.831Z","_id":"ck6ax1lgl000bj1p271y3b93u","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h3><p>This blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. </p>\n<p>These series of blogs of mine are mostly based on the following works and I’m really grateful to the contributors: </p>\n<ul>\n<li>Online courses of <a href=\"https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u\">Stanford University CS234: Reinforcement Learning, Emma Brunskill</a> and the <a href=\"https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn\">notes</a>.</li>\n<li><a href=\"https://blog.csdn.net/solo95/category_9298323.html\">Blogs of 从流域到海域</a>.</li>\n<li><a href=\"https://zhuanlan.zhihu.com/reinforce\">Blogs of 叶强</a>.</li>\n</ul>\n<p>If you find any mistake in my articles, please feel free to tell me in comments.</p>\n<h3 id=\"What-is-reinforcement-learning-RL\"><a href=\"#What-is-reinforcement-learning-RL\" class=\"headerlink\" title=\"What is reinforcement learning (RL)?\"></a>What is reinforcement learning (RL)?</h3><p>RL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.</p>\n<p>A RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent’s desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more “smarter” and has a better performance.</p>\n<h3 id=\"Some-basic-notions-of-RL\"><a href=\"#Some-basic-notions-of-RL\" class=\"headerlink\" title=\"Some basic notions of RL\"></a>Some basic notions of RL</h3><p>Because in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce “time” to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript “t” means time it is in a time sequence. </p>\n<ul>\n<li><strong>Agent</strong>: The subject of RL, it is agent that interact with the world.</li>\n<li><strong>Model</strong>: The world, the environment, the <em>agent</em> stays in the <em>model</em>.</li>\n<li><strong>Reward</strong>: $ {r_t} $ , the feedback signal from the <em>model</em>, <em>agent</em> recieves the <em>reward</em>. The <em>reward</em> can have different values according to the different <strong>states</strong> of the <em>agent</em>.</li>\n<li><strong>State</strong>: ${s_t}$ , the <em>state</em> of the <em>agent</em>. The <em>state</em> can be either finite or infinite, and it is set by people.</li>\n<li><strong>Action</strong>: ${a_t}$ , the movement of the <em>agent</em> in the <em>model</em>, <em>actions</em> are different under different <em>states</em>.</li>\n<li><strong>Observation</strong>: ${o_t}$ , the <em>agent</em> need to observe its <em>state</em> and determine the <em>reward</em>.</li>\n<li><strong>History</strong>: a sequence of <em>action</em>, <em>reward</em>, <em>observation</em>, which is: $h_t=(a_1,o_1,r_1,…,a_t,o_t,r_t)$.</li>\n<li><strong>Sequential Decision Making</strong>: make decision base on the <em>history</em>, that is: $a_{t+1}=f(h_t)$.</li>\n</ul>\n<p>Figure 1.1 shows how an agent interact with its world.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/rl1.1.jpeg\" alt=\"Figure 1.1\"></p>\n<h3 id=\"How-to-model-the-world\"><a href=\"#How-to-model-the-world\" class=\"headerlink\" title=\"How to model the world?\"></a>How to model the world?</h3><h4 id=\"Markov-Property\"><a href=\"#Markov-Property\" class=\"headerlink\" title=\"Markov Property\"></a>Markov Property</h4><p>$P(s_{t+1}|s_t,a_t,…,s_1,a_1)=P(s_{t+1}|s_t,a_t)$</p>\n<p>Left-hand side is called the <em>transition dynamics</em> of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. </p>\n<p>A model consists of the two elements below. </p>\n<h4 id=\"Transition-dynamics-P-s-t-1-s-t-a-t\"><a href=\"#Transition-dynamics-P-s-t-1-s-t-a-t\" class=\"headerlink\" title=\"Transition dynamics $P(s_{t+1}|s_t,a_t)$\"></a>Transition dynamics $P(s_{t+1}|s_t,a_t)$</h4><p>The probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. </p>\n<h4 id=\"Reward-function-R-s-a\"><a href=\"#Reward-function-R-s-a\" class=\"headerlink\" title=\"Reward function $R(s,a)$\"></a>Reward function $R(s,a)$</h4><p>Usually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n<h3 id=\"How-to-make-a-RL-agent\"><a href=\"#How-to-make-a-RL-agent\" class=\"headerlink\" title=\"How to make a RL agent?\"></a>How to make a RL agent?</h3><p>Let the agent state be a function of the history, $s_t^a=g(h_t)$.</p>\n<p>An agent often consists the three elements below.</p>\n<h4 id=\"Policy-pi-a-t-s-a-t\"><a href=\"#Policy-pi-a-t-s-a-t\" class=\"headerlink\" title=\"Policy $\\pi(a_t|s_a^t)$\"></a>Policy $\\pi(a_t|s_a^t)$</h4><p>Policy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability</p>\n<p>$P(a_t=a)=\\pi(a|s_t^a)$.</p>\n<h4 id=\"Value-function-V-pi\"><a href=\"#Value-function-V-pi\" class=\"headerlink\" title=\"Value function $V^\\pi$\"></a>Value function $V^\\pi$</h4><p>If we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards</p>\n<p>$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+…|s_t=s]$.</p>\n<h4 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h4><p>The agent in RL may have a model. I have introduced how to make a model in section 3.</p>\n<h3 id=\"Three-questions-we-are-facing\"><a href=\"#Three-questions-we-are-facing\" class=\"headerlink\" title=\"Three questions we are facing\"></a>Three questions we are facing</h3><h4 id=\"Do-we-need-exploration-or-exploitation\"><a href=\"#Do-we-need-exploration-or-exploitation\" class=\"headerlink\" title=\"Do we need exploration or exploitation?\"></a>Do we need exploration or exploitation?</h4><p>In RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.</p>\n<h4 id=\"Can-the-agent-generalize-its-experience\"><a href=\"#Can-the-agent-generalize-its-experience\" class=\"headerlink\" title=\"Can the agent generalize its experience?\"></a>Can the agent generalize its experience?</h4><p>In actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?</p>\n<h4 id=\"Delayed-consequences\"><a href=\"#Delayed-consequences\" class=\"headerlink\" title=\"Delayed consequences\"></a>Delayed consequences</h4><p>The action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?</p>\n<h3 id=\"What’s-next\"><a href=\"#What’s-next\" class=\"headerlink\" title=\"What’s next?\"></a>What’s next?</h3><p>Now we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what’s the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.</p>\n","site":{"data":{}},"more":"<h3 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h3><p>This blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. </p>\n<p>These series of blogs of mine are mostly based on the following works and I’m really grateful to the contributors: </p>\n<ul>\n<li>Online courses of <a href=\"https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u\">Stanford University CS234: Reinforcement Learning, Emma Brunskill</a> and the <a href=\"https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn\">notes</a>.</li>\n<li><a href=\"https://blog.csdn.net/solo95/category_9298323.html\">Blogs of 从流域到海域</a>.</li>\n<li><a href=\"https://zhuanlan.zhihu.com/reinforce\">Blogs of 叶强</a>.</li>\n</ul>\n<p>If you find any mistake in my articles, please feel free to tell me in comments.</p>\n<h3 id=\"What-is-reinforcement-learning-RL\"><a href=\"#What-is-reinforcement-learning-RL\" class=\"headerlink\" title=\"What is reinforcement learning (RL)?\"></a>What is reinforcement learning (RL)?</h3><p>RL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.</p>\n<p>A RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent’s desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more “smarter” and has a better performance.</p>\n<h3 id=\"Some-basic-notions-of-RL\"><a href=\"#Some-basic-notions-of-RL\" class=\"headerlink\" title=\"Some basic notions of RL\"></a>Some basic notions of RL</h3><p>Because in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce “time” to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript “t” means time it is in a time sequence. </p>\n<ul>\n<li><strong>Agent</strong>: The subject of RL, it is agent that interact with the world.</li>\n<li><strong>Model</strong>: The world, the environment, the <em>agent</em> stays in the <em>model</em>.</li>\n<li><strong>Reward</strong>: $ {r_t} $ , the feedback signal from the <em>model</em>, <em>agent</em> recieves the <em>reward</em>. The <em>reward</em> can have different values according to the different <strong>states</strong> of the <em>agent</em>.</li>\n<li><strong>State</strong>: ${s_t}$ , the <em>state</em> of the <em>agent</em>. The <em>state</em> can be either finite or infinite, and it is set by people.</li>\n<li><strong>Action</strong>: ${a_t}$ , the movement of the <em>agent</em> in the <em>model</em>, <em>actions</em> are different under different <em>states</em>.</li>\n<li><strong>Observation</strong>: ${o_t}$ , the <em>agent</em> need to observe its <em>state</em> and determine the <em>reward</em>.</li>\n<li><strong>History</strong>: a sequence of <em>action</em>, <em>reward</em>, <em>observation</em>, which is: $h_t=(a_1,o_1,r_1,…,a_t,o_t,r_t)$.</li>\n<li><strong>Sequential Decision Making</strong>: make decision base on the <em>history</em>, that is: $a_{t+1}=f(h_t)$.</li>\n</ul>\n<p>Figure 1.1 shows how an agent interact with its world.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/rl1.1.jpeg\" alt=\"Figure 1.1\"></p>\n<h3 id=\"How-to-model-the-world\"><a href=\"#How-to-model-the-world\" class=\"headerlink\" title=\"How to model the world?\"></a>How to model the world?</h3><h4 id=\"Markov-Property\"><a href=\"#Markov-Property\" class=\"headerlink\" title=\"Markov Property\"></a>Markov Property</h4><p>$P(s_{t+1}|s_t,a_t,…,s_1,a_1)=P(s_{t+1}|s_t,a_t)$</p>\n<p>Left-hand side is called the <em>transition dynamics</em> of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. </p>\n<p>A model consists of the two elements below. </p>\n<h4 id=\"Transition-dynamics-P-s-t-1-s-t-a-t\"><a href=\"#Transition-dynamics-P-s-t-1-s-t-a-t\" class=\"headerlink\" title=\"Transition dynamics $P(s_{t+1}|s_t,a_t)$\"></a>Transition dynamics $P(s_{t+1}|s_t,a_t)$</h4><p>The probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. </p>\n<h4 id=\"Reward-function-R-s-a\"><a href=\"#Reward-function-R-s-a\" class=\"headerlink\" title=\"Reward function $R(s,a)$\"></a>Reward function $R(s,a)$</h4><p>Usually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n<h3 id=\"How-to-make-a-RL-agent\"><a href=\"#How-to-make-a-RL-agent\" class=\"headerlink\" title=\"How to make a RL agent?\"></a>How to make a RL agent?</h3><p>Let the agent state be a function of the history, $s_t^a=g(h_t)$.</p>\n<p>An agent often consists the three elements below.</p>\n<h4 id=\"Policy-pi-a-t-s-a-t\"><a href=\"#Policy-pi-a-t-s-a-t\" class=\"headerlink\" title=\"Policy $\\pi(a_t|s_a^t)$\"></a>Policy $\\pi(a_t|s_a^t)$</h4><p>Policy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability</p>\n<p>$P(a_t=a)=\\pi(a|s_t^a)$.</p>\n<h4 id=\"Value-function-V-pi\"><a href=\"#Value-function-V-pi\" class=\"headerlink\" title=\"Value function $V^\\pi$\"></a>Value function $V^\\pi$</h4><p>If we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards</p>\n<p>$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+…|s_t=s]$.</p>\n<h4 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h4><p>The agent in RL may have a model. I have introduced how to make a model in section 3.</p>\n<h3 id=\"Three-questions-we-are-facing\"><a href=\"#Three-questions-we-are-facing\" class=\"headerlink\" title=\"Three questions we are facing\"></a>Three questions we are facing</h3><h4 id=\"Do-we-need-exploration-or-exploitation\"><a href=\"#Do-we-need-exploration-or-exploitation\" class=\"headerlink\" title=\"Do we need exploration or exploitation?\"></a>Do we need exploration or exploitation?</h4><p>In RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.</p>\n<h4 id=\"Can-the-agent-generalize-its-experience\"><a href=\"#Can-the-agent-generalize-its-experience\" class=\"headerlink\" title=\"Can the agent generalize its experience?\"></a>Can the agent generalize its experience?</h4><p>In actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?</p>\n<h4 id=\"Delayed-consequences\"><a href=\"#Delayed-consequences\" class=\"headerlink\" title=\"Delayed consequences\"></a>Delayed consequences</h4><p>The action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?</p>\n<h3 id=\"What’s-next\"><a href=\"#What’s-next\" class=\"headerlink\" title=\"What’s next?\"></a>What’s next?</h3><p>Now we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what’s the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.</p>\n"},{"title":"华为云+nginx服务器搭建总结","date":"2020-01-08T02:29:00.000Z","thumbnail":"https://kinsta.com/wp-content/uploads/2018/03/what-is-nginx.png","excerpt":"搭建自己的服务器并不难，只是过程较为复杂。","_content":"\n> 由于自己是去年七月配置好的服务器，有一些细节或者遇到的问题已经记不太清，故本文可能会有不完整的地方，遇到问题请善用搜索引擎，而且服务器的配置方法也不只有这一种。本文主要用作对自己操作步骤和方法的一个总结，以便于日后查阅。本文章将持续更新。\n\n### 购买服务器\n\n首先去[华为云官网](https://www.huaweicloud.com/?locale=zh-cn)注册一个账号。如果是学生，可以搜索“学生”，并进行学生认证。学生认证的步骤参见[学生认证流程](https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html)。进行身份验证后可以购买学生优惠套餐，云服务器价格只要99元/年，比阿里云和腾讯云的都要便宜一些。\n\n![华为云学生优惠](https://astrobear.top/resource/astroblog/content/hwcloud_discount.png)\n\n购买完成后，你可以在控制台看到自己现有的资源以及运行情况。\n\n![控制台](https://astrobear.top/resource/astroblog/content/console.png)\n\n### 配置安全组\n\n> 安全组是一个逻辑上的分组，为具有相同安全保护需求并相互信任的云服务器提供访问策略。安全组创建后，用户可以在安全组中定义各种访问规则，当云服务器加入该安全组后，即受到这些访问规则的保护。\n>\n> 系统会为每个用户默认创建一个默认安全组，默认安全组的规则是在出方向上的数据报文全部放行，入方向访问受限，安全组内的云服务器无需添加规则即可互相访问。默认安全组可以直接使用。\n>\n> 安全组创建后，你可以在安全组中设置出方向、入方向规则，这些规则会对安全组内部的云服务器出入方向网络流量进行访问控制，当云服务器加入该安全组后，即受到这些访问规则的保护。[^1]\n\n在控制台点击“弹性云服务器ECS”，在这里你可看到你的服务器的公网IP，请记下这个IP地址。然后点击在列表中点击你的服务器的名称。\n\n![选择服务器](https://astrobear.top/resource/astroblog/content/security_groups.png)\n\n进入云服务器管理页面后，点击“安全组”。再点击“Sys-default”可以看到默认安全组。然后下面给出的图片是我目前的安全组设置，仅供参考。选择“入/出方向方向规则”，再点击“添加规则“即可手动添加规则。一般来说，配置的都是入方向的安全组，并且源地址（访问服务器的设备的IP地址）都为“0.0.0.0/0”（所有IP地址）。\n\n通常需要配置如下几个功能：\n\n- SSH远程连接Linux弹性云服务器（协议：SSH，端口：22）\n- 公网“ping”ECS弹性云服务器（协议：ICMP，端口：全部）\n- 弹性云服务器作Web服务器\n  - 协议：http，端口：80\n  - 协议：https，端口：433\n\n详细配置请参考[安全组配置示例](https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html)。\n\n![安全组设置](https://astrobear.top/resource/astroblog/content/sg_settings.png)\n\n![安全组设置](https://astrobear.top/resource/astroblog/content/sg_settings1.png)\n\n配置完成后，可以打开电脑上的终端，用下面的语句测试一下：\n\n`ping 你的公网IP`\n\n出现类似下面的内容就代表成功了：\n\n![ping测试](https://astrobear.top/resource/astroblog/content/ping_test.png)\n\n你可以按下`Ctrl+C`来结束`ping`这个进程。\n\n然后在终端里输入：\n\n`ssh 你的公网IP`\n\n如果你的安全组配置正确的话，会让你输入服务器的登录密码。输入密码（注意：密码是不会显示的）后回车，应该可以看到这样的输出：\n\n![ssh登录](https://astrobear.top/resource/astroblog/content/ssh_login.png)\n\n这个时候，你的终端就已经连接上了服务器的系统了，你在终端里的一切操作都是作用在服务器上的。\n\n### 在服务器上安装nginx\n\n首先请在终端使用ssh登录你的服务器，然后按照下面给出的顺序输入命令。\n\n```shell\nyum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #安装编译工具及库文件\ncd /usr/local/ #切换到目标安装文件夹\nwget http://nginx.org/download/nginx-1.16.1.tar.gz #下载最新版本的Nginx\ntar -zxvf nginx-1.16.1.tar.gz #解压文件\ncd nginx-1.16.1 #进入解压的文件夹\n./configure #执行程序\nmake #编译\nmake install #安装\ncd /usr/local/nginx/sbin #进入Nginx安装目录\n./nginx #运行Nginx\n```\n\n此时，安装应该已经完成了。打开浏览器，在地址栏中输入你的公网ip。如果看到下图所示内容，就代表安装成功了。\n\n![nginx安装成功](https://astrobear.top/resource/astroblog/content/nginx_install.png)\n\n### 创建属于你自己的域名\n\n在拥有了自己的服务器以后，就可以做很多事情了。但是现在你只能通过IP地址访问自己的服务器，看起来总是有点别扭。另外，如果你想要网站有一定的影响力的话，仅有IP地址会让人几乎找不到你的网站，而且也不符合国家法律规定。所以还是建议大家弄一个自己的域名。\n\n现在市面上的云服务器提供商也都提供域名注册的服务，直接在你的服务提供商的平台上面注册即可。下面我继续用华为云的平台演示。\n\n首先在华为云网站页面的导航栏的搜索框内搜索“域名”，打开第一个链接“域名注册服务”。也可以直接点击这里：[域名注册服务_华为云](https://www.huaweicloud.com/product/domain.html)。\n\n然后你可以在网页中选择你的域名，常见的如`.com`，`.cn`，`.net`等。这些域名会相对比较贵。作为学生党，我选择一个最便宜的域名`.top`，只需要9元/年。\n\n点击你想要的域名后，会跳转到一个新的页面。接下来再次选择你要的域名，并且在“查域名”的搜索框内输入你想要的域名，看看是否已经被占用，如果被占用了就换一个。若显示“域名可注册”，就点击“立即购买”。\n\n![域名购买](https://astrobear.top/resource/astroblog/content/buy_domain.png)\n\n购买完成后，你就拥有了自己域名了！\n\n### 备案\n\n> 备案是中国大陆的一项法规，使用大陆节点服务器提供互联网信息服务的用户，需要在服务器提供商处提交备案申请。\n>\n> 根据工信部《互联网信息服务管理办法》(国务院292号令)和工信部令第33号《非经营性互联网信息服务备案管理办法》规定，国家对经营性互联网信息服务实行许可制度，对非经营性互联网信息服务实行备案制度。未取得许可或者未履行备案手续的，不得从事互联网信息服务，否则属违法行为。通俗来讲，要开办网站必须先办理网站备案，备案成功并获取通信管理局下发的ICP备案号后才能开通访问。[^2]\n\n这一步不多说了，具体步骤比较繁琐，花费的时间也比较长，需要一两周。网站上有很清晰的[操作方法](https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html)，请自行查阅，根据步骤操作即可。需要注意一点的是，在审核过程中可能会接到服务提供商打来的电话，不要漏接。\n\n需要注意的是，上面的备案操作是在工信部备案的。完成了在工信部的备案以后还需要公安备案。具体[操作方法](http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c)也请自行查阅。\n\n### 域名解析\n\n在完成一系列繁琐的备案流程以后，你的网站还不可以通过域名访问。只有把你的域名跟服务器的IP地址绑定在一起之后，并且在服务器上修改了配置文件之后才可以。\n\n首先打开管理控制台，在控制台中选择“域名注册”。然后在下面的页面中点击“解析”。\n\n![域名注册](https://astrobear.top/resource/astroblog/content/domain.png)\n\n点击你的域名，显示如下页面。这里显示的是你域名的记录集，前两个记录集应该是预置设置，不可暂停服务。<span id=\"1\">你可以在这基础上添加自己的记录集。</span>\n\n![记录集](https://astrobear.top/resource/astroblog/content/record.png)\n\n点击页面右上角红色按钮以添加记录集。添加记录集的配置如下图所示。下图中给出的例子是添加的“A”型记录集，也即通过`example.com`访问网站。若需要通过`www.example.com`访问网站，则需要为`example.com`的子域名添加“A”型记录集。具体配置参见：[配置网站解析_华为云](https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1)。点击“确定”，完成添加。你可以通过`ping 你的域名`来测试你添加的记录集是否生效了。\n\n![添加记录集](https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png)\n\n### 配置nginx\n\n<span id=\"2\">打开</span>你电脑上的终端，输入命令：`ssh 你的IP地址`，输入你的服务器的密码。\n\n进入你的nginx的安装目录：`cd /usr/local/nginx/`。\n\n使用vim打开nginx的配置文件：`vim ./conf/nginx.conf`。\n\n按`I`开始输入。\n\n在最后一个大括号前插入以下内容：\n\n```nginx\nserver {\n\t    listen   80; #监听端口设为 80\n\t    server_name  example.com; #绑定您的域名\n\t    index index.htm index.html; #指定默认文件\n\t    root html; #指定网站根目录\n}\n```\n\n然后按`esc`退出编辑，再按`Shift+zz`保存。\n\n输入：`cd ./sbin`，切换文件夹。\n\n执行命令：`nginx -s relod`，重启nginx服务。\n\n这时候再尝试用浏览器访问你的域名，应该会显示之前出现过的“Welcome to nginx ”的页面了！\n\n### 申请SSL证书\n\nSSL证书可以在数据传输的过程中对其进行加密和隐藏，可以极大地提高数据传输的安全性。拥有SSL证书的网站的请求头都是`https`，并且在链接旁边会出现一把小锁。但是，SSL证书并不是所有网站都必须的，这视你的需要而定。比如，微信小程序的服务器就必须要有域名和SSL证书。另外，出于信息传输的安全性方面的考虑，有SSL证书还是显得更为妥当和专业一点。\n\n现在市面上各大云服务器提供商也都提供配套的SSL证书申请服务，一般都是提供企业级的证书，价格比较昂贵。但是同时网络上也有一些免费的SSL证书服务可以选择。下面还是以华为云的平台为例，简单说明一下如何申请SSL证书。\n\n首先在华为云页面的导航栏的搜索框内搜索“免费证书“，然后点击[亚洲诚信域名型DV单域名SSL证书--免费证书](https://marketplace.huaweicloud.com/product/00301-315148-0--0)，可以看到证书的价格是0.00元。点击“立即购买”。\n\n![购买SSL证书](https://astrobear.top/resource/astroblog/content/buy_ssl.png)\n\n完成购买后请不要立即关闭页面，页面中的订单号在之后还需要用到。尔后，系统会发送”HuaweiCloud账户申请”邮件至用户邮箱，即你在华为云的注册邮箱。\n\n![HuaweiCloud账户申请](https://astrobear.top/resource/astroblog/content/request_account.png)\n\n点击邮件中的登录地址进入系统，并使用邮件提供的账号和初始密码进行登录。登入系统后请修改你的初始密码，然后请根据华为云中给你提供的订单号在该系统中查询你的订单。查询到你的订单以后，需要你补充一些信息，请如实填写。系统会要你填写公司信息，如果只是个人网站，那么公司名称直接填写你的名字即可，公司地址就填写你的住址。\n\n填写完成后会进入审核阶段，系统会给你发送一封邮件。\n\n![证书审核](https://astrobear.top/resource/astroblog/content/check.png)\n\n根据邮件的提示，需要在记录集中添加新的内容。请根据[前文](#1)所述方法，将邮件中的内容添加至新的记录集。填写方法如下图所示。\n\n![填写记录集](https://astrobear.top/resource/astroblog/content/modify_record.png)\n\n填写完成后，可以在本地电脑的终端里输入`nslookup -querytype=txt 你的域名`来测试记录集是否生效。\n\n![测试记录集](https://astrobear.top/resource/astroblog/content/test_record.png)\n\n一般来说，记录集生效后10分钟以内证书就会颁发了。\n\n![证书颁发](https://astrobear.top/resource/astroblog/content/issue.png)\n\n### SSL证书部署\n\n接下来我们要把SSL证书部署到我们的服务器上。\n\n在收到的“证书颁发”的邮件的底部有一条链接，点击这条链接，进入证书管理系统。登录系统，在左侧导航栏中点击“SSL证书”，再点击“预览”，再在右侧的“信息预览”中点击“下载最新证书“。\n\n![下载证书](https://astrobear.top/resource/astroblog/content/download_cert.png)\n\n在弹出的对话框内，选择证书格式为“PEM(适用于Nginx,SLB)”，输入你的订单密码。证书密码可以留空。\n\n![下载证书](https://astrobear.top/resource/astroblog/content/download_cert1.png)\n\n下载完成后，解压下载的压缩包，需要输入你的订单密码（如果你没有设置证书密码）。解压以后可以得到下图两个文件。\n\n![解压缩](https://astrobear.top/resource/astroblog/content/unzip_cert.png)\n\n接下来，打开你的终端，按顺序输入下列命令：\n\n```shell\nssh 你的公网IP #ssh登录，输入你的密码\ncd /usr/local/nginx #切换到nginx的安装目录\nmkdir ./cert #创建一个新文件夹cert用于存放你的证书\nexit #断开与服务器的连接\nscp 文件的路径/你的域名.key 你的服务器用户名@你的服务器IP地址:./cert #将.key文件上传到你的服务器的指定目录下\nscp 文件的路径/你的域名.crt 你的服务器用户名@你的服务器IP地址:./cert #将.crt文件上传到你的服务器的指定目录下\n```\n\n接下来我们需要修改nginx的配置文件。参考[前文](#2)所述方法打开nginx的配置文件。先将你之前插入的内容删除或者使用`#`注释掉，然后在最后一个大括号前插入以下内容：\n\n```nginx\nserver {\n         listen       443 ssl;\n         server_name  example.com; #你证书绑定的域名;\n\n        ssl_certificate      /usr/local/nginx/cert/你的域名.crt;\n        ssl_certificate_key  /usr/local/nginx/cert/你的域名.key;\n\n        ssl_session_cache    shared:SSL:1m;\n        ssl_session_timeout  5m;\n\n        ssl_ciphers  HIGH:!aNULL:!MD5;\n        ssl_prefer_server_ciphers  on;\n        \n        location / {\n            index index.htm index.html; #指定默认文件。\n\t    \t\t\troot html; #指定网站根目录。\n        }\n}\nserver { #将你的80端口重定向至433端口，即强制使用https访问\n  \t\t\tlisten 80;\n  \t\t\tserver_name; example.com; #你的域名\n\t\t\t\trewrite ^/(.*)$ https://example.com:443/$1 permanent;\n}\n```\n\n将文件保存以后重启nginx服务。\n\n重启以后你可能会遇到这样的问题：`**unknown directive “ssl” in /usr/local/nginx/conf/nginx.conf:121**`，这是因为你在安装nginx时，没有编译SSL模块。你可以在终端里按照下述步骤解决[^ 3]：\n\n```shell\ncd ../nginx-1.16.1 #进入到nginx的源码包的目录下\n./configure --with-http_ssl_module #带参数执行程序\nmake #编译\ncp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #备份旧的nginx\ncp ./objs/nginx /usr/local/nginx/sbin/ #然后将新的nginx的程序复制一份\ncd /usr/local/nginx/sbin/ #切换到sbin目录\n./nginx -s reload #重启nginx服务\n```\n\n如果重启成功的话，打开浏览器访问你的域名，这时候应该可以在链接旁边看到一个小锁了！\n\n[^1]:https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\n\n[^2]: https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\n[^ 3]: https://blog.csdn.net/qq_26369317/article/details/102863613\n\n","source":"_posts/华为云+nginx服务器搭建总结.md","raw":"---\ntitle: 华为云+nginx服务器搭建总结\ndate: 2020-1-8 10:29\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- Nginx\n\t- Internet server\n\t- Network Technology\n\t- Experience\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://kinsta.com/wp-content/uploads/2018/03/what-is-nginx.png\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: 搭建自己的服务器并不难，只是过程较为复杂。\n\n#You can begin to input your article below now.\n---\n\n> 由于自己是去年七月配置好的服务器，有一些细节或者遇到的问题已经记不太清，故本文可能会有不完整的地方，遇到问题请善用搜索引擎，而且服务器的配置方法也不只有这一种。本文主要用作对自己操作步骤和方法的一个总结，以便于日后查阅。本文章将持续更新。\n\n### 购买服务器\n\n首先去[华为云官网](https://www.huaweicloud.com/?locale=zh-cn)注册一个账号。如果是学生，可以搜索“学生”，并进行学生认证。学生认证的步骤参见[学生认证流程](https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html)。进行身份验证后可以购买学生优惠套餐，云服务器价格只要99元/年，比阿里云和腾讯云的都要便宜一些。\n\n![华为云学生优惠](https://astrobear.top/resource/astroblog/content/hwcloud_discount.png)\n\n购买完成后，你可以在控制台看到自己现有的资源以及运行情况。\n\n![控制台](https://astrobear.top/resource/astroblog/content/console.png)\n\n### 配置安全组\n\n> 安全组是一个逻辑上的分组，为具有相同安全保护需求并相互信任的云服务器提供访问策略。安全组创建后，用户可以在安全组中定义各种访问规则，当云服务器加入该安全组后，即受到这些访问规则的保护。\n>\n> 系统会为每个用户默认创建一个默认安全组，默认安全组的规则是在出方向上的数据报文全部放行，入方向访问受限，安全组内的云服务器无需添加规则即可互相访问。默认安全组可以直接使用。\n>\n> 安全组创建后，你可以在安全组中设置出方向、入方向规则，这些规则会对安全组内部的云服务器出入方向网络流量进行访问控制，当云服务器加入该安全组后，即受到这些访问规则的保护。[^1]\n\n在控制台点击“弹性云服务器ECS”，在这里你可看到你的服务器的公网IP，请记下这个IP地址。然后点击在列表中点击你的服务器的名称。\n\n![选择服务器](https://astrobear.top/resource/astroblog/content/security_groups.png)\n\n进入云服务器管理页面后，点击“安全组”。再点击“Sys-default”可以看到默认安全组。然后下面给出的图片是我目前的安全组设置，仅供参考。选择“入/出方向方向规则”，再点击“添加规则“即可手动添加规则。一般来说，配置的都是入方向的安全组，并且源地址（访问服务器的设备的IP地址）都为“0.0.0.0/0”（所有IP地址）。\n\n通常需要配置如下几个功能：\n\n- SSH远程连接Linux弹性云服务器（协议：SSH，端口：22）\n- 公网“ping”ECS弹性云服务器（协议：ICMP，端口：全部）\n- 弹性云服务器作Web服务器\n  - 协议：http，端口：80\n  - 协议：https，端口：433\n\n详细配置请参考[安全组配置示例](https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html)。\n\n![安全组设置](https://astrobear.top/resource/astroblog/content/sg_settings.png)\n\n![安全组设置](https://astrobear.top/resource/astroblog/content/sg_settings1.png)\n\n配置完成后，可以打开电脑上的终端，用下面的语句测试一下：\n\n`ping 你的公网IP`\n\n出现类似下面的内容就代表成功了：\n\n![ping测试](https://astrobear.top/resource/astroblog/content/ping_test.png)\n\n你可以按下`Ctrl+C`来结束`ping`这个进程。\n\n然后在终端里输入：\n\n`ssh 你的公网IP`\n\n如果你的安全组配置正确的话，会让你输入服务器的登录密码。输入密码（注意：密码是不会显示的）后回车，应该可以看到这样的输出：\n\n![ssh登录](https://astrobear.top/resource/astroblog/content/ssh_login.png)\n\n这个时候，你的终端就已经连接上了服务器的系统了，你在终端里的一切操作都是作用在服务器上的。\n\n### 在服务器上安装nginx\n\n首先请在终端使用ssh登录你的服务器，然后按照下面给出的顺序输入命令。\n\n```shell\nyum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #安装编译工具及库文件\ncd /usr/local/ #切换到目标安装文件夹\nwget http://nginx.org/download/nginx-1.16.1.tar.gz #下载最新版本的Nginx\ntar -zxvf nginx-1.16.1.tar.gz #解压文件\ncd nginx-1.16.1 #进入解压的文件夹\n./configure #执行程序\nmake #编译\nmake install #安装\ncd /usr/local/nginx/sbin #进入Nginx安装目录\n./nginx #运行Nginx\n```\n\n此时，安装应该已经完成了。打开浏览器，在地址栏中输入你的公网ip。如果看到下图所示内容，就代表安装成功了。\n\n![nginx安装成功](https://astrobear.top/resource/astroblog/content/nginx_install.png)\n\n### 创建属于你自己的域名\n\n在拥有了自己的服务器以后，就可以做很多事情了。但是现在你只能通过IP地址访问自己的服务器，看起来总是有点别扭。另外，如果你想要网站有一定的影响力的话，仅有IP地址会让人几乎找不到你的网站，而且也不符合国家法律规定。所以还是建议大家弄一个自己的域名。\n\n现在市面上的云服务器提供商也都提供域名注册的服务，直接在你的服务提供商的平台上面注册即可。下面我继续用华为云的平台演示。\n\n首先在华为云网站页面的导航栏的搜索框内搜索“域名”，打开第一个链接“域名注册服务”。也可以直接点击这里：[域名注册服务_华为云](https://www.huaweicloud.com/product/domain.html)。\n\n然后你可以在网页中选择你的域名，常见的如`.com`，`.cn`，`.net`等。这些域名会相对比较贵。作为学生党，我选择一个最便宜的域名`.top`，只需要9元/年。\n\n点击你想要的域名后，会跳转到一个新的页面。接下来再次选择你要的域名，并且在“查域名”的搜索框内输入你想要的域名，看看是否已经被占用，如果被占用了就换一个。若显示“域名可注册”，就点击“立即购买”。\n\n![域名购买](https://astrobear.top/resource/astroblog/content/buy_domain.png)\n\n购买完成后，你就拥有了自己域名了！\n\n### 备案\n\n> 备案是中国大陆的一项法规，使用大陆节点服务器提供互联网信息服务的用户，需要在服务器提供商处提交备案申请。\n>\n> 根据工信部《互联网信息服务管理办法》(国务院292号令)和工信部令第33号《非经营性互联网信息服务备案管理办法》规定，国家对经营性互联网信息服务实行许可制度，对非经营性互联网信息服务实行备案制度。未取得许可或者未履行备案手续的，不得从事互联网信息服务，否则属违法行为。通俗来讲，要开办网站必须先办理网站备案，备案成功并获取通信管理局下发的ICP备案号后才能开通访问。[^2]\n\n这一步不多说了，具体步骤比较繁琐，花费的时间也比较长，需要一两周。网站上有很清晰的[操作方法](https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html)，请自行查阅，根据步骤操作即可。需要注意一点的是，在审核过程中可能会接到服务提供商打来的电话，不要漏接。\n\n需要注意的是，上面的备案操作是在工信部备案的。完成了在工信部的备案以后还需要公安备案。具体[操作方法](http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c)也请自行查阅。\n\n### 域名解析\n\n在完成一系列繁琐的备案流程以后，你的网站还不可以通过域名访问。只有把你的域名跟服务器的IP地址绑定在一起之后，并且在服务器上修改了配置文件之后才可以。\n\n首先打开管理控制台，在控制台中选择“域名注册”。然后在下面的页面中点击“解析”。\n\n![域名注册](https://astrobear.top/resource/astroblog/content/domain.png)\n\n点击你的域名，显示如下页面。这里显示的是你域名的记录集，前两个记录集应该是预置设置，不可暂停服务。<span id=\"1\">你可以在这基础上添加自己的记录集。</span>\n\n![记录集](https://astrobear.top/resource/astroblog/content/record.png)\n\n点击页面右上角红色按钮以添加记录集。添加记录集的配置如下图所示。下图中给出的例子是添加的“A”型记录集，也即通过`example.com`访问网站。若需要通过`www.example.com`访问网站，则需要为`example.com`的子域名添加“A”型记录集。具体配置参见：[配置网站解析_华为云](https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1)。点击“确定”，完成添加。你可以通过`ping 你的域名`来测试你添加的记录集是否生效了。\n\n![添加记录集](https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png)\n\n### 配置nginx\n\n<span id=\"2\">打开</span>你电脑上的终端，输入命令：`ssh 你的IP地址`，输入你的服务器的密码。\n\n进入你的nginx的安装目录：`cd /usr/local/nginx/`。\n\n使用vim打开nginx的配置文件：`vim ./conf/nginx.conf`。\n\n按`I`开始输入。\n\n在最后一个大括号前插入以下内容：\n\n```nginx\nserver {\n\t    listen   80; #监听端口设为 80\n\t    server_name  example.com; #绑定您的域名\n\t    index index.htm index.html; #指定默认文件\n\t    root html; #指定网站根目录\n}\n```\n\n然后按`esc`退出编辑，再按`Shift+zz`保存。\n\n输入：`cd ./sbin`，切换文件夹。\n\n执行命令：`nginx -s relod`，重启nginx服务。\n\n这时候再尝试用浏览器访问你的域名，应该会显示之前出现过的“Welcome to nginx ”的页面了！\n\n### 申请SSL证书\n\nSSL证书可以在数据传输的过程中对其进行加密和隐藏，可以极大地提高数据传输的安全性。拥有SSL证书的网站的请求头都是`https`，并且在链接旁边会出现一把小锁。但是，SSL证书并不是所有网站都必须的，这视你的需要而定。比如，微信小程序的服务器就必须要有域名和SSL证书。另外，出于信息传输的安全性方面的考虑，有SSL证书还是显得更为妥当和专业一点。\n\n现在市面上各大云服务器提供商也都提供配套的SSL证书申请服务，一般都是提供企业级的证书，价格比较昂贵。但是同时网络上也有一些免费的SSL证书服务可以选择。下面还是以华为云的平台为例，简单说明一下如何申请SSL证书。\n\n首先在华为云页面的导航栏的搜索框内搜索“免费证书“，然后点击[亚洲诚信域名型DV单域名SSL证书--免费证书](https://marketplace.huaweicloud.com/product/00301-315148-0--0)，可以看到证书的价格是0.00元。点击“立即购买”。\n\n![购买SSL证书](https://astrobear.top/resource/astroblog/content/buy_ssl.png)\n\n完成购买后请不要立即关闭页面，页面中的订单号在之后还需要用到。尔后，系统会发送”HuaweiCloud账户申请”邮件至用户邮箱，即你在华为云的注册邮箱。\n\n![HuaweiCloud账户申请](https://astrobear.top/resource/astroblog/content/request_account.png)\n\n点击邮件中的登录地址进入系统，并使用邮件提供的账号和初始密码进行登录。登入系统后请修改你的初始密码，然后请根据华为云中给你提供的订单号在该系统中查询你的订单。查询到你的订单以后，需要你补充一些信息，请如实填写。系统会要你填写公司信息，如果只是个人网站，那么公司名称直接填写你的名字即可，公司地址就填写你的住址。\n\n填写完成后会进入审核阶段，系统会给你发送一封邮件。\n\n![证书审核](https://astrobear.top/resource/astroblog/content/check.png)\n\n根据邮件的提示，需要在记录集中添加新的内容。请根据[前文](#1)所述方法，将邮件中的内容添加至新的记录集。填写方法如下图所示。\n\n![填写记录集](https://astrobear.top/resource/astroblog/content/modify_record.png)\n\n填写完成后，可以在本地电脑的终端里输入`nslookup -querytype=txt 你的域名`来测试记录集是否生效。\n\n![测试记录集](https://astrobear.top/resource/astroblog/content/test_record.png)\n\n一般来说，记录集生效后10分钟以内证书就会颁发了。\n\n![证书颁发](https://astrobear.top/resource/astroblog/content/issue.png)\n\n### SSL证书部署\n\n接下来我们要把SSL证书部署到我们的服务器上。\n\n在收到的“证书颁发”的邮件的底部有一条链接，点击这条链接，进入证书管理系统。登录系统，在左侧导航栏中点击“SSL证书”，再点击“预览”，再在右侧的“信息预览”中点击“下载最新证书“。\n\n![下载证书](https://astrobear.top/resource/astroblog/content/download_cert.png)\n\n在弹出的对话框内，选择证书格式为“PEM(适用于Nginx,SLB)”，输入你的订单密码。证书密码可以留空。\n\n![下载证书](https://astrobear.top/resource/astroblog/content/download_cert1.png)\n\n下载完成后，解压下载的压缩包，需要输入你的订单密码（如果你没有设置证书密码）。解压以后可以得到下图两个文件。\n\n![解压缩](https://astrobear.top/resource/astroblog/content/unzip_cert.png)\n\n接下来，打开你的终端，按顺序输入下列命令：\n\n```shell\nssh 你的公网IP #ssh登录，输入你的密码\ncd /usr/local/nginx #切换到nginx的安装目录\nmkdir ./cert #创建一个新文件夹cert用于存放你的证书\nexit #断开与服务器的连接\nscp 文件的路径/你的域名.key 你的服务器用户名@你的服务器IP地址:./cert #将.key文件上传到你的服务器的指定目录下\nscp 文件的路径/你的域名.crt 你的服务器用户名@你的服务器IP地址:./cert #将.crt文件上传到你的服务器的指定目录下\n```\n\n接下来我们需要修改nginx的配置文件。参考[前文](#2)所述方法打开nginx的配置文件。先将你之前插入的内容删除或者使用`#`注释掉，然后在最后一个大括号前插入以下内容：\n\n```nginx\nserver {\n         listen       443 ssl;\n         server_name  example.com; #你证书绑定的域名;\n\n        ssl_certificate      /usr/local/nginx/cert/你的域名.crt;\n        ssl_certificate_key  /usr/local/nginx/cert/你的域名.key;\n\n        ssl_session_cache    shared:SSL:1m;\n        ssl_session_timeout  5m;\n\n        ssl_ciphers  HIGH:!aNULL:!MD5;\n        ssl_prefer_server_ciphers  on;\n        \n        location / {\n            index index.htm index.html; #指定默认文件。\n\t    \t\t\troot html; #指定网站根目录。\n        }\n}\nserver { #将你的80端口重定向至433端口，即强制使用https访问\n  \t\t\tlisten 80;\n  \t\t\tserver_name; example.com; #你的域名\n\t\t\t\trewrite ^/(.*)$ https://example.com:443/$1 permanent;\n}\n```\n\n将文件保存以后重启nginx服务。\n\n重启以后你可能会遇到这样的问题：`**unknown directive “ssl” in /usr/local/nginx/conf/nginx.conf:121**`，这是因为你在安装nginx时，没有编译SSL模块。你可以在终端里按照下述步骤解决[^ 3]：\n\n```shell\ncd ../nginx-1.16.1 #进入到nginx的源码包的目录下\n./configure --with-http_ssl_module #带参数执行程序\nmake #编译\ncp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #备份旧的nginx\ncp ./objs/nginx /usr/local/nginx/sbin/ #然后将新的nginx的程序复制一份\ncd /usr/local/nginx/sbin/ #切换到sbin目录\n./nginx -s reload #重启nginx服务\n```\n\n如果重启成功的话，打开浏览器访问你的域名，这时候应该可以在链接旁边看到一个小锁了！\n\n[^1]:https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\n\n[^2]: https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\n[^ 3]: https://blog.csdn.net/qq_26369317/article/details/102863613\n\n","slug":"华为云+nginx服务器搭建总结","published":1,"updated":"2020-01-10T08:45:23.632Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck6ax1lgp000ej1p28m9ng16k","content":"<blockquote>\n<p>由于自己是去年七月配置好的服务器，有一些细节或者遇到的问题已经记不太清，故本文可能会有不完整的地方，遇到问题请善用搜索引擎，而且服务器的配置方法也不只有这一种。本文主要用作对自己操作步骤和方法的一个总结，以便于日后查阅。本文章将持续更新。</p>\n</blockquote>\n<h3 id=\"购买服务器\"><a href=\"#购买服务器\" class=\"headerlink\" title=\"购买服务器\"></a>购买服务器</h3><p>首先去<a href=\"https://www.huaweicloud.com/?locale=zh-cn\">华为云官网</a>注册一个账号。如果是学生，可以搜索“学生”，并进行学生认证。学生认证的步骤参见<a href=\"https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html\">学生认证流程</a>。进行身份验证后可以购买学生优惠套餐，云服务器价格只要99元/年，比阿里云和腾讯云的都要便宜一些。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hwcloud_discount.png\" alt=\"华为云学生优惠\"></p>\n<p>购买完成后，你可以在控制台看到自己现有的资源以及运行情况。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/console.png\" alt=\"控制台\"></p>\n<h3 id=\"配置安全组\"><a href=\"#配置安全组\" class=\"headerlink\" title=\"配置安全组\"></a>配置安全组</h3><blockquote>\n<p>安全组是一个逻辑上的分组，为具有相同安全保护需求并相互信任的云服务器提供访问策略。安全组创建后，用户可以在安全组中定义各种访问规则，当云服务器加入该安全组后，即受到这些访问规则的保护。</p>\n<p>系统会为每个用户默认创建一个默认安全组，默认安全组的规则是在出方向上的数据报文全部放行，入方向访问受限，安全组内的云服务器无需添加规则即可互相访问。默认安全组可以直接使用。</p>\n<p>安全组创建后，你可以在安全组中设置出方向、入方向规则，这些规则会对安全组内部的云服务器出入方向网络流量进行访问控制，当云服务器加入该安全组后，即受到这些访问规则的保护。<a href=\"https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\">^1</a></p>\n</blockquote>\n<p>在控制台点击“弹性云服务器ECS”，在这里你可看到你的服务器的公网IP，请记下这个IP地址。然后点击在列表中点击你的服务器的名称。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/security_groups.png\" alt=\"选择服务器\"></p>\n<p>进入云服务器管理页面后，点击“安全组”。再点击“Sys-default”可以看到默认安全组。然后下面给出的图片是我目前的安全组设置，仅供参考。选择“入/出方向方向规则”，再点击“添加规则“即可手动添加规则。一般来说，配置的都是入方向的安全组，并且源地址（访问服务器的设备的IP地址）都为“0.0.0.0/0”（所有IP地址）。</p>\n<p>通常需要配置如下几个功能：</p>\n<ul>\n<li>SSH远程连接Linux弹性云服务器（协议：SSH，端口：22）</li>\n<li>公网“ping”ECS弹性云服务器（协议：ICMP，端口：全部）</li>\n<li>弹性云服务器作Web服务器<ul>\n<li>协议：http，端口：80</li>\n<li>协议：https，端口：433</li>\n</ul>\n</li>\n</ul>\n<p>详细配置请参考<a href=\"https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html\">安全组配置示例</a>。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings.png\" alt=\"安全组设置\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings1.png\" alt=\"安全组设置\"></p>\n<p>配置完成后，可以打开电脑上的终端，用下面的语句测试一下：</p>\n<p><code>ping 你的公网IP</code></p>\n<p>出现类似下面的内容就代表成功了：</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ping_test.png\" alt=\"ping测试\"></p>\n<p>你可以按下<code>Ctrl+C</code>来结束<code>ping</code>这个进程。</p>\n<p>然后在终端里输入：</p>\n<p><code>ssh 你的公网IP</code></p>\n<p>如果你的安全组配置正确的话，会让你输入服务器的登录密码。输入密码（注意：密码是不会显示的）后回车，应该可以看到这样的输出：</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ssh_login.png\" alt=\"ssh登录\"></p>\n<p>这个时候，你的终端就已经连接上了服务器的系统了，你在终端里的一切操作都是作用在服务器上的。</p>\n<h3 id=\"在服务器上安装nginx\"><a href=\"#在服务器上安装nginx\" class=\"headerlink\" title=\"在服务器上安装nginx\"></a>在服务器上安装nginx</h3><p>首先请在终端使用ssh登录你的服务器，然后按照下面给出的顺序输入命令。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #安装编译工具及库文件</span><br><span class=\"line\">cd /usr/local/ #切换到目标安装文件夹</span><br><span class=\"line\">wget http://nginx.org/download/nginx-1.16.1.tar.gz #下载最新版本的Nginx</span><br><span class=\"line\">tar -zxvf nginx-1.16.1.tar.gz #解压文件</span><br><span class=\"line\">cd nginx-1.16.1 #进入解压的文件夹</span><br><span class=\"line\">./configure #执行程序</span><br><span class=\"line\">make #编译</span><br><span class=\"line\">make install #安装</span><br><span class=\"line\">cd /usr/local/nginx/sbin #进入Nginx安装目录</span><br><span class=\"line\">./nginx #运行Nginx</span><br></pre></td></tr></table></figure>\n\n<p>此时，安装应该已经完成了。打开浏览器，在地址栏中输入你的公网ip。如果看到下图所示内容，就代表安装成功了。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/nginx_install.png\" alt=\"nginx安装成功\"></p>\n<h3 id=\"创建属于你自己的域名\"><a href=\"#创建属于你自己的域名\" class=\"headerlink\" title=\"创建属于你自己的域名\"></a>创建属于你自己的域名</h3><p>在拥有了自己的服务器以后，就可以做很多事情了。但是现在你只能通过IP地址访问自己的服务器，看起来总是有点别扭。另外，如果你想要网站有一定的影响力的话，仅有IP地址会让人几乎找不到你的网站，而且也不符合国家法律规定。所以还是建议大家弄一个自己的域名。</p>\n<p>现在市面上的云服务器提供商也都提供域名注册的服务，直接在你的服务提供商的平台上面注册即可。下面我继续用华为云的平台演示。</p>\n<p>首先在华为云网站页面的导航栏的搜索框内搜索“域名”，打开第一个链接“域名注册服务”。也可以直接点击这里：<a href=\"https://www.huaweicloud.com/product/domain.html\">域名注册服务_华为云</a>。</p>\n<p>然后你可以在网页中选择你的域名，常见的如<code>.com</code>，<code>.cn</code>，<code>.net</code>等。这些域名会相对比较贵。作为学生党，我选择一个最便宜的域名<code>.top</code>，只需要9元/年。</p>\n<p>点击你想要的域名后，会跳转到一个新的页面。接下来再次选择你要的域名，并且在“查域名”的搜索框内输入你想要的域名，看看是否已经被占用，如果被占用了就换一个。若显示“域名可注册”，就点击“立即购买”。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_domain.png\" alt=\"域名购买\"></p>\n<p>购买完成后，你就拥有了自己域名了！</p>\n<h3 id=\"备案\"><a href=\"#备案\" class=\"headerlink\" title=\"备案\"></a>备案</h3><blockquote>\n<p>备案是中国大陆的一项法规，使用大陆节点服务器提供互联网信息服务的用户，需要在服务器提供商处提交备案申请。</p>\n<p>根据工信部《互联网信息服务管理办法》(国务院292号令)和工信部令第33号《非经营性互联网信息服务备案管理办法》规定，国家对经营性互联网信息服务实行许可制度，对非经营性互联网信息服务实行备案制度。未取得许可或者未履行备案手续的，不得从事互联网信息服务，否则属违法行为。通俗来讲，要开办网站必须先办理网站备案，备案成功并获取通信管理局下发的ICP备案号后才能开通访问。<a href=\"https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\">^2</a></p>\n</blockquote>\n<p>这一步不多说了，具体步骤比较繁琐，花费的时间也比较长，需要一两周。网站上有很清晰的<a href=\"https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html\">操作方法</a>，请自行查阅，根据步骤操作即可。需要注意一点的是，在审核过程中可能会接到服务提供商打来的电话，不要漏接。</p>\n<p>需要注意的是，上面的备案操作是在工信部备案的。完成了在工信部的备案以后还需要公安备案。具体<a href=\"http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c\">操作方法</a>也请自行查阅。</p>\n<h3 id=\"域名解析\"><a href=\"#域名解析\" class=\"headerlink\" title=\"域名解析\"></a>域名解析</h3><p>在完成一系列繁琐的备案流程以后，你的网站还不可以通过域名访问。只有把你的域名跟服务器的IP地址绑定在一起之后，并且在服务器上修改了配置文件之后才可以。</p>\n<p>首先打开管理控制台，在控制台中选择“域名注册”。然后在下面的页面中点击“解析”。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/domain.png\" alt=\"域名注册\"></p>\n<p>点击你的域名，显示如下页面。这里显示的是你域名的记录集，前两个记录集应该是预置设置，不可暂停服务。<span id=\"1\">你可以在这基础上添加自己的记录集。</span></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/record.png\" alt=\"记录集\"></p>\n<p>点击页面右上角红色按钮以添加记录集。添加记录集的配置如下图所示。下图中给出的例子是添加的“A”型记录集，也即通过<code>example.com</code>访问网站。若需要通过<code>www.example.com</code>访问网站，则需要为<code>example.com</code>的子域名添加“A”型记录集。具体配置参见：<a href=\"https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1\">配置网站解析_华为云</a>。点击“确定”，完成添加。你可以通过<code>ping 你的域名</code>来测试你添加的记录集是否生效了。</p>\n<p><img src=\"https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png\" alt=\"添加记录集\"></p>\n<h3 id=\"配置nginx\"><a href=\"#配置nginx\" class=\"headerlink\" title=\"配置nginx\"></a>配置nginx</h3><p><span id=\"2\">打开</span>你电脑上的终端，输入命令：<code>ssh 你的IP地址</code>，输入你的服务器的密码。</p>\n<p>进入你的nginx的安装目录：<code>cd /usr/local/nginx/</code>。</p>\n<p>使用vim打开nginx的配置文件：<code>vim ./conf/nginx.conf</code>。</p>\n<p>按<code>I</code>开始输入。</p>\n<p>在最后一个大括号前插入以下内容：</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">\t    <span class=\"attribute\">listen</span>   <span class=\"number\">80</span>; <span class=\"comment\">#监听端口设为 80</span></span><br><span class=\"line\">\t    <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#绑定您的域名</span></span><br><span class=\"line\">\t    <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#指定默认文件</span></span><br><span class=\"line\">\t    <span class=\"attribute\">root</span> html; <span class=\"comment\">#指定网站根目录</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>然后按<code>esc</code>退出编辑，再按<code>Shift+zz</code>保存。</p>\n<p>输入：<code>cd ./sbin</code>，切换文件夹。</p>\n<p>执行命令：<code>nginx -s relod</code>，重启nginx服务。</p>\n<p>这时候再尝试用浏览器访问你的域名，应该会显示之前出现过的“Welcome to nginx ”的页面了！</p>\n<h3 id=\"申请SSL证书\"><a href=\"#申请SSL证书\" class=\"headerlink\" title=\"申请SSL证书\"></a>申请SSL证书</h3><p>SSL证书可以在数据传输的过程中对其进行加密和隐藏，可以极大地提高数据传输的安全性。拥有SSL证书的网站的请求头都是<code>https</code>，并且在链接旁边会出现一把小锁。但是，SSL证书并不是所有网站都必须的，这视你的需要而定。比如，微信小程序的服务器就必须要有域名和SSL证书。另外，出于信息传输的安全性方面的考虑，有SSL证书还是显得更为妥当和专业一点。</p>\n<p>现在市面上各大云服务器提供商也都提供配套的SSL证书申请服务，一般都是提供企业级的证书，价格比较昂贵。但是同时网络上也有一些免费的SSL证书服务可以选择。下面还是以华为云的平台为例，简单说明一下如何申请SSL证书。</p>\n<p>首先在华为云页面的导航栏的搜索框内搜索“免费证书“，然后点击<a href=\"https://marketplace.huaweicloud.com/product/00301-315148-0--0\">亚洲诚信域名型DV单域名SSL证书–免费证书</a>，可以看到证书的价格是0.00元。点击“立即购买”。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_ssl.png\" alt=\"购买SSL证书\"></p>\n<p>完成购买后请不要立即关闭页面，页面中的订单号在之后还需要用到。尔后，系统会发送”HuaweiCloud账户申请”邮件至用户邮箱，即你在华为云的注册邮箱。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/request_account.png\" alt=\"HuaweiCloud账户申请\"></p>\n<p>点击邮件中的登录地址进入系统，并使用邮件提供的账号和初始密码进行登录。登入系统后请修改你的初始密码，然后请根据华为云中给你提供的订单号在该系统中查询你的订单。查询到你的订单以后，需要你补充一些信息，请如实填写。系统会要你填写公司信息，如果只是个人网站，那么公司名称直接填写你的名字即可，公司地址就填写你的住址。</p>\n<p>填写完成后会进入审核阶段，系统会给你发送一封邮件。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/check.png\" alt=\"证书审核\"></p>\n<p>根据邮件的提示，需要在记录集中添加新的内容。请根据<a href=\"#1\">前文</a>所述方法，将邮件中的内容添加至新的记录集。填写方法如下图所示。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/modify_record.png\" alt=\"填写记录集\"></p>\n<p>填写完成后，可以在本地电脑的终端里输入<code>nslookup -querytype=txt 你的域名</code>来测试记录集是否生效。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/test_record.png\" alt=\"测试记录集\"></p>\n<p>一般来说，记录集生效后10分钟以内证书就会颁发了。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/issue.png\" alt=\"证书颁发\"></p>\n<h3 id=\"SSL证书部署\"><a href=\"#SSL证书部署\" class=\"headerlink\" title=\"SSL证书部署\"></a>SSL证书部署</h3><p>接下来我们要把SSL证书部署到我们的服务器上。</p>\n<p>在收到的“证书颁发”的邮件的底部有一条链接，点击这条链接，进入证书管理系统。登录系统，在左侧导航栏中点击“SSL证书”，再点击“预览”，再在右侧的“信息预览”中点击“下载最新证书“。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert.png\" alt=\"下载证书\"></p>\n<p>在弹出的对话框内，选择证书格式为“PEM(适用于Nginx,SLB)”，输入你的订单密码。证书密码可以留空。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert1.png\" alt=\"下载证书\"></p>\n<p>下载完成后，解压下载的压缩包，需要输入你的订单密码（如果你没有设置证书密码）。解压以后可以得到下图两个文件。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/unzip_cert.png\" alt=\"解压缩\"></p>\n<p>接下来，打开你的终端，按顺序输入下列命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh 你的公网IP #ssh登录，输入你的密码</span><br><span class=\"line\">cd /usr/local/nginx #切换到nginx的安装目录</span><br><span class=\"line\">mkdir ./cert #创建一个新文件夹cert用于存放你的证书</span><br><span class=\"line\">exit #断开与服务器的连接</span><br><span class=\"line\">scp 文件的路径/你的域名.key 你的服务器用户名@你的服务器IP地址:./cert #将.key文件上传到你的服务器的指定目录下</span><br><span class=\"line\">scp 文件的路径/你的域名.crt 你的服务器用户名@你的服务器IP地址:./cert #将.crt文件上传到你的服务器的指定目录下</span><br></pre></td></tr></table></figure>\n\n<p>接下来我们需要修改nginx的配置文件。参考<a href=\"#2\">前文</a>所述方法打开nginx的配置文件。先将你之前插入的内容删除或者使用<code>#</code>注释掉，然后在最后一个大括号前插入以下内容：</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">         <span class=\"attribute\">listen</span>       <span class=\"number\">443</span> ssl;</span><br><span class=\"line\">         <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#你证书绑定的域名;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate</span>      /usr/local/nginx/cert/你的域名.crt;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate_key</span>  /usr/local/nginx/cert/你的域名.key;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_cache</span>    shared:SSL:<span class=\"number\">1m</span>;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_timeout</span>  <span class=\"number\">5m</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_ciphers</span>  HIGH:!aNULL:!MD5;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_prefer_server_ciphers</span>  <span class=\"literal\">on</span>;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"attribute\">location</span> / &#123;</span><br><span class=\"line\">            <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#指定默认文件。</span></span><br><span class=\"line\">\t    \t\t\t<span class=\"attribute\">root</span> html; <span class=\"comment\">#指定网站根目录。</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"section\">server</span> &#123; <span class=\"comment\">#将你的80端口重定向至433端口，即强制使用https访问</span></span><br><span class=\"line\">  \t\t\t<span class=\"attribute\">listen</span> <span class=\"number\">80</span>;</span><br><span class=\"line\">  \t\t\tserver_name; example.com; #你的域名</span><br><span class=\"line\">\t\t\t\t<span class=\"attribute\">rewrite</span><span class=\"regexp\"> ^/(.*)$</span> https://example.com:443/<span class=\"variable\">$1</span> <span class=\"literal\">permanent</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>将文件保存以后重启nginx服务。</p>\n<p>重启以后你可能会遇到这样的问题：<code>**unknown directive “ssl” in /usr/local/nginx/conf/nginx.conf:121**</code>，这是因为你在安装nginx时，没有编译SSL模块。你可以在终端里按照下述步骤解决<a href=\"https://blog.csdn.net/qq_26369317/article/details/102863613\">^ 3</a>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ../nginx-1.16.1 #进入到nginx的源码包的目录下</span><br><span class=\"line\">./configure --with-http_ssl_module #带参数执行程序</span><br><span class=\"line\">make #编译</span><br><span class=\"line\">cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #备份旧的nginx</span><br><span class=\"line\">cp ./objs/nginx /usr/local/nginx/sbin/ #然后将新的nginx的程序复制一份</span><br><span class=\"line\">cd /usr/local/nginx/sbin/ #切换到sbin目录</span><br><span class=\"line\">./nginx -s reload #重启nginx服务</span><br></pre></td></tr></table></figure>\n\n<p>如果重启成功的话，打开浏览器访问你的域名，这时候应该可以在链接旁边看到一个小锁了！</p>\n","site":{"data":{}},"more":"<blockquote>\n<p>由于自己是去年七月配置好的服务器，有一些细节或者遇到的问题已经记不太清，故本文可能会有不完整的地方，遇到问题请善用搜索引擎，而且服务器的配置方法也不只有这一种。本文主要用作对自己操作步骤和方法的一个总结，以便于日后查阅。本文章将持续更新。</p>\n</blockquote>\n<h3 id=\"购买服务器\"><a href=\"#购买服务器\" class=\"headerlink\" title=\"购买服务器\"></a>购买服务器</h3><p>首先去<a href=\"https://www.huaweicloud.com/?locale=zh-cn\">华为云官网</a>注册一个账号。如果是学生，可以搜索“学生”，并进行学生认证。学生认证的步骤参见<a href=\"https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html\">学生认证流程</a>。进行身份验证后可以购买学生优惠套餐，云服务器价格只要99元/年，比阿里云和腾讯云的都要便宜一些。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hwcloud_discount.png\" alt=\"华为云学生优惠\"></p>\n<p>购买完成后，你可以在控制台看到自己现有的资源以及运行情况。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/console.png\" alt=\"控制台\"></p>\n<h3 id=\"配置安全组\"><a href=\"#配置安全组\" class=\"headerlink\" title=\"配置安全组\"></a>配置安全组</h3><blockquote>\n<p>安全组是一个逻辑上的分组，为具有相同安全保护需求并相互信任的云服务器提供访问策略。安全组创建后，用户可以在安全组中定义各种访问规则，当云服务器加入该安全组后，即受到这些访问规则的保护。</p>\n<p>系统会为每个用户默认创建一个默认安全组，默认安全组的规则是在出方向上的数据报文全部放行，入方向访问受限，安全组内的云服务器无需添加规则即可互相访问。默认安全组可以直接使用。</p>\n<p>安全组创建后，你可以在安全组中设置出方向、入方向规则，这些规则会对安全组内部的云服务器出入方向网络流量进行访问控制，当云服务器加入该安全组后，即受到这些访问规则的保护。<a href=\"https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\">^1</a></p>\n</blockquote>\n<p>在控制台点击“弹性云服务器ECS”，在这里你可看到你的服务器的公网IP，请记下这个IP地址。然后点击在列表中点击你的服务器的名称。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/security_groups.png\" alt=\"选择服务器\"></p>\n<p>进入云服务器管理页面后，点击“安全组”。再点击“Sys-default”可以看到默认安全组。然后下面给出的图片是我目前的安全组设置，仅供参考。选择“入/出方向方向规则”，再点击“添加规则“即可手动添加规则。一般来说，配置的都是入方向的安全组，并且源地址（访问服务器的设备的IP地址）都为“0.0.0.0/0”（所有IP地址）。</p>\n<p>通常需要配置如下几个功能：</p>\n<ul>\n<li>SSH远程连接Linux弹性云服务器（协议：SSH，端口：22）</li>\n<li>公网“ping”ECS弹性云服务器（协议：ICMP，端口：全部）</li>\n<li>弹性云服务器作Web服务器<ul>\n<li>协议：http，端口：80</li>\n<li>协议：https，端口：433</li>\n</ul>\n</li>\n</ul>\n<p>详细配置请参考<a href=\"https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html\">安全组配置示例</a>。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings.png\" alt=\"安全组设置\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings1.png\" alt=\"安全组设置\"></p>\n<p>配置完成后，可以打开电脑上的终端，用下面的语句测试一下：</p>\n<p><code>ping 你的公网IP</code></p>\n<p>出现类似下面的内容就代表成功了：</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ping_test.png\" alt=\"ping测试\"></p>\n<p>你可以按下<code>Ctrl+C</code>来结束<code>ping</code>这个进程。</p>\n<p>然后在终端里输入：</p>\n<p><code>ssh 你的公网IP</code></p>\n<p>如果你的安全组配置正确的话，会让你输入服务器的登录密码。输入密码（注意：密码是不会显示的）后回车，应该可以看到这样的输出：</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ssh_login.png\" alt=\"ssh登录\"></p>\n<p>这个时候，你的终端就已经连接上了服务器的系统了，你在终端里的一切操作都是作用在服务器上的。</p>\n<h3 id=\"在服务器上安装nginx\"><a href=\"#在服务器上安装nginx\" class=\"headerlink\" title=\"在服务器上安装nginx\"></a>在服务器上安装nginx</h3><p>首先请在终端使用ssh登录你的服务器，然后按照下面给出的顺序输入命令。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #安装编译工具及库文件</span><br><span class=\"line\">cd /usr/local/ #切换到目标安装文件夹</span><br><span class=\"line\">wget http://nginx.org/download/nginx-1.16.1.tar.gz #下载最新版本的Nginx</span><br><span class=\"line\">tar -zxvf nginx-1.16.1.tar.gz #解压文件</span><br><span class=\"line\">cd nginx-1.16.1 #进入解压的文件夹</span><br><span class=\"line\">./configure #执行程序</span><br><span class=\"line\">make #编译</span><br><span class=\"line\">make install #安装</span><br><span class=\"line\">cd /usr/local/nginx/sbin #进入Nginx安装目录</span><br><span class=\"line\">./nginx #运行Nginx</span><br></pre></td></tr></table></figure>\n\n<p>此时，安装应该已经完成了。打开浏览器，在地址栏中输入你的公网ip。如果看到下图所示内容，就代表安装成功了。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/nginx_install.png\" alt=\"nginx安装成功\"></p>\n<h3 id=\"创建属于你自己的域名\"><a href=\"#创建属于你自己的域名\" class=\"headerlink\" title=\"创建属于你自己的域名\"></a>创建属于你自己的域名</h3><p>在拥有了自己的服务器以后，就可以做很多事情了。但是现在你只能通过IP地址访问自己的服务器，看起来总是有点别扭。另外，如果你想要网站有一定的影响力的话，仅有IP地址会让人几乎找不到你的网站，而且也不符合国家法律规定。所以还是建议大家弄一个自己的域名。</p>\n<p>现在市面上的云服务器提供商也都提供域名注册的服务，直接在你的服务提供商的平台上面注册即可。下面我继续用华为云的平台演示。</p>\n<p>首先在华为云网站页面的导航栏的搜索框内搜索“域名”，打开第一个链接“域名注册服务”。也可以直接点击这里：<a href=\"https://www.huaweicloud.com/product/domain.html\">域名注册服务_华为云</a>。</p>\n<p>然后你可以在网页中选择你的域名，常见的如<code>.com</code>，<code>.cn</code>，<code>.net</code>等。这些域名会相对比较贵。作为学生党，我选择一个最便宜的域名<code>.top</code>，只需要9元/年。</p>\n<p>点击你想要的域名后，会跳转到一个新的页面。接下来再次选择你要的域名，并且在“查域名”的搜索框内输入你想要的域名，看看是否已经被占用，如果被占用了就换一个。若显示“域名可注册”，就点击“立即购买”。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_domain.png\" alt=\"域名购买\"></p>\n<p>购买完成后，你就拥有了自己域名了！</p>\n<h3 id=\"备案\"><a href=\"#备案\" class=\"headerlink\" title=\"备案\"></a>备案</h3><blockquote>\n<p>备案是中国大陆的一项法规，使用大陆节点服务器提供互联网信息服务的用户，需要在服务器提供商处提交备案申请。</p>\n<p>根据工信部《互联网信息服务管理办法》(国务院292号令)和工信部令第33号《非经营性互联网信息服务备案管理办法》规定，国家对经营性互联网信息服务实行许可制度，对非经营性互联网信息服务实行备案制度。未取得许可或者未履行备案手续的，不得从事互联网信息服务，否则属违法行为。通俗来讲，要开办网站必须先办理网站备案，备案成功并获取通信管理局下发的ICP备案号后才能开通访问。<a href=\"https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\">^2</a></p>\n</blockquote>\n<p>这一步不多说了，具体步骤比较繁琐，花费的时间也比较长，需要一两周。网站上有很清晰的<a href=\"https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html\">操作方法</a>，请自行查阅，根据步骤操作即可。需要注意一点的是，在审核过程中可能会接到服务提供商打来的电话，不要漏接。</p>\n<p>需要注意的是，上面的备案操作是在工信部备案的。完成了在工信部的备案以后还需要公安备案。具体<a href=\"http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c\">操作方法</a>也请自行查阅。</p>\n<h3 id=\"域名解析\"><a href=\"#域名解析\" class=\"headerlink\" title=\"域名解析\"></a>域名解析</h3><p>在完成一系列繁琐的备案流程以后，你的网站还不可以通过域名访问。只有把你的域名跟服务器的IP地址绑定在一起之后，并且在服务器上修改了配置文件之后才可以。</p>\n<p>首先打开管理控制台，在控制台中选择“域名注册”。然后在下面的页面中点击“解析”。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/domain.png\" alt=\"域名注册\"></p>\n<p>点击你的域名，显示如下页面。这里显示的是你域名的记录集，前两个记录集应该是预置设置，不可暂停服务。<span id=\"1\">你可以在这基础上添加自己的记录集。</span></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/record.png\" alt=\"记录集\"></p>\n<p>点击页面右上角红色按钮以添加记录集。添加记录集的配置如下图所示。下图中给出的例子是添加的“A”型记录集，也即通过<code>example.com</code>访问网站。若需要通过<code>www.example.com</code>访问网站，则需要为<code>example.com</code>的子域名添加“A”型记录集。具体配置参见：<a href=\"https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1\">配置网站解析_华为云</a>。点击“确定”，完成添加。你可以通过<code>ping 你的域名</code>来测试你添加的记录集是否生效了。</p>\n<p><img src=\"https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png\" alt=\"添加记录集\"></p>\n<h3 id=\"配置nginx\"><a href=\"#配置nginx\" class=\"headerlink\" title=\"配置nginx\"></a>配置nginx</h3><p><span id=\"2\">打开</span>你电脑上的终端，输入命令：<code>ssh 你的IP地址</code>，输入你的服务器的密码。</p>\n<p>进入你的nginx的安装目录：<code>cd /usr/local/nginx/</code>。</p>\n<p>使用vim打开nginx的配置文件：<code>vim ./conf/nginx.conf</code>。</p>\n<p>按<code>I</code>开始输入。</p>\n<p>在最后一个大括号前插入以下内容：</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">\t    <span class=\"attribute\">listen</span>   <span class=\"number\">80</span>; <span class=\"comment\">#监听端口设为 80</span></span><br><span class=\"line\">\t    <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#绑定您的域名</span></span><br><span class=\"line\">\t    <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#指定默认文件</span></span><br><span class=\"line\">\t    <span class=\"attribute\">root</span> html; <span class=\"comment\">#指定网站根目录</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>然后按<code>esc</code>退出编辑，再按<code>Shift+zz</code>保存。</p>\n<p>输入：<code>cd ./sbin</code>，切换文件夹。</p>\n<p>执行命令：<code>nginx -s relod</code>，重启nginx服务。</p>\n<p>这时候再尝试用浏览器访问你的域名，应该会显示之前出现过的“Welcome to nginx ”的页面了！</p>\n<h3 id=\"申请SSL证书\"><a href=\"#申请SSL证书\" class=\"headerlink\" title=\"申请SSL证书\"></a>申请SSL证书</h3><p>SSL证书可以在数据传输的过程中对其进行加密和隐藏，可以极大地提高数据传输的安全性。拥有SSL证书的网站的请求头都是<code>https</code>，并且在链接旁边会出现一把小锁。但是，SSL证书并不是所有网站都必须的，这视你的需要而定。比如，微信小程序的服务器就必须要有域名和SSL证书。另外，出于信息传输的安全性方面的考虑，有SSL证书还是显得更为妥当和专业一点。</p>\n<p>现在市面上各大云服务器提供商也都提供配套的SSL证书申请服务，一般都是提供企业级的证书，价格比较昂贵。但是同时网络上也有一些免费的SSL证书服务可以选择。下面还是以华为云的平台为例，简单说明一下如何申请SSL证书。</p>\n<p>首先在华为云页面的导航栏的搜索框内搜索“免费证书“，然后点击<a href=\"https://marketplace.huaweicloud.com/product/00301-315148-0--0\">亚洲诚信域名型DV单域名SSL证书–免费证书</a>，可以看到证书的价格是0.00元。点击“立即购买”。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_ssl.png\" alt=\"购买SSL证书\"></p>\n<p>完成购买后请不要立即关闭页面，页面中的订单号在之后还需要用到。尔后，系统会发送”HuaweiCloud账户申请”邮件至用户邮箱，即你在华为云的注册邮箱。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/request_account.png\" alt=\"HuaweiCloud账户申请\"></p>\n<p>点击邮件中的登录地址进入系统，并使用邮件提供的账号和初始密码进行登录。登入系统后请修改你的初始密码，然后请根据华为云中给你提供的订单号在该系统中查询你的订单。查询到你的订单以后，需要你补充一些信息，请如实填写。系统会要你填写公司信息，如果只是个人网站，那么公司名称直接填写你的名字即可，公司地址就填写你的住址。</p>\n<p>填写完成后会进入审核阶段，系统会给你发送一封邮件。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/check.png\" alt=\"证书审核\"></p>\n<p>根据邮件的提示，需要在记录集中添加新的内容。请根据<a href=\"#1\">前文</a>所述方法，将邮件中的内容添加至新的记录集。填写方法如下图所示。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/modify_record.png\" alt=\"填写记录集\"></p>\n<p>填写完成后，可以在本地电脑的终端里输入<code>nslookup -querytype=txt 你的域名</code>来测试记录集是否生效。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/test_record.png\" alt=\"测试记录集\"></p>\n<p>一般来说，记录集生效后10分钟以内证书就会颁发了。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/issue.png\" alt=\"证书颁发\"></p>\n<h3 id=\"SSL证书部署\"><a href=\"#SSL证书部署\" class=\"headerlink\" title=\"SSL证书部署\"></a>SSL证书部署</h3><p>接下来我们要把SSL证书部署到我们的服务器上。</p>\n<p>在收到的“证书颁发”的邮件的底部有一条链接，点击这条链接，进入证书管理系统。登录系统，在左侧导航栏中点击“SSL证书”，再点击“预览”，再在右侧的“信息预览”中点击“下载最新证书“。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert.png\" alt=\"下载证书\"></p>\n<p>在弹出的对话框内，选择证书格式为“PEM(适用于Nginx,SLB)”，输入你的订单密码。证书密码可以留空。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert1.png\" alt=\"下载证书\"></p>\n<p>下载完成后，解压下载的压缩包，需要输入你的订单密码（如果你没有设置证书密码）。解压以后可以得到下图两个文件。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/unzip_cert.png\" alt=\"解压缩\"></p>\n<p>接下来，打开你的终端，按顺序输入下列命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh 你的公网IP #ssh登录，输入你的密码</span><br><span class=\"line\">cd /usr/local/nginx #切换到nginx的安装目录</span><br><span class=\"line\">mkdir ./cert #创建一个新文件夹cert用于存放你的证书</span><br><span class=\"line\">exit #断开与服务器的连接</span><br><span class=\"line\">scp 文件的路径/你的域名.key 你的服务器用户名@你的服务器IP地址:./cert #将.key文件上传到你的服务器的指定目录下</span><br><span class=\"line\">scp 文件的路径/你的域名.crt 你的服务器用户名@你的服务器IP地址:./cert #将.crt文件上传到你的服务器的指定目录下</span><br></pre></td></tr></table></figure>\n\n<p>接下来我们需要修改nginx的配置文件。参考<a href=\"#2\">前文</a>所述方法打开nginx的配置文件。先将你之前插入的内容删除或者使用<code>#</code>注释掉，然后在最后一个大括号前插入以下内容：</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">         <span class=\"attribute\">listen</span>       <span class=\"number\">443</span> ssl;</span><br><span class=\"line\">         <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#你证书绑定的域名;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate</span>      /usr/local/nginx/cert/你的域名.crt;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate_key</span>  /usr/local/nginx/cert/你的域名.key;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_cache</span>    shared:SSL:<span class=\"number\">1m</span>;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_timeout</span>  <span class=\"number\">5m</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_ciphers</span>  HIGH:!aNULL:!MD5;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_prefer_server_ciphers</span>  <span class=\"literal\">on</span>;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"attribute\">location</span> / &#123;</span><br><span class=\"line\">            <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#指定默认文件。</span></span><br><span class=\"line\">\t    \t\t\t<span class=\"attribute\">root</span> html; <span class=\"comment\">#指定网站根目录。</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"section\">server</span> &#123; <span class=\"comment\">#将你的80端口重定向至433端口，即强制使用https访问</span></span><br><span class=\"line\">  \t\t\t<span class=\"attribute\">listen</span> <span class=\"number\">80</span>;</span><br><span class=\"line\">  \t\t\tserver_name; example.com; #你的域名</span><br><span class=\"line\">\t\t\t\t<span class=\"attribute\">rewrite</span><span class=\"regexp\"> ^/(.*)$</span> https://example.com:443/<span class=\"variable\">$1</span> <span class=\"literal\">permanent</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>将文件保存以后重启nginx服务。</p>\n<p>重启以后你可能会遇到这样的问题：<code>**unknown directive “ssl” in /usr/local/nginx/conf/nginx.conf:121**</code>，这是因为你在安装nginx时，没有编译SSL模块。你可以在终端里按照下述步骤解决<a href=\"https://blog.csdn.net/qq_26369317/article/details/102863613\">^ 3</a>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ../nginx-1.16.1 #进入到nginx的源码包的目录下</span><br><span class=\"line\">./configure --with-http_ssl_module #带参数执行程序</span><br><span class=\"line\">make #编译</span><br><span class=\"line\">cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #备份旧的nginx</span><br><span class=\"line\">cp ./objs/nginx /usr/local/nginx/sbin/ #然后将新的nginx的程序复制一份</span><br><span class=\"line\">cd /usr/local/nginx/sbin/ #切换到sbin目录</span><br><span class=\"line\">./nginx -s reload #重启nginx服务</span><br></pre></td></tr></table></figure>\n\n<p>如果重启成功的话，打开浏览器访问你的域名，这时候应该可以在链接旁边看到一个小锁了！</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ck6ax1lfy0000j1p2e2ht8nn6","category_id":"ck4zczyf50004vq39a3nk34a3","_id":"ck6ax1lgk000aj1p24du0d7rk"},{"post_id":"ck6ax1lgj0009j1p22bs352jd","category_id":"ck6ax1lgi0007j1p20c54bs9g","_id":"ck6ax1lgr000hj1p2a8c9888a"},{"post_id":"ck6ax1lg50001j1p27ayd7j78","category_id":"ck6ax1lgi0007j1p20c54bs9g","_id":"ck6ax1lgr000ij1p24advef9p"},{"post_id":"ck6ax1lgl000bj1p271y3b93u","category_id":"ck6ax1lgi0007j1p20c54bs9g","_id":"ck6ax1lgs000kj1p2b4qc79mx"},{"post_id":"ck6ax1lgp000ej1p28m9ng16k","category_id":"ck6ax1lgi0007j1p20c54bs9g","_id":"ck6ax1lgs000mj1p22x3t61vs"},{"post_id":"ck6ax1lg80003j1p26y1j8hbi","category_id":"ck6ax1lgi0007j1p20c54bs9g","_id":"ck6ax1lgt000pj1p2egzc5x48"},{"post_id":"ck6ax1lgh0006j1p22s0p8u3s","category_id":"ck6ax1lgi0007j1p20c54bs9g","_id":"ck6ax1lgt000rj1p2bzro8hr2"},{"post_id":"ck4zczyf20002vq391oi1h22d","category_id":"ck4zczyf50004vq39a3nk34a3","_id":"ck6ax1lh2001kj1p2aawz08o6"}],"PostTag":[{"post_id":"ck4zczyf20002vq391oi1h22d","tag_id":"ck4zczyfa0009vq399q1t8lt4","_id":"ck4zczyff000ivq39bk6f16lr"},{"post_id":"ck4zczyf20002vq391oi1h22d","tag_id":"ck4zczyfb000dvq3911dn1jvf","_id":"ck4zczyff000jvq39bsfbbih9"},{"post_id":"ck4zczyf20002vq391oi1h22d","tag_id":"ck4zczyf70005vq3962g4f78i","_id":"ck4zczyff000kvq396bz728uv"},{"post_id":"ck4zczyf20002vq391oi1h22d","tag_id":"ck4zczyf80007vq39hd07brgp","_id":"ck4zczyff000lvq39das1hc8r"},{"post_id":"ck4zczyf20002vq391oi1h22d","tag_id":"ck4zczyf40003vq39gmca1ghi","_id":"ck4zczyff000mvq39aicm1mv5"},{"post_id":"ck6ax1lfy0000j1p2e2ht8nn6","tag_id":"ck4zczyf40003vq39gmca1ghi","_id":"ck6ax1lg80002j1p27306dy7m"},{"post_id":"ck6ax1lfy0000j1p2e2ht8nn6","tag_id":"ck4zczyf70005vq3962g4f78i","_id":"ck6ax1lgh0005j1p29ts9avjs"},{"post_id":"ck6ax1lfy0000j1p2e2ht8nn6","tag_id":"ck4zczyf80007vq39hd07brgp","_id":"ck6ax1lgj0008j1p23pe08xxk"},{"post_id":"ck6ax1lg50001j1p27ayd7j78","tag_id":"ck6ax1lg90004j1p2dvieauok","_id":"ck6ax1lgs000lj1p25ysqfi4y"},{"post_id":"ck6ax1lg50001j1p27ayd7j78","tag_id":"ck6ax1lgn000cj1p2ggygbggr","_id":"ck6ax1lgs000nj1p27xas7x9g"},{"post_id":"ck6ax1lg50001j1p27ayd7j78","tag_id":"ck6ax1lgr000gj1p2avrb8jb1","_id":"ck6ax1lgt000qj1p264lna9df"},{"post_id":"ck6ax1lg80003j1p26y1j8hbi","tag_id":"ck6ax1lgs000jj1p27pu419aw","_id":"ck6ax1lgu000uj1p2ftl75k3m"},{"post_id":"ck6ax1lg80003j1p26y1j8hbi","tag_id":"ck6ax1lgn000cj1p2ggygbggr","_id":"ck6ax1lgu000vj1p2bhg89zhg"},{"post_id":"ck6ax1lg80003j1p26y1j8hbi","tag_id":"ck6ax1lgr000gj1p2avrb8jb1","_id":"ck6ax1lgv000xj1p2ftn614w1"},{"post_id":"ck6ax1lgh0006j1p22s0p8u3s","tag_id":"ck6ax1lgs000jj1p27pu419aw","_id":"ck6ax1lgw0010j1p2h4by2pdv"},{"post_id":"ck6ax1lgh0006j1p22s0p8u3s","tag_id":"ck6ax1lgn000cj1p2ggygbggr","_id":"ck6ax1lgw0011j1p23k0obiha"},{"post_id":"ck6ax1lgh0006j1p22s0p8u3s","tag_id":"ck6ax1lgr000gj1p2avrb8jb1","_id":"ck6ax1lgw0013j1p2ge2w0yyy"},{"post_id":"ck6ax1lgj0009j1p22bs352jd","tag_id":"ck6ax1lgr000gj1p2avrb8jb1","_id":"ck6ax1lgx0015j1p2gm7y07fv"},{"post_id":"ck6ax1lgj0009j1p22bs352jd","tag_id":"ck6ax1lgw0012j1p27s2r4cey","_id":"ck6ax1lgx0016j1p27kr6dpah"},{"post_id":"ck6ax1lgl000bj1p271y3b93u","tag_id":"ck6ax1lgs000jj1p27pu419aw","_id":"ck6ax1lgy001aj1p2faf22g50"},{"post_id":"ck6ax1lgl000bj1p271y3b93u","tag_id":"ck6ax1lgn000cj1p2ggygbggr","_id":"ck6ax1lgy001bj1p20ngz8umg"},{"post_id":"ck6ax1lgl000bj1p271y3b93u","tag_id":"ck6ax1lgr000gj1p2avrb8jb1","_id":"ck6ax1lgz001dj1p2aoyr9haq"},{"post_id":"ck6ax1lgp000ej1p28m9ng16k","tag_id":"ck6ax1lgy0019j1p2dogrhp9r","_id":"ck6ax1lh0001gj1p26a42754l"},{"post_id":"ck6ax1lgp000ej1p28m9ng16k","tag_id":"ck6ax1lgy001cj1p2h70dcs6d","_id":"ck6ax1lh0001hj1p2du941eny"},{"post_id":"ck6ax1lgp000ej1p28m9ng16k","tag_id":"ck6ax1lgz001ej1p26t4x7mfc","_id":"ck6ax1lh0001ij1p25h9n4wot"},{"post_id":"ck6ax1lgp000ej1p28m9ng16k","tag_id":"ck6ax1lgz001fj1p2b2x57i5g","_id":"ck6ax1lh0001jj1p22ikf5j4s"}],"Tag":[{"name":"Astrobear","_id":"ck4zczyf40003vq39gmca1ghi"},{"name":"Life","_id":"ck4zczyf70005vq3962g4f78i"},{"name":"Others","_id":"ck4zczyf80007vq39hd07brgp"},{"name":"Photos","_id":"ck4zczyfa0009vq399q1t8lt4"},{"name":"Astrophotography","_id":"ck4zczyfb000dvq3911dn1jvf"},{"name":"AirSim","_id":"ck6ax1lg90004j1p2dvieauok"},{"name":"Research","_id":"ck6ax1lgn000cj1p2ggygbggr"},{"name":"Python","_id":"ck6ax1lgr000gj1p2avrb8jb1"},{"name":"RL","_id":"ck6ax1lgs000jj1p27pu419aw"},{"name":"Programming Language","_id":"ck6ax1lgw0012j1p27s2r4cey"},{"name":"Nginx","_id":"ck6ax1lgy0019j1p2dogrhp9r"},{"name":"Internet server","_id":"ck6ax1lgy001cj1p2h70dcs6d"},{"name":"Network Technology","_id":"ck6ax1lgz001ej1p26t4x7mfc"},{"name":"Experience","_id":"ck6ax1lgz001fj1p2b2x57i5g"}]}}