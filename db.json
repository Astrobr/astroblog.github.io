{"meta":{"version":1,"warehouse":"3.0.1"},"models":{"Asset":[{"_id":"themes/icarus/source/css/back-to-top.css","path":"css/back-to-top.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/progressbar.css","path":"css/progressbar.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/insight.css","path":"css/insight.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/search.css","path":"css/search.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/animation.js","path":"js/animation.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/insight.js","path":"js/insight.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/gallery.js","path":"js/gallery.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/back-to-top.js","path":"js/back-to-top.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/favicon.png","path":"images/favicon.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/logo.png","path":"images/logo.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/og_image.png","path":"images/og_image.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/thumbnail.svg","path":"images/thumbnail.svg","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/alipay.JPG","path":"images/alipay.JPG","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/avatar.jpg","path":"images/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/wechatpay.JPG","path":"images/wechatpay.JPG","modified":0,"renderable":1}],"Cache":[{"_id":"themes/icarus/.DS_Store","hash":"61a9b03ed2c71583e45a0c14fe11ec055a46b717","modified":1578128474241},{"_id":"themes/icarus/README.md","hash":"c351bc76d3b4a138989c50f57bd3c52fa95eaa56","modified":1578039467871},{"_id":"themes/icarus/LICENSE","hash":"41f72cd544612bc4589c924c776422b800a4eff7","modified":1577937081622},{"_id":"themes/icarus/_config.yml","hash":"86571ff21f7b02cfabea9a130c860a7c3469fcc4","modified":1578073534991},{"_id":"themes/icarus/package.json","hash":"4f362aa241ad5d5d9422cc8f1e825edfcd8cbb1a","modified":1577937081648},{"_id":"source/.DS_Store","hash":"637de53a8294d668770de6d972389f1d7d47ba6a","modified":1581000770337},{"_id":"themes/icarus/includes/.DS_Store","hash":"e6690115edc4626c57e09b6a6541c7af78c7b044","modified":1578069394351},{"_id":"themes/icarus/languages/en.yml","hash":"66d873bbe04894610c2d28b0d03e10143997b51b","modified":1577937081628},{"_id":"themes/icarus/languages/es.yml","hash":"2e59e579d393c881dcbb885516d93eeaf469cec2","modified":1577937081628},{"_id":"themes/icarus/languages/id.yml","hash":"92d2d19a62a17b6e99f82a014309bbf6c13c9ae8","modified":1577937081628},{"_id":"themes/icarus/languages/ja.yml","hash":"6eed7771de2353d71b720c6e605cceb3f230b12e","modified":1577937081628},{"_id":"themes/icarus/languages/fr.yml","hash":"0017f93a5d491a9c0e55911cdc35316762c5a94e","modified":1577937081628},{"_id":"themes/icarus/languages/ko.yml","hash":"e7ac736b604429adedd67f3ca6043201eff7b23b","modified":1577937081629},{"_id":"themes/icarus/languages/pt-BR.yml","hash":"bcf5bc81ca855d26bbc3b3bfabc7d84429e74b85","modified":1577937081629},{"_id":"themes/icarus/languages/pl.yml","hash":"43f5447c38c9be2e1f5ce6181a0f97eeb437b059","modified":1577937081629},{"_id":"themes/icarus/languages/ru.yml","hash":"ba8b4f7d77eb1d1e28aa1f9107bd0bbbdc4cba99","modified":1577937081629},{"_id":"themes/icarus/languages/tr.yml","hash":"eff1c0b3d5c4b328f6dd74a195ff378c898f4d29","modified":1577937081629},{"_id":"themes/icarus/languages/vn.yml","hash":"6d9f4fabca711a6cb0a0efd72aa75c3641beb4a6","modified":1577937081629},{"_id":"themes/icarus/languages/zh-CN.yml","hash":"804f6a1edee49bb6a5ecb8e9d14d3e93eaca37c0","modified":1577937081629},{"_id":"themes/icarus/languages/zh-TW.yml","hash":"6ff978a0c4c11e996925e1a912a1d805f4680a6c","modified":1577937081630},{"_id":"themes/icarus/layout/.DS_Store","hash":"a35945b9f253a9a05d95202ea4e9343f69531ce3","modified":1578061687314},{"_id":"themes/icarus/layout/archive.ejs","hash":"2527527eaf3e757ab476325f691d2e2e0ff9c2d5","modified":1577937081630},{"_id":"themes/icarus/layout/categories.ejs","hash":"29d304f2b95a04fbc5e7529f9bdce9648e3545ef","modified":1577937081630},{"_id":"themes/icarus/layout/category.ejs","hash":"58aa84f75193b978b2072f29dbb84ed8279574b9","modified":1577937081630},{"_id":"themes/icarus/layout/index.ejs","hash":"8ab440868f721bb7256ab9f2be96996850b0cf44","modified":1577937081637},{"_id":"themes/icarus/layout/layout.ejs","hash":"c2b47692e9db24db485265ca7d10f9ddbe10e49c","modified":1578048397463},{"_id":"themes/icarus/layout/page.ejs","hash":"ebf120d46074f67ea25a231d2f7a64fd1e751904","modified":1577937081637},{"_id":"themes/icarus/layout/post.ejs","hash":"ebf120d46074f67ea25a231d2f7a64fd1e751904","modified":1577937081641},{"_id":"themes/icarus/layout/tags.ejs","hash":"0c527c6b72386f11c18e8aa5249be8c601e69906","modified":1577937081644},{"_id":"themes/icarus/layout/tag.ejs","hash":"45eb077f2ac86f5c8090cb1a2361eed56a368e95","modified":1577937081644},{"_id":"themes/icarus/scripts/.DS_Store","hash":"a56011e9cbd55e51cd3897d1b33636876bb4225d","modified":1578048397464},{"_id":"themes/icarus/scripts/index.js","hash":"40839db58041e31eb06d428a91869b0789277e7e","modified":1577937081648},{"_id":"themes/icarus/source/.DS_Store","hash":"f5abc8eba3cba8f51cb421bf95d081510e193f34","modified":1578128480463},{"_id":"source/_drafts/template.md","hash":"b738a1718c9a344de4cef83e3289208d55ed6d64","modified":1578069394349},{"_id":"source/_drafts/.DS_Store","hash":"090feb3c86cfe00ec59c67c50c2ddc02cd2251c5","modified":1578464882985},{"_id":"source/_posts/About_2019-1-3.md","hash":"154ce21a19d1aac0bf4e2ff4a6b99c0e405b7cf7","modified":1578112205263},{"_id":"source/_posts/.DS_Store","hash":"a7eddd5b0051407c6b8d52187686a7d3a9555b9d","modified":1580909428030},{"_id":"source/_posts/Gallery.md","hash":"5adb43f4969c824078267c3af8523b1fefec4f9b","modified":1578301165714},{"_id":"themes/icarus/includes/common/ConfigGenerator.js","hash":"451397efc7808787419fa3eb6b043c0bd8bbdf30","modified":1577937081622},{"_id":"themes/icarus/includes/common/ConfigValidator.js","hash":"48cff5402e93b11d5266370e9c4b78ee21369cb9","modified":1577937081622},{"_id":"themes/icarus/includes/common/utils.js","hash":"c0aeaeb57a42bcc71a92da2249762f91abd83ffe","modified":1577937081622},{"_id":"themes/icarus/includes/generators/categories.js","hash":"7cb370ac53a05d6b1b9203579716c0ca83d35c36","modified":1577937081623},{"_id":"themes/icarus/includes/generators/category.js","hash":"313e170e55d74526c4e1be7181ef7a21439147c9","modified":1577937081623},{"_id":"themes/icarus/includes/generators/insight.js","hash":"c4b981443927b87cc14a3a583029e13f819d6d71","modified":1577937081623},{"_id":"themes/icarus/includes/generators/tags.js","hash":"8195322c208706427a1cf56361669dca4d86f6f1","modified":1577937081623},{"_id":"themes/icarus/includes/helpers/cdn.js","hash":"7d34ea6400cb3611c374c135304abcb65ef291b7","modified":1577937081623},{"_id":"themes/icarus/includes/helpers/config.js","hash":"2f76cfc1481cfb8ba2820e1611c3dfc340e5531b","modified":1578060620091},{"_id":"themes/icarus/includes/helpers/layout.js","hash":"c24589e283bc9c9cd47680eb93b9aeba1195ae84","modified":1578117322951},{"_id":"themes/icarus/includes/helpers/override.js","hash":"223771860caddffcbce7d84dfb07798f6aa0bdda","modified":1577937081624},{"_id":"themes/icarus/includes/helpers/page.js","hash":"d85844cda82512e5541db0e28bfe778bd117d207","modified":1578048397461},{"_id":"themes/icarus/includes/helpers/site.js","hash":"2f55818448fe83c73418dcf9751745c7918c10e3","modified":1577937081624},{"_id":"themes/icarus/includes/specs/article.spec.js","hash":"ce24279cd0cd39855216dab0cd5223c755757cdf","modified":1577937081624},{"_id":"themes/icarus/includes/specs/comment.spec.js","hash":"b0ef033e363b918134fb5a003143e9bd8fafa300","modified":1577937081624},{"_id":"themes/icarus/includes/specs/config.spec.js","hash":"7a9bac384a73cf9f39173fdb2dfc2813784d8891","modified":1577937081624},{"_id":"themes/icarus/includes/specs/donate.spec.js","hash":"722cb2662569957e8b1d1a467d9632b8cc6e69d6","modified":1577937081625},{"_id":"themes/icarus/includes/specs/footer.spec.js","hash":"8e6d7c5f9a13ce03241b6562259d210b389cb88e","modified":1577937081625},{"_id":"themes/icarus/includes/specs/icon_link.spec.js","hash":"7bce7e778a622ed3b3bccbe4d51d481454d78ba7","modified":1578048397462},{"_id":"themes/icarus/includes/specs/meta.spec.js","hash":"ed1b818b929d71930608291514a72ef5a639efee","modified":1577937081625},{"_id":"themes/icarus/includes/specs/navbar.spec.js","hash":"7de29c0031738a4de4d31ed4f7b0c43447c7961c","modified":1577937081625},{"_id":"themes/icarus/includes/specs/plugins.spec.js","hash":"2fb7a28fdde9a46f576e69b9967f24d66adffb57","modified":1577937081626},{"_id":"themes/icarus/includes/specs/providers.spec.js","hash":"820cc6936ba75e3104cc2e8641716ed65ada8b6f","modified":1577937081626},{"_id":"themes/icarus/includes/specs/search.spec.js","hash":"1e3995cdc471e6a2817cd45e2b6f0fd39b4540ec","modified":1577937081626},{"_id":"themes/icarus/includes/specs/share.spec.js","hash":"5ec65409a17ead13974140fc5ddc19e526586d9f","modified":1577937081626},{"_id":"themes/icarus/includes/specs/sidebar.spec.js","hash":"630c9701affe2549abc61cd4d1e5153af2224fb6","modified":1577937081626},{"_id":"themes/icarus/includes/specs/widgets.spec.js","hash":"c5cedfe1074c0566baf8aca248f0392a501d9a74","modified":1577937081627},{"_id":"themes/icarus/includes/utils/lru.js","hash":"35c0ede3553549758ff5e4ded2bc650778793377","modified":1577937081627},{"_id":"themes/icarus/includes/tasks/check_config.js","hash":"ce7626d643737c90dee6b75435ccdec26b89dacf","modified":1577937081627},{"_id":"themes/icarus/includes/tasks/check_deps.js","hash":"ab08051f785eab2a0685aa537270d2988bc13639","modified":1577937081627},{"_id":"themes/icarus/includes/tasks/welcome.js","hash":"00d1ef8c9609552b82e9a5140b838a9057c59508","modified":1577937081627},{"_id":"themes/icarus/layout/comment/changyan.ejs","hash":"73038ac4fdfdfa71d92edaa98cc194b3446586a3","modified":1577937081630},{"_id":"themes/icarus/layout/comment/changyan.locals.js","hash":"49bce2ee742c7224bda97092d6e0a1a09184ef34","modified":1577937081630},{"_id":"themes/icarus/layout/comment/disqus.ejs","hash":"7a8c656c8651d48e21ed24c469ea75898b2b12df","modified":1577937081631},{"_id":"themes/icarus/layout/comment/disqus.locals.js","hash":"a8d2cecaa82ec9e2e2e61cb73417d63d115335d6","modified":1577937081631},{"_id":"themes/icarus/layout/comment/facebook.ejs","hash":"1c3751f36f737527e352c65bb1ca7172ff792979","modified":1577937081631},{"_id":"themes/icarus/layout/comment/facebook.locals.js","hash":"77e3ef1d933660d980b26d15968aa1a5c8a93a56","modified":1577937081631},{"_id":"themes/icarus/layout/comment/gitalk.ejs","hash":"39686c7ffa1077fbaeea9e0bbc9402f9ca4a0d18","modified":1578069394351},{"_id":"themes/icarus/layout/comment/gitalk.locals.js","hash":"f920f130598148b4d9f213c82f2d7f88a796012f","modified":1577937081632},{"_id":"themes/icarus/layout/comment/gitment.ejs","hash":"d5e1a396e23df4e75e139d12846290bdb08ba01e","modified":1577937081632},{"_id":"themes/icarus/layout/comment/gitment.locals.js","hash":"f920f130598148b4d9f213c82f2d7f88a796012f","modified":1577937081632},{"_id":"themes/icarus/layout/comment/isso.ejs","hash":"cc6a43bd24be764086f88ad7c5c97ff04df87e0b","modified":1577937081632},{"_id":"themes/icarus/layout/comment/isso.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081632},{"_id":"themes/icarus/layout/comment/livere.ejs","hash":"12ff9a345f6bba2f732f592e39508c2afde89b00","modified":1577937081632},{"_id":"themes/icarus/layout/comment/livere.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081633},{"_id":"themes/icarus/layout/comment/valine.ejs","hash":"31471cd05018583249b4c09a78cf1d02e7987244","modified":1577937081633},{"_id":"themes/icarus/layout/comment/valine.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081633},{"_id":"themes/icarus/layout/common/article.ejs","hash":"6d513f3cf313b34349e125821408decdfd09ca66","modified":1578111563282},{"_id":"themes/icarus/layout/common/article.locals.js","hash":"1f108fa96e61a681d7b1ee390b4f0ff60d042720","modified":1577937081634},{"_id":"themes/icarus/layout/common/footer.ejs","hash":"8b4f636aa30ad588ce094528375336375a0585b6","modified":1578048397463},{"_id":"themes/icarus/layout/common/footer.locals.js","hash":"504ed92dc76723f19777463d690acfbe1d89e2ba","modified":1577937081634},{"_id":"themes/icarus/layout/common/head.ejs","hash":"0998072c9ece90e26339f2503633c4ca08da62e9","modified":1577937081634},{"_id":"themes/icarus/layout/common/navbar.ejs","hash":"0fe0c940fcb112828d465830713a0cdd42864951","modified":1577937081634},{"_id":"themes/icarus/layout/common/navbar.locals.js","hash":"7e523ba80667038f2e58cf4f9cb073e9afbc70e6","modified":1577937081635},{"_id":"themes/icarus/layout/common/paginator.ejs","hash":"7837d80b27f166161b3deeffb571680025c7d723","modified":1577937081635},{"_id":"themes/icarus/layout/common/scripts.ejs","hash":"99da87756d2bd234b058e90d2c6dabc1bc10f20e","modified":1577937081635},{"_id":"themes/icarus/layout/common/widget.ejs","hash":"0f2526f2d2696c598d829972e33f35748b8450c1","modified":1578048397463},{"_id":"themes/icarus/layout/donate/alipay.ejs","hash":"3290058879973e403a05472a0fe2ac0219d5b961","modified":1577937081635},{"_id":"themes/icarus/layout/donate/alipay.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081636},{"_id":"themes/icarus/layout/donate/patreon.ejs","hash":"fc19da9674649c035d133535078ff7e37d0f54c1","modified":1577937081636},{"_id":"themes/icarus/layout/donate/patreon.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081636},{"_id":"themes/icarus/layout/donate/paypal.ejs","hash":"dbb90fa9214d659ea6bbd5a92ea00888adf3761e","modified":1577937081636},{"_id":"themes/icarus/layout/donate/paypal.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081636},{"_id":"themes/icarus/layout/donate/wechat.ejs","hash":"051b873e1fc28c1d7c2d6443991b6a2f43813e6b","modified":1577937081637},{"_id":"themes/icarus/layout/donate/wechat.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081637},{"_id":"themes/icarus/layout/plugin/animejs.ejs","hash":"c17ea2cfe5cb239342166e2ba72cbfc663c8160f","modified":1577937081638},{"_id":"themes/icarus/layout/plugin/animejs.locals.js","hash":"3bf911060a222f00b03be708c37f20e36cb66ba9","modified":1577937081638},{"_id":"themes/icarus/layout/plugin/back-to-top.ejs","hash":"5936b5fd2f2444605a21c6c422623f07f02d5c9a","modified":1577937081638},{"_id":"themes/icarus/layout/plugin/back-to-top.locals.js","hash":"3bf911060a222f00b03be708c37f20e36cb66ba9","modified":1577937081638},{"_id":"themes/icarus/layout/plugin/baidu-analytics.ejs","hash":"7dbbea5722277e00a624c1796ec83d5f9c12d059","modified":1577937081638},{"_id":"themes/icarus/layout/plugin/baidu-analytics.locals.js","hash":"c02eb152e6aff05833006e6edd32b74c1c4258c3","modified":1577937081638},{"_id":"themes/icarus/layout/plugin/busuanzi.ejs","hash":"4285b0ae608c7c54e4ecbebb6d22d4cd1be28f70","modified":1577937081639},{"_id":"themes/icarus/layout/plugin/busuanzi.locals.js","hash":"ec80bcfa4c1302c04130a746df4b1298d117de0b","modified":1577937081639},{"_id":"themes/icarus/layout/plugin/gallery.ejs","hash":"7d19b7a5713d08a614578f079f1327a651c472ae","modified":1577937081639},{"_id":"themes/icarus/layout/plugin/gallery.locals.js","hash":"037fb56dffc128d3a91c1cb8852998d9539d3fac","modified":1577937081639},{"_id":"themes/icarus/layout/plugin/google-analytics.ejs","hash":"13b298b0026bfc7bcb6a47b6c795fe15cc4584fc","modified":1577937081639},{"_id":"themes/icarus/layout/plugin/google-analytics.locals.js","hash":"c02eb152e6aff05833006e6edd32b74c1c4258c3","modified":1577937081640},{"_id":"themes/icarus/layout/plugin/hotjar.ejs","hash":"6df0d8f77ed39e4d32c78177844115e31bf3a776","modified":1577937081640},{"_id":"themes/icarus/layout/plugin/hotjar.locals.js","hash":"9258fc2af057d2545a43fae54790743b63450378","modified":1577937081640},{"_id":"themes/icarus/layout/plugin/mathjax.ejs","hash":"dddb6f37487286fe2080118bcbb4a8d82dc84d5e","modified":1577937081640},{"_id":"themes/icarus/layout/plugin/mathjax.locals.js","hash":"7faa26fa6da6a93dc3f7fdcf5a784d1f8825b031","modified":1577937081640},{"_id":"themes/icarus/layout/plugin/outdated-browser.ejs","hash":"1437d1ac085a8110e61317254f6c0a034121bc39","modified":1577937081640},{"_id":"themes/icarus/layout/plugin/outdated-browser.locals.js","hash":"037fb56dffc128d3a91c1cb8852998d9539d3fac","modified":1577937081641},{"_id":"themes/icarus/layout/plugin/progressbar.ejs","hash":"34423f74787cc9d67b2598dd69b07c84d5bf2280","modified":1577937081641},{"_id":"themes/icarus/layout/plugin/progressbar.locals.js","hash":"ec80bcfa4c1302c04130a746df4b1298d117de0b","modified":1577937081641},{"_id":"themes/icarus/layout/search/baidu.ejs","hash":"850aa91778100d693a52b10eaa8586c8e3215ee6","modified":1577937081641},{"_id":"themes/icarus/layout/search/baidu.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081641},{"_id":"themes/icarus/layout/search/google-cse.ejs","hash":"4b881a99325a6a0cebf97ac53e09d8fc67f87d29","modified":1577937081642},{"_id":"themes/icarus/layout/search/google-cse.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081642},{"_id":"themes/icarus/layout/search/insight.ejs","hash":"9a27db2a007582ceee7ca4b1eebddbd456893568","modified":1577937081642},{"_id":"themes/icarus/layout/search/insight.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081642},{"_id":"themes/icarus/layout/share/addthis.ejs","hash":"f1c5f337333009d5f00dfbac4864a16ef8f9cb8d","modified":1577937081642},{"_id":"themes/icarus/layout/share/addthis.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081643},{"_id":"themes/icarus/layout/share/addtoany.ejs","hash":"95d3bc1a841bd934b1ae9209ad1af74e743ecb10","modified":1577937081643},{"_id":"themes/icarus/layout/share/addtoany.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081643},{"_id":"themes/icarus/layout/share/bdshare.ejs","hash":"f14c8084b7ee16a091f0bd2ae9039e3bfff7e7b7","modified":1577937081643},{"_id":"themes/icarus/layout/share/bdshare.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081643},{"_id":"themes/icarus/layout/share/sharejs.ejs","hash":"0f28a2bed23ba80014e7fd5e28a5eb807ec04ba2","modified":1578048397463},{"_id":"themes/icarus/layout/share/sharejs.locals.js","hash":"11976fd4cfed1044be29b476b34c33175c9b4308","modified":1577937081644},{"_id":"themes/icarus/layout/share/sharethis.ejs","hash":"4f2c40f790f3be0a4e79db04f02ea41ba2f4d4c0","modified":1577937081644},{"_id":"themes/icarus/layout/share/sharethis.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081644},{"_id":"themes/icarus/layout/widget/.DS_Store","hash":"d880bfca0110ea7f19179d834cdc916d16e72b72","modified":1578048397463},{"_id":"themes/icarus/layout/widget/archive.ejs","hash":"742952c7ace79b4d66db9a447bf5977597de841e","modified":1578048397463},{"_id":"themes/icarus/layout/widget/archive.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081645},{"_id":"themes/icarus/layout/widget/category.ejs","hash":"17e58e537645c4434a1140377ae3e7f43cca4927","modified":1577937081645},{"_id":"themes/icarus/layout/widget/category.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081645},{"_id":"themes/icarus/layout/widget/links.ejs","hash":"bb6510193632413e83227bdffad7c3b37629dcde","modified":1577937081645},{"_id":"themes/icarus/layout/widget/links.locals.js","hash":"872cf1a18e152361f5739c6d5fecc0bf46d59513","modified":1577937081645},{"_id":"themes/icarus/layout/widget/profile.ejs","hash":"436974e24e793d66beea44a97cf3fa6c5bdda867","modified":1578048397464},{"_id":"themes/icarus/layout/widget/recent_posts.ejs","hash":"14a2f4587831e017b93818c06dbe18a7e8a27c1e","modified":1577937081646},{"_id":"themes/icarus/layout/widget/profile.locals.js","hash":"9a43112ac0a58df98bb418563ec04558023e1fae","modified":1577937081646},{"_id":"themes/icarus/layout/widget/recent_posts.locals.js","hash":"5065aca74ec2c98ec88994636fee8408f769c5f2","modified":1577937081646},{"_id":"themes/icarus/layout/widget/subscribe_email.ejs","hash":"391622e9c1d17bf79180faa617ed8c1ee1871a87","modified":1577937081646},{"_id":"themes/icarus/layout/widget/subscribe_email.locals.js","hash":"aae87fbdb7a1245a0fc0637225a935fc39836916","modified":1577937081647},{"_id":"themes/icarus/layout/widget/tag.ejs","hash":"e41aff420cc4ea1c454de49bd8af0e7a93f3db3f","modified":1577937081647},{"_id":"themes/icarus/layout/widget/tagcloud.ejs","hash":"8bb486e10b4e0e3900dcc5ffb46717777c460e44","modified":1578048397464},{"_id":"themes/icarus/layout/widget/tagcloud.locals.js","hash":"093f59d2f43e7ffa47bee79da15f98705300dfba","modified":1577937081647},{"_id":"themes/icarus/layout/widget/toc.ejs","hash":"b6e0a3d95e8660ceb9a94b6e671ad9a6e345acd1","modified":1578048397464},{"_id":"themes/icarus/layout/widget/toc.locals.js","hash":"e730a7fff2717f17741540e5ed77b89e289fdeab","modified":1577937081647},{"_id":"themes/icarus/source/css/back-to-top.css","hash":"5805bee2445e997d64dfe526b08b5fe0bce357eb","modified":1577937081648},{"_id":"themes/icarus/source/css/progressbar.css","hash":"bbc737b7a8feb19901e792c447a846273779d5c3","modified":1577937081649},{"_id":"themes/icarus/source/css/insight.css","hash":"22943a610d5cfffedfb823c692f4db2b1f37a4c9","modified":1577937081648},{"_id":"themes/icarus/source/css/search.css","hash":"d6a59894819e7431d42b249b6c2fc9ff3b99a488","modified":1577937081649},{"_id":"themes/icarus/source/css/style.styl","hash":"d9c612f4e67cdc4e787046dc9e4e64ce31264d40","modified":1578111041259},{"_id":"themes/icarus/source/js/animation.js","hash":"d744581909d2d092a584be07c39f9d3f0d009ec7","modified":1577937081651},{"_id":"themes/icarus/source/js/insight.js","hash":"8ba56fd5e4232a05ccef5f8b733c7ecca0814633","modified":1577937081652},{"_id":"themes/icarus/source/js/gallery.js","hash":"bb74e694457dc23b83ac80cf5aadcd26b60469fd","modified":1577937081652},{"_id":"themes/icarus/source/js/back-to-top.js","hash":"b1dcf30577cefe833dc6151757c0a05ea5b5a643","modified":1577937081652},{"_id":"themes/icarus/source/js/main.js","hash":"009991bd09cfc4f62ce1ce42d138709a76732a6f","modified":1578048316901},{"_id":"themes/icarus/source/images/.DS_Store","hash":"b2057735a0742688a1279c5cc099fbf91519b3f7","modified":1578128705520},{"_id":"themes/icarus/source/images/favicon.png","hash":"ced4774159b178d14dc3504bfcf6a2e928587883","modified":1577970272080},{"_id":"themes/icarus/source/images/logo.png","hash":"bf936514d39216192308530005354793cce73f4e","modified":1577969646704},{"_id":"themes/icarus/source/images/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1577937081651},{"_id":"themes/icarus/source/images/thumbnail.svg","hash":"b9c58ff09ed415e6cf08b42b35faa2bc000d5059","modified":1577937081651},{"_id":"themes/icarus/source/images/alipay.JPG","hash":"dbc4a7854afd6c2a7e42869b9b0ddb5b1a43866c","modified":1577972646000},{"_id":"themes/icarus/source/images/avatar.jpg","hash":"3c5113043990ad941b130bd4cbc4ef8fe1fcc7e6","modified":1547818171000},{"_id":"themes/icarus/source/images/wechatpay.JPG","hash":"1d840127f10a0a6dc0b0d6b10e1d6eeb30ad2321","modified":1577972646000},{"_id":"public/content.json","hash":"4207dbd179b4dcc7fdfbc13b11ec830a40e6cfce","modified":1581068346626},{"_id":"public/2020/01/03/Gallery/index.html","hash":"8f0706737e77312863dcdfcf448a806a24be824e","modified":1581004138459},{"_id":"public/2020/01/03/About_2019-1-3/index.html","hash":"1b19d8f9cac14013dc441ab4a1a798ff5c5dd086","modified":1578128559917},{"_id":"public/archives/2020/index.html","hash":"b76efe24a7a263b620e07d7d66edcb982d8de3dd","modified":1581004138459},{"_id":"public/archives/index.html","hash":"44eb4c19040bbfe89da2a82c32d7bfc0d2eb3732","modified":1581004138459},{"_id":"public/archives/2020/01/index.html","hash":"91d5b18934902a39a18d9fe1f2b95c8eb62d3a50","modified":1581004138459},{"_id":"public/tags/Astrobear/index.html","hash":"2f50ca6d59b8b74a93ea6c7475614f3c3975704c","modified":1581004138459},{"_id":"public/tags/Life/index.html","hash":"7cfb7a6c64f3d0a68df8c80ffe64010881f67f9c","modified":1581004138459},{"_id":"public/tags/Others/index.html","hash":"327614626ed9685f7b487f381a9b2ba69c5d1c6f","modified":1581004138459},{"_id":"public/tags/Photos/index.html","hash":"a2f9db414869d1922428e78e55ad9e43cf3a5409","modified":1581004138459},{"_id":"public/tags/Astrophotography/index.html","hash":"b2f210476610817c202b06a013a6f91a09ed3753","modified":1581004138459},{"_id":"public/index.html","hash":"be1c9d6e8fa86b75a6b1485c51d2e1ccfc32ef9e","modified":1581068346626},{"_id":"public/categories/Others/index.html","hash":"e30f3b4b79b984d131fca9d7ad9123a98a1145df","modified":1581004138459},{"_id":"public/categories/Gellary/index.html","hash":"747d76167826efaf0abfdd0e64593791b96cf1cd","modified":1578128559917},{"_id":"public/categories/index.html","hash":"f9c862485fe12d01ace73054c982fb6d1eaf46c1","modified":1581004138459},{"_id":"public/tags/index.html","hash":"66a3f02e381fab261fed072cb58e623913050402","modified":1581004138459},{"_id":"public/images/favicon.png","hash":"ced4774159b178d14dc3504bfcf6a2e928587883","modified":1578128559917},{"_id":"public/images/thumbnail.svg","hash":"b9c58ff09ed415e6cf08b42b35faa2bc000d5059","modified":1578128559917},{"_id":"public/images/logo.png","hash":"bf936514d39216192308530005354793cce73f4e","modified":1578128559917},{"_id":"public/images/alipay.JPG","hash":"dbc4a7854afd6c2a7e42869b9b0ddb5b1a43866c","modified":1578128559917},{"_id":"public/images/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1578128559917},{"_id":"public/css/search.css","hash":"d6a59894819e7431d42b249b6c2fc9ff3b99a488","modified":1578128559917},{"_id":"public/css/insight.css","hash":"22943a610d5cfffedfb823c692f4db2b1f37a4c9","modified":1578128559917},{"_id":"public/css/style.css","hash":"86c5814101a1a0f5fec222acd168ba26b98e3771","modified":1578128559917},{"_id":"public/css/back-to-top.css","hash":"5805bee2445e997d64dfe526b08b5fe0bce357eb","modified":1578128559917},{"_id":"public/js/animation.js","hash":"d744581909d2d092a584be07c39f9d3f0d009ec7","modified":1578128559917},{"_id":"public/js/gallery.js","hash":"bb74e694457dc23b83ac80cf5aadcd26b60469fd","modified":1578128559917},{"_id":"public/js/insight.js","hash":"8ba56fd5e4232a05ccef5f8b733c7ecca0814633","modified":1578128559917},{"_id":"public/js/back-to-top.js","hash":"b1dcf30577cefe833dc6151757c0a05ea5b5a643","modified":1578128559917},{"_id":"public/css/progressbar.css","hash":"bbc737b7a8feb19901e792c447a846273779d5c3","modified":1578128559917},{"_id":"public/js/main.js","hash":"009991bd09cfc4f62ce1ce42d138709a76732a6f","modified":1578128559917},{"_id":"public/images/avatar.jpg","hash":"3c5113043990ad941b130bd4cbc4ef8fe1fcc7e6","modified":1578128559917},{"_id":"public/images/wechatpay.JPG","hash":"1d840127f10a0a6dc0b0d6b10e1d6eeb30ad2321","modified":1578128559917},{"_id":"source/_posts/About.md","hash":"154ce21a19d1aac0bf4e2ff4a6b99c0e405b7cf7","modified":1578128618244},{"_id":"source/_posts/RLSummarize2.md","hash":"1bcb47a425729be5e1d29b8fd2cd82da8d15b5b3","modified":1581005495224},{"_id":"source/_posts/RLSummarize3.md","hash":"41d68356c0fc75613c590bc6c8634dadef67c5d0","modified":1581001067072},{"_id":"source/_posts/AirSimMultirotorAPIs.md","hash":"2a3de6bc11468cc1e1da5f26d7504e24634ea5c2","modified":1579276481295},{"_id":"source/_posts/Pythonå­¦ä¹ ç¬”è®°.md","hash":"61890a075ed055448a6aec975fa185a2db2b7396","modified":1578889105089},{"_id":"source/_posts/RLSummarize1.md","hash":"439438a529e22bd748390437811224fbc8d39b77","modified":1581068340831},{"_id":"source/_posts/åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“.md","hash":"8963423b10f0cae9e120af65b82295c4a7ac21a6","modified":1578645923632},{"_id":"public/2020/02/01/RLSummarize3/index.html","hash":"a129ae7e4264ad95b878957332ac9b235413a4e9","modified":1581004138459},{"_id":"public/2020/01/17/RLSummarize1/index.html","hash":"8635c0b313d887c6790cf8ea5ac814c239d883a0","modified":1581068346626},{"_id":"public/2020/01/18/RLSummarize2/index.html","hash":"fecd0970311d233bcfdb97781b252cd412707816","modified":1581005503476},{"_id":"public/2020/01/15/AirSimMultirotorAPIs/index.html","hash":"ee2908c9a3f2a24c479e8384eb7cb34591875a27","modified":1581004138459},{"_id":"public/2020/01/08/åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“/index.html","hash":"306774d9af87571cdbef364bc498c1c8c2c71a02","modified":1581004138459},{"_id":"public/2020/01/06/Pythonå­¦ä¹ ç¬”è®°/index.html","hash":"b84ff504223bee175c0a24c116f63b983dd0fed4","modified":1581004138459},{"_id":"public/2020/01/03/About/index.html","hash":"93928eb334850033de9cd3e5b00974c241fec7b2","modified":1581004138459},{"_id":"public/tags/AirSim/index.html","hash":"7e26e82c9a78ea949e30962da97b94e902b284e2","modified":1581004138459},{"_id":"public/tags/Python/index.html","hash":"19552d0ef6c8ca674cd679b974ae9fd71e9084ec","modified":1581068346626},{"_id":"public/tags/RL/index.html","hash":"82c28a8e9a424d471432cbc31517e37116f2ebad","modified":1581068346626},{"_id":"public/tags/Research/index.html","hash":"b2d4425ae2f89193b77dadb6592693b7e7497c13","modified":1581068346626},{"_id":"public/tags/Programming-Language/index.html","hash":"2fc30c84c4e83445bace013fc2ba8d83007c28c2","modified":1581004138459},{"_id":"public/tags/Nginx/index.html","hash":"0276478d6c9ce496ba35827459f24e1217abdbfd","modified":1581004138459},{"_id":"public/tags/Internet-server/index.html","hash":"438e30addb85f944476aff9783f395ca439f25e7","modified":1581004138459},{"_id":"public/tags/Network-Technology/index.html","hash":"4d1fc46a302deeb300a5ff727592fe3a26a49987","modified":1581004138459},{"_id":"public/tags/Experience/index.html","hash":"a2b755837b85fbd2e677b94813bbecdefcfedd24","modified":1581004138459},{"_id":"public/archives/2020/02/index.html","hash":"d8d9994fda77ec236fb9cb6e3076a50645613cba","modified":1581004138459},{"_id":"public/categories/CS/index.html","hash":"c9b8046cf4383acb2f77999b89fb4f35b9030464","modified":1581068346626}],"Category":[{"name":"Others","_id":"ck4zczyf50004vq39a3nk34a3"},{"name":"Gellary","_id":"ck4zczyf70006vq39g17adi5e"},{"name":"CS","_id":"ck6ax1lgi0007j1p20c54bs9g"}],"Data":[],"Page":[],"Post":[{"title":"template","date":"2020-01-03T16:36:34.349Z","_content":"\n","source":"_drafts/template.md","raw":"---\ntitle: #title\ndate: #yyyy-mm-dd hh:mm:ss\ncategories: \n\t#- [cate1]\n\t#- [cate2]\n\t#...\ntags: \n\t#- tag1\n\t#- tag2\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\n#thumbnail: /thumbnail/xxx.xxx\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\n#excerpt: ...\n\n#You can begin to input your article below now.\n\n---\n\n","slug":"template","published":0,"updated":"2020-01-03T16:36:34.349Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck4zczyeu0000vq39ec769wap","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Gallery","date":"2020-01-03T15:25:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t1.jpg","excerpt":"Welcome to my gallery!","widgets":[],"_content":"\n> Photos will continue to update...\n\n<div class=\"justified-gallery\">\n\n\n\n\n![Seattle Space Needle Tower](https://astrobear.top/resource/astroblog/gallery/g1.jpg)\n\n![SF Golden Gate Bridge](https://astrobear.top/resource/astroblog/gallery/g2.jpg)\n\n![Stanford University](https://astrobear.top/resource/astroblog/gallery/g3.jpg)\n\n![Fengyun Hill](https://astrobear.top/resource/astroblog/gallery/g4.jpg)\n\n![Beyond the Clouds](https://astrobear.top/resource/astroblog/gallery/g5.jpg)\n\n![Temple](https://astrobear.top/resource/astroblog/gallery/g6.jpg)\n\n![Chaka Salt Lake](https://astrobear.top/resource/astroblog/gallery/g7.jpg)\n\n![Lizard](https://astrobear.top/resource/astroblog/gallery/g8.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g9.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g10.jpg)\n\n![Host's Cat](https://astrobear.top/resource/astroblog/gallery/g11.jpg)\n\n![Changbai Mountain](https://astrobear.top/resource/astroblog/gallery/g12.jpg)\n\n![Forbidden City](https://astrobear.top/resource/astroblog/gallery/g13.jpg)\n\n![Signal Hill, Tsingtao](https://astrobear.top/resource/astroblog/gallery/g14.jpg)\n\n![The Milky Way and Sunflower](https://astrobear.top/resource/astroblog/gallery/g15.jpg)\n\n![NGC7000 The North America Nebula](https://astrobear.top/resource/astroblog/gallery/g16.jpg)\n\n![The North Lake](https://astrobear.top/resource/astroblog/gallery/g17.jpg)\n\n![Shanghai Bund](https://astrobear.top/resource/astroblog/gallery/g18.jpg)\n\n![Xinjiekou, Nanjing](https://astrobear.top/resource/astroblog/gallery/g19.jpg)\n\n![Huangpu River](https://astrobear.top/resource/astroblog/gallery/g20.jpg)\n\n![Art Show](https://astrobear.top/resource/astroblog/gallery/g21.jpg)\n\n![Milky Way and Car](https://astrobear.top/resource/astroblog/gallery/g22.jpg)\n\n![Milky Way and Camera](https://astrobear.top/resource/astroblog/gallery/g23.jpg)\n\n![Teradacho Park](https://astrobear.top/resource/astroblog/gallery/g24.jpg)\n\n![The Jellyfish in Osaka Aquarium](https://astrobear.top/resource/astroblog/gallery/g25.jpg)\n\n![Osaka City](https://astrobear.top/resource/astroblog/gallery/g26.jpg)\n\n![Wakakusa Yama](https://astrobear.top/resource/astroblog/gallery/g27.jpg)\n\n![Kasuga Taisha](https://astrobear.top/resource/astroblog/gallery/g28.jpg)\n\n![Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g29.jpg)\n\n![Gate of Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g30.jpg)\n\n![Kyoto Tower](https://astrobear.top/resource/astroblog/gallery/g31.jpg)\n\n![Torii Gate](https://astrobear.top/resource/astroblog/gallery/g32.jpg)\n\n![M45 Pleiades](https://astrobear.top/resource/astroblog/gallery/g33.jpg)\n\n![The Milky Way](https://astrobear.top/resource/astroblog/gallery/g34.jpg)\n\n\n\n</div>","source":"_posts/Gallery.md","raw":"---\ntitle: Gallery\ndate: 2020-1-3 23:25:00\ncategories: \n\t- [Others]\n\t#- [cate2]\n\t#...\ntags: \n\t- Photos\n\t- Astrophotography\n\t- Life\n\t- Others\n\t- Astrobear\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t1.jpg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Welcome to my gallery!\n\nwidgets: []\n\n#You can begin to input your article below now.\n---\n\n> Photos will continue to update...\n\n<div class=\"justified-gallery\">\n\n\n\n\n![Seattle Space Needle Tower](https://astrobear.top/resource/astroblog/gallery/g1.jpg)\n\n![SF Golden Gate Bridge](https://astrobear.top/resource/astroblog/gallery/g2.jpg)\n\n![Stanford University](https://astrobear.top/resource/astroblog/gallery/g3.jpg)\n\n![Fengyun Hill](https://astrobear.top/resource/astroblog/gallery/g4.jpg)\n\n![Beyond the Clouds](https://astrobear.top/resource/astroblog/gallery/g5.jpg)\n\n![Temple](https://astrobear.top/resource/astroblog/gallery/g6.jpg)\n\n![Chaka Salt Lake](https://astrobear.top/resource/astroblog/gallery/g7.jpg)\n\n![Lizard](https://astrobear.top/resource/astroblog/gallery/g8.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g9.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g10.jpg)\n\n![Host's Cat](https://astrobear.top/resource/astroblog/gallery/g11.jpg)\n\n![Changbai Mountain](https://astrobear.top/resource/astroblog/gallery/g12.jpg)\n\n![Forbidden City](https://astrobear.top/resource/astroblog/gallery/g13.jpg)\n\n![Signal Hill, Tsingtao](https://astrobear.top/resource/astroblog/gallery/g14.jpg)\n\n![The Milky Way and Sunflower](https://astrobear.top/resource/astroblog/gallery/g15.jpg)\n\n![NGC7000 The North America Nebula](https://astrobear.top/resource/astroblog/gallery/g16.jpg)\n\n![The North Lake](https://astrobear.top/resource/astroblog/gallery/g17.jpg)\n\n![Shanghai Bund](https://astrobear.top/resource/astroblog/gallery/g18.jpg)\n\n![Xinjiekou, Nanjing](https://astrobear.top/resource/astroblog/gallery/g19.jpg)\n\n![Huangpu River](https://astrobear.top/resource/astroblog/gallery/g20.jpg)\n\n![Art Show](https://astrobear.top/resource/astroblog/gallery/g21.jpg)\n\n![Milky Way and Car](https://astrobear.top/resource/astroblog/gallery/g22.jpg)\n\n![Milky Way and Camera](https://astrobear.top/resource/astroblog/gallery/g23.jpg)\n\n![Teradacho Park](https://astrobear.top/resource/astroblog/gallery/g24.jpg)\n\n![The Jellyfish in Osaka Aquarium](https://astrobear.top/resource/astroblog/gallery/g25.jpg)\n\n![Osaka City](https://astrobear.top/resource/astroblog/gallery/g26.jpg)\n\n![Wakakusa Yama](https://astrobear.top/resource/astroblog/gallery/g27.jpg)\n\n![Kasuga Taisha](https://astrobear.top/resource/astroblog/gallery/g28.jpg)\n\n![Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g29.jpg)\n\n![Gate of Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g30.jpg)\n\n![Kyoto Tower](https://astrobear.top/resource/astroblog/gallery/g31.jpg)\n\n![Torii Gate](https://astrobear.top/resource/astroblog/gallery/g32.jpg)\n\n![M45 Pleiades](https://astrobear.top/resource/astroblog/gallery/g33.jpg)\n\n![The Milky Way](https://astrobear.top/resource/astroblog/gallery/g34.jpg)\n\n\n\n</div>","slug":"Gallery","published":1,"updated":"2020-01-06T08:59:25.714Z","_id":"ck4zczyf20002vq391oi1h22d","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>Photos will continue to updateâ€¦</p>\n</blockquote>\n<div class=\"justified-gallery\">\n\n\n\n\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g1.jpg\" alt=\"Seattle Space Needle Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g2.jpg\" alt=\"SF Golden Gate Bridge\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g3.jpg\" alt=\"Stanford University\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g4.jpg\" alt=\"Fengyun Hill\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g5.jpg\" alt=\"Beyond the Clouds\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g6.jpg\" alt=\"Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g7.jpg\" alt=\"Chaka Salt Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g8.jpg\" alt=\"Lizard\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g9.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g10.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g11.jpg\" alt=\"Host&#39;s Cat\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g12.jpg\" alt=\"Changbai Mountain\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g13.jpg\" alt=\"Forbidden City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g14.jpg\" alt=\"Signal Hill, Tsingtao\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g15.jpg\" alt=\"The Milky Way and Sunflower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g16.jpg\" alt=\"NGC7000 The North America Nebula\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g17.jpg\" alt=\"The North Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g18.jpg\" alt=\"Shanghai Bund\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g19.jpg\" alt=\"Xinjiekou, Nanjing\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g20.jpg\" alt=\"Huangpu River\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g21.jpg\" alt=\"Art Show\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g22.jpg\" alt=\"Milky Way and Car\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g23.jpg\" alt=\"Milky Way and Camera\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g24.jpg\" alt=\"Teradacho Park\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g25.jpg\" alt=\"The Jellyfish in Osaka Aquarium\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g26.jpg\" alt=\"Osaka City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g27.jpg\" alt=\"Wakakusa Yama\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g28.jpg\" alt=\"Kasuga Taisha\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g29.jpg\" alt=\"Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g30.jpg\" alt=\"Gate of Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g31.jpg\" alt=\"Kyoto Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g32.jpg\" alt=\"Torii Gate\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g33.jpg\" alt=\"M45 Pleiades\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g34.jpg\" alt=\"The Milky Way\"></p>\n</div>","site":{"data":{}},"more":"<blockquote>\n<p>Photos will continue to updateâ€¦</p>\n</blockquote>\n<div class=\"justified-gallery\">\n\n\n\n\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g1.jpg\" alt=\"Seattle Space Needle Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g2.jpg\" alt=\"SF Golden Gate Bridge\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g3.jpg\" alt=\"Stanford University\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g4.jpg\" alt=\"Fengyun Hill\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g5.jpg\" alt=\"Beyond the Clouds\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g6.jpg\" alt=\"Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g7.jpg\" alt=\"Chaka Salt Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g8.jpg\" alt=\"Lizard\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g9.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g10.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g11.jpg\" alt=\"Host&#39;s Cat\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g12.jpg\" alt=\"Changbai Mountain\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g13.jpg\" alt=\"Forbidden City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g14.jpg\" alt=\"Signal Hill, Tsingtao\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g15.jpg\" alt=\"The Milky Way and Sunflower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g16.jpg\" alt=\"NGC7000 The North America Nebula\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g17.jpg\" alt=\"The North Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g18.jpg\" alt=\"Shanghai Bund\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g19.jpg\" alt=\"Xinjiekou, Nanjing\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g20.jpg\" alt=\"Huangpu River\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g21.jpg\" alt=\"Art Show\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g22.jpg\" alt=\"Milky Way and Car\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g23.jpg\" alt=\"Milky Way and Camera\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g24.jpg\" alt=\"Teradacho Park\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g25.jpg\" alt=\"The Jellyfish in Osaka Aquarium\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g26.jpg\" alt=\"Osaka City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g27.jpg\" alt=\"Wakakusa Yama\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g28.jpg\" alt=\"Kasuga Taisha\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g29.jpg\" alt=\"Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g30.jpg\" alt=\"Gate of Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g31.jpg\" alt=\"Kyoto Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g32.jpg\" alt=\"Torii Gate\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g33.jpg\" alt=\"M45 Pleiades\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g34.jpg\" alt=\"The Milky Way\"></p>\n</div>"},{"title":"æ¬¢è¿æ¥åˆ°Astroblogï¼","date":"2020-01-03T12:47:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t2.jpg","excerpt":"Astroblogæ˜¯Astrobearçš„åŸºåœ°ï¼è¿™é‡Œæœ‰çŸ¥è¯†ï¼Œæ–¹æ³•ï¼Œè¿˜æœ‰æ›´å¤šï¼","_content":"\n## æ¸Šæº\n\næœ¬äºº2019å¹´4æœˆåœ¨åä¸ºäº‘è´­ä¹°äº†ä¸€å°äº‘æœåŠ¡å™¨ã€‚æœ¬æ¥æ‰“ç®—æ˜¯ä¸ºäº†ç»™è‡ªå·±â€œæœªæ¥è¦åšçš„â€œå¾®ä¿¡å°ç¨‹åºæä¾›åç«¯æœåŠ¡çš„ï¼Œç»“æœä¸€ç›´æ‹–åˆ°8æœˆä»½æ‰è´­ä¹°äº†åŸŸåå¹¶å®Œæˆäº†å¤‡æ¡ˆã€‚åœ¨è¿™ä¹‹åä¸€æ®µæ—¶é—´å†…åˆæ²¡æœ‰æ–°çš„å°ç¨‹åºè¦åšï¼Œäºæ˜¯è¿™ä¸ªæœåŠ¡å™¨å’ŒåŸŸåä¾¿ä¸€ç›´è’åºŸäº†å¿«å¤§åŠå¹´ã€‚ç”±äºå¤§ä¸‰ä¸Šå­¦æœŸç»“æŸçš„éå¸¸ä¹‹æ—©ï¼Œæˆ‘äººç”Ÿä¸­ç¬¬ä¸€æ¬¡æ‹¥æœ‰äº†å°†è¿‘ä¸¤ä¸ªæœˆçš„å¯’å‡ã€‚è¶æ­¤æœºä¼šï¼Œæˆ‘å†³å®šå°†è¿™ä¸ªæœåŠ¡å™¨å…ˆåˆ©ç”¨èµ·æ¥ï¼Œäºæ˜¯å°±æœ‰äº†Astroblogã€‚\n\n## ç®€ä»‹\n\nAstroblogä¸Šä¸»è¦å°†åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š\n\n- åœ¨å­¦æ ¡çš„è¯¾ç¨‹æ€»ç»“ï¼Œç›®å‰è®¡åˆ’å°†æ€»ç»“çš„èµ„æ–™å…¨éƒ¨ç”µå­åŒ–ï¼ˆå°½é‡ï¼Œçœ‹å¿ƒæƒ…ğŸ˜‚ï¼‰\n- ä¸è®¡ç®—æœºæŠ€æœ¯ç›¸å…³çš„æŠ€æœ¯æ€»ç»“ï¼Œæ¯”å¦‚ä¸€äº›æ•™ç¨‹æˆ–æ–¹æ³•ç­‰ï¼Œç”¨ä½œå¤‡å¿˜ï¼Œå¤§è‡´åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š\n  - ç¼–ç¨‹è¯­è¨€çŸ¥è¯†æ€»ç»“\n  - è®¡ç®—æœºç½‘ç»œæŠ€æœ¯\n  - ç¨‹åºå¼€å‘\n  - é»‘è‹¹æœï£¿\n- ä¸ªäººæ‘„å½±ä½œå“ä»¥åŠå…¶ä»–ä¼˜ç§€æ‘„å½±ä½œå“çš„å±•è§ˆ\n- ä¸€äº›æ°‘èˆªçŸ¥è¯†\n- å…¶ä»–å†…å®¹\n\n## å…³äºAstrobear\n\nç«™é•¿ç°åœ¨ï¼ˆ2020å¹´1æœˆï¼‰æ˜¯ä¸€ä¸ªå¤§ä¸‰å­¦ç”Ÿï¼Œä¸“ä¸šæ˜¯æ¢æµ‹åˆ¶å¯¼ä¸æ§åˆ¶æŠ€æœ¯ã€‚ä»å°çš„æ¢¦æƒ³æ˜¯æˆä¸ºä¸€åé£è¡Œå‘˜ï¼Œä½†æ˜¯é«˜ä¸‰ä½“æ£€è¢«åˆ·äº†ğŸ˜­ã€‚äºæ˜¯è¿·è¿·ç³Šç³Šå°±åˆ°äº†ç°åœ¨çš„å­¦æ ¡ï¼Œè¿›äº†ç°åœ¨çš„ä¸“ä¸šã€‚é«˜ä¸­çš„æ—¶å€™åŠ å…¥äº†å­¦æ ¡å¤©æ–‡ç¤¾ï¼Œä»é‚£æ—¶èµ·å–œæ¬¢ä¸Šäº†å¤©æ–‡ï¼ˆæ‘„å½±ï¼‰ã€‚æœ¬æ¸£åœ¨å¤§ä¸€ä¸Šå­¦æœŸå­¦äº†Cè¯­è¨€ä»¥åå¯¹è®¡ç®—æœºæŠ€æœ¯äº§ç”Ÿäº†å…´è¶£ï¼Œç„¶ååœ¨å¤§ä¸€æš‘å‡å‡ ä¹ç‹¬ç«‹å¼€å‘äº†~~ï¼ˆåˆä¼™äººæ²‰è¿·acmå¤ªå¿™äº†ï¼‰~~ä¸€ä¸ªç»™è‡ªå·±åœ¨çš„å­¦ç”Ÿç»„ç»‡ç”¨çš„ç®¡ç†ç±»å¾®ä¿¡å°ç¨‹åºï¼Œä¹‹ååœ¨â€œç¼–ç¨‹â€çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œã€‚\n\næ€»ç»“ä¸€ä¸‹ï¼ŒAstrobearæ˜¯ï¼šå­¦æ§åˆ¶çš„æ¢¦æƒ³æˆä¸ºé£è¡Œå‘˜çš„å¤©æ–‡å’Œè®¡ç®—æœºçˆ±å¥½è€…ã€‚\n\nç”±äºæˆ‘å¹¶ä¸æ˜¯è®¡ç®—æœºä¸“ä¸šçš„ï¼Œæ‰€ä»¥å…³äºè®¡ç®—æœºæŠ€æœ¯è¿™ä¸€å—æ˜¯æœ¬ç€â€œæ‹¿æ¥ä¸»ä¹‰â€çš„æ€åº¦å»å­¦çš„â€”â€”ä¼šç”¨å°±è¡Œã€‚å› æ­¤åœ¨è¿™æ–¹é¢éš¾å…ä¼šæœ‰ç–æ¼é”™è¯¯ä¹‹å¤„ï¼Œä¹Ÿè¯·å¤§å®¶æµ·æ¶µã€‚\n\nå¤šäºæœ‰äº†äº’è”ç½‘çš„å‘å±•ï¼ŒçŸ¥è¯†çš„ä¼ æ’­å¯ä»¥å¦‚æ­¤åœ°è¿…é€Ÿã€‚åœ¨æ­¤ï¼Œå¯¹ä¹‹å‰å¯¹æˆ‘æœ‰è¿‡å¸®åŠ©çš„åšä¸»è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼","source":"_posts/About.md","raw":"---\ntitle: æ¬¢è¿æ¥åˆ°Astroblogï¼\ndate: 2020-1-3 20:47\ncategories: \n\t- [Others]\n\t#- [cate2]\n\t#...\ntags: \n\t- Astrobear\n\t- Life\n\t- Others\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t2.jpg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Astroblogæ˜¯Astrobearçš„åŸºåœ°ï¼è¿™é‡Œæœ‰çŸ¥è¯†ï¼Œæ–¹æ³•ï¼Œè¿˜æœ‰æ›´å¤šï¼\n\n#You can begin to input your article below now.\n\n---\n\n## æ¸Šæº\n\næœ¬äºº2019å¹´4æœˆåœ¨åä¸ºäº‘è´­ä¹°äº†ä¸€å°äº‘æœåŠ¡å™¨ã€‚æœ¬æ¥æ‰“ç®—æ˜¯ä¸ºäº†ç»™è‡ªå·±â€œæœªæ¥è¦åšçš„â€œå¾®ä¿¡å°ç¨‹åºæä¾›åç«¯æœåŠ¡çš„ï¼Œç»“æœä¸€ç›´æ‹–åˆ°8æœˆä»½æ‰è´­ä¹°äº†åŸŸåå¹¶å®Œæˆäº†å¤‡æ¡ˆã€‚åœ¨è¿™ä¹‹åä¸€æ®µæ—¶é—´å†…åˆæ²¡æœ‰æ–°çš„å°ç¨‹åºè¦åšï¼Œäºæ˜¯è¿™ä¸ªæœåŠ¡å™¨å’ŒåŸŸåä¾¿ä¸€ç›´è’åºŸäº†å¿«å¤§åŠå¹´ã€‚ç”±äºå¤§ä¸‰ä¸Šå­¦æœŸç»“æŸçš„éå¸¸ä¹‹æ—©ï¼Œæˆ‘äººç”Ÿä¸­ç¬¬ä¸€æ¬¡æ‹¥æœ‰äº†å°†è¿‘ä¸¤ä¸ªæœˆçš„å¯’å‡ã€‚è¶æ­¤æœºä¼šï¼Œæˆ‘å†³å®šå°†è¿™ä¸ªæœåŠ¡å™¨å…ˆåˆ©ç”¨èµ·æ¥ï¼Œäºæ˜¯å°±æœ‰äº†Astroblogã€‚\n\n## ç®€ä»‹\n\nAstroblogä¸Šä¸»è¦å°†åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š\n\n- åœ¨å­¦æ ¡çš„è¯¾ç¨‹æ€»ç»“ï¼Œç›®å‰è®¡åˆ’å°†æ€»ç»“çš„èµ„æ–™å…¨éƒ¨ç”µå­åŒ–ï¼ˆå°½é‡ï¼Œçœ‹å¿ƒæƒ…ğŸ˜‚ï¼‰\n- ä¸è®¡ç®—æœºæŠ€æœ¯ç›¸å…³çš„æŠ€æœ¯æ€»ç»“ï¼Œæ¯”å¦‚ä¸€äº›æ•™ç¨‹æˆ–æ–¹æ³•ç­‰ï¼Œç”¨ä½œå¤‡å¿˜ï¼Œå¤§è‡´åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š\n  - ç¼–ç¨‹è¯­è¨€çŸ¥è¯†æ€»ç»“\n  - è®¡ç®—æœºç½‘ç»œæŠ€æœ¯\n  - ç¨‹åºå¼€å‘\n  - é»‘è‹¹æœï£¿\n- ä¸ªäººæ‘„å½±ä½œå“ä»¥åŠå…¶ä»–ä¼˜ç§€æ‘„å½±ä½œå“çš„å±•è§ˆ\n- ä¸€äº›æ°‘èˆªçŸ¥è¯†\n- å…¶ä»–å†…å®¹\n\n## å…³äºAstrobear\n\nç«™é•¿ç°åœ¨ï¼ˆ2020å¹´1æœˆï¼‰æ˜¯ä¸€ä¸ªå¤§ä¸‰å­¦ç”Ÿï¼Œä¸“ä¸šæ˜¯æ¢æµ‹åˆ¶å¯¼ä¸æ§åˆ¶æŠ€æœ¯ã€‚ä»å°çš„æ¢¦æƒ³æ˜¯æˆä¸ºä¸€åé£è¡Œå‘˜ï¼Œä½†æ˜¯é«˜ä¸‰ä½“æ£€è¢«åˆ·äº†ğŸ˜­ã€‚äºæ˜¯è¿·è¿·ç³Šç³Šå°±åˆ°äº†ç°åœ¨çš„å­¦æ ¡ï¼Œè¿›äº†ç°åœ¨çš„ä¸“ä¸šã€‚é«˜ä¸­çš„æ—¶å€™åŠ å…¥äº†å­¦æ ¡å¤©æ–‡ç¤¾ï¼Œä»é‚£æ—¶èµ·å–œæ¬¢ä¸Šäº†å¤©æ–‡ï¼ˆæ‘„å½±ï¼‰ã€‚æœ¬æ¸£åœ¨å¤§ä¸€ä¸Šå­¦æœŸå­¦äº†Cè¯­è¨€ä»¥åå¯¹è®¡ç®—æœºæŠ€æœ¯äº§ç”Ÿäº†å…´è¶£ï¼Œç„¶ååœ¨å¤§ä¸€æš‘å‡å‡ ä¹ç‹¬ç«‹å¼€å‘äº†~~ï¼ˆåˆä¼™äººæ²‰è¿·acmå¤ªå¿™äº†ï¼‰~~ä¸€ä¸ªç»™è‡ªå·±åœ¨çš„å­¦ç”Ÿç»„ç»‡ç”¨çš„ç®¡ç†ç±»å¾®ä¿¡å°ç¨‹åºï¼Œä¹‹ååœ¨â€œç¼–ç¨‹â€çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œã€‚\n\næ€»ç»“ä¸€ä¸‹ï¼ŒAstrobearæ˜¯ï¼šå­¦æ§åˆ¶çš„æ¢¦æƒ³æˆä¸ºé£è¡Œå‘˜çš„å¤©æ–‡å’Œè®¡ç®—æœºçˆ±å¥½è€…ã€‚\n\nç”±äºæˆ‘å¹¶ä¸æ˜¯è®¡ç®—æœºä¸“ä¸šçš„ï¼Œæ‰€ä»¥å…³äºè®¡ç®—æœºæŠ€æœ¯è¿™ä¸€å—æ˜¯æœ¬ç€â€œæ‹¿æ¥ä¸»ä¹‰â€çš„æ€åº¦å»å­¦çš„â€”â€”ä¼šç”¨å°±è¡Œã€‚å› æ­¤åœ¨è¿™æ–¹é¢éš¾å…ä¼šæœ‰ç–æ¼é”™è¯¯ä¹‹å¤„ï¼Œä¹Ÿè¯·å¤§å®¶æµ·æ¶µã€‚\n\nå¤šäºæœ‰äº†äº’è”ç½‘çš„å‘å±•ï¼ŒçŸ¥è¯†çš„ä¼ æ’­å¯ä»¥å¦‚æ­¤åœ°è¿…é€Ÿã€‚åœ¨æ­¤ï¼Œå¯¹ä¹‹å‰å¯¹æˆ‘æœ‰è¿‡å¸®åŠ©çš„åšä¸»è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼","slug":"About","published":1,"updated":"2020-01-04T09:03:38.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck6ax1lfy0000j1p2e2ht8nn6","content":"<h2 id=\"æ¸Šæº\"><a href=\"#æ¸Šæº\" class=\"headerlink\" title=\"æ¸Šæº\"></a>æ¸Šæº</h2><p>æœ¬äºº2019å¹´4æœˆåœ¨åä¸ºäº‘è´­ä¹°äº†ä¸€å°äº‘æœåŠ¡å™¨ã€‚æœ¬æ¥æ‰“ç®—æ˜¯ä¸ºäº†ç»™è‡ªå·±â€œæœªæ¥è¦åšçš„â€œå¾®ä¿¡å°ç¨‹åºæä¾›åç«¯æœåŠ¡çš„ï¼Œç»“æœä¸€ç›´æ‹–åˆ°8æœˆä»½æ‰è´­ä¹°äº†åŸŸåå¹¶å®Œæˆäº†å¤‡æ¡ˆã€‚åœ¨è¿™ä¹‹åä¸€æ®µæ—¶é—´å†…åˆæ²¡æœ‰æ–°çš„å°ç¨‹åºè¦åšï¼Œäºæ˜¯è¿™ä¸ªæœåŠ¡å™¨å’ŒåŸŸåä¾¿ä¸€ç›´è’åºŸäº†å¿«å¤§åŠå¹´ã€‚ç”±äºå¤§ä¸‰ä¸Šå­¦æœŸç»“æŸçš„éå¸¸ä¹‹æ—©ï¼Œæˆ‘äººç”Ÿä¸­ç¬¬ä¸€æ¬¡æ‹¥æœ‰äº†å°†è¿‘ä¸¤ä¸ªæœˆçš„å¯’å‡ã€‚è¶æ­¤æœºä¼šï¼Œæˆ‘å†³å®šå°†è¿™ä¸ªæœåŠ¡å™¨å…ˆåˆ©ç”¨èµ·æ¥ï¼Œäºæ˜¯å°±æœ‰äº†Astroblogã€‚</p>\n<h2 id=\"ç®€ä»‹\"><a href=\"#ç®€ä»‹\" class=\"headerlink\" title=\"ç®€ä»‹\"></a>ç®€ä»‹</h2><p>Astroblogä¸Šä¸»è¦å°†åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š</p>\n<ul>\n<li>åœ¨å­¦æ ¡çš„è¯¾ç¨‹æ€»ç»“ï¼Œç›®å‰è®¡åˆ’å°†æ€»ç»“çš„èµ„æ–™å…¨éƒ¨ç”µå­åŒ–ï¼ˆå°½é‡ï¼Œçœ‹å¿ƒæƒ…ğŸ˜‚ï¼‰</li>\n<li>ä¸è®¡ç®—æœºæŠ€æœ¯ç›¸å…³çš„æŠ€æœ¯æ€»ç»“ï¼Œæ¯”å¦‚ä¸€äº›æ•™ç¨‹æˆ–æ–¹æ³•ç­‰ï¼Œç”¨ä½œå¤‡å¿˜ï¼Œå¤§è‡´åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š<ul>\n<li>ç¼–ç¨‹è¯­è¨€çŸ¥è¯†æ€»ç»“</li>\n<li>è®¡ç®—æœºç½‘ç»œæŠ€æœ¯</li>\n<li>ç¨‹åºå¼€å‘</li>\n<li>é»‘è‹¹æœï£¿</li>\n</ul>\n</li>\n<li>ä¸ªäººæ‘„å½±ä½œå“ä»¥åŠå…¶ä»–ä¼˜ç§€æ‘„å½±ä½œå“çš„å±•è§ˆ</li>\n<li>ä¸€äº›æ°‘èˆªçŸ¥è¯†</li>\n<li>å…¶ä»–å†…å®¹</li>\n</ul>\n<h2 id=\"å…³äºAstrobear\"><a href=\"#å…³äºAstrobear\" class=\"headerlink\" title=\"å…³äºAstrobear\"></a>å…³äºAstrobear</h2><p>ç«™é•¿ç°åœ¨ï¼ˆ2020å¹´1æœˆï¼‰æ˜¯ä¸€ä¸ªå¤§ä¸‰å­¦ç”Ÿï¼Œä¸“ä¸šæ˜¯æ¢æµ‹åˆ¶å¯¼ä¸æ§åˆ¶æŠ€æœ¯ã€‚ä»å°çš„æ¢¦æƒ³æ˜¯æˆä¸ºä¸€åé£è¡Œå‘˜ï¼Œä½†æ˜¯é«˜ä¸‰ä½“æ£€è¢«åˆ·äº†ğŸ˜­ã€‚äºæ˜¯è¿·è¿·ç³Šç³Šå°±åˆ°äº†ç°åœ¨çš„å­¦æ ¡ï¼Œè¿›äº†ç°åœ¨çš„ä¸“ä¸šã€‚é«˜ä¸­çš„æ—¶å€™åŠ å…¥äº†å­¦æ ¡å¤©æ–‡ç¤¾ï¼Œä»é‚£æ—¶èµ·å–œæ¬¢ä¸Šäº†å¤©æ–‡ï¼ˆæ‘„å½±ï¼‰ã€‚æœ¬æ¸£åœ¨å¤§ä¸€ä¸Šå­¦æœŸå­¦äº†Cè¯­è¨€ä»¥åå¯¹è®¡ç®—æœºæŠ€æœ¯äº§ç”Ÿäº†å…´è¶£ï¼Œç„¶ååœ¨å¤§ä¸€æš‘å‡å‡ ä¹ç‹¬ç«‹å¼€å‘äº†<del>ï¼ˆåˆä¼™äººæ²‰è¿·acmå¤ªå¿™äº†ï¼‰</del>ä¸€ä¸ªç»™è‡ªå·±åœ¨çš„å­¦ç”Ÿç»„ç»‡ç”¨çš„ç®¡ç†ç±»å¾®ä¿¡å°ç¨‹åºï¼Œä¹‹ååœ¨â€œç¼–ç¨‹â€çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œã€‚</p>\n<p>æ€»ç»“ä¸€ä¸‹ï¼ŒAstrobearæ˜¯ï¼šå­¦æ§åˆ¶çš„æ¢¦æƒ³æˆä¸ºé£è¡Œå‘˜çš„å¤©æ–‡å’Œè®¡ç®—æœºçˆ±å¥½è€…ã€‚</p>\n<p>ç”±äºæˆ‘å¹¶ä¸æ˜¯è®¡ç®—æœºä¸“ä¸šçš„ï¼Œæ‰€ä»¥å…³äºè®¡ç®—æœºæŠ€æœ¯è¿™ä¸€å—æ˜¯æœ¬ç€â€œæ‹¿æ¥ä¸»ä¹‰â€çš„æ€åº¦å»å­¦çš„â€”â€”ä¼šç”¨å°±è¡Œã€‚å› æ­¤åœ¨è¿™æ–¹é¢éš¾å…ä¼šæœ‰ç–æ¼é”™è¯¯ä¹‹å¤„ï¼Œä¹Ÿè¯·å¤§å®¶æµ·æ¶µã€‚</p>\n<p>å¤šäºæœ‰äº†äº’è”ç½‘çš„å‘å±•ï¼ŒçŸ¥è¯†çš„ä¼ æ’­å¯ä»¥å¦‚æ­¤åœ°è¿…é€Ÿã€‚åœ¨æ­¤ï¼Œå¯¹ä¹‹å‰å¯¹æˆ‘æœ‰è¿‡å¸®åŠ©çš„åšä¸»è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼</p>\n","site":{"data":{}},"more":"<h2 id=\"æ¸Šæº\"><a href=\"#æ¸Šæº\" class=\"headerlink\" title=\"æ¸Šæº\"></a>æ¸Šæº</h2><p>æœ¬äºº2019å¹´4æœˆåœ¨åä¸ºäº‘è´­ä¹°äº†ä¸€å°äº‘æœåŠ¡å™¨ã€‚æœ¬æ¥æ‰“ç®—æ˜¯ä¸ºäº†ç»™è‡ªå·±â€œæœªæ¥è¦åšçš„â€œå¾®ä¿¡å°ç¨‹åºæä¾›åç«¯æœåŠ¡çš„ï¼Œç»“æœä¸€ç›´æ‹–åˆ°8æœˆä»½æ‰è´­ä¹°äº†åŸŸåå¹¶å®Œæˆäº†å¤‡æ¡ˆã€‚åœ¨è¿™ä¹‹åä¸€æ®µæ—¶é—´å†…åˆæ²¡æœ‰æ–°çš„å°ç¨‹åºè¦åšï¼Œäºæ˜¯è¿™ä¸ªæœåŠ¡å™¨å’ŒåŸŸåä¾¿ä¸€ç›´è’åºŸäº†å¿«å¤§åŠå¹´ã€‚ç”±äºå¤§ä¸‰ä¸Šå­¦æœŸç»“æŸçš„éå¸¸ä¹‹æ—©ï¼Œæˆ‘äººç”Ÿä¸­ç¬¬ä¸€æ¬¡æ‹¥æœ‰äº†å°†è¿‘ä¸¤ä¸ªæœˆçš„å¯’å‡ã€‚è¶æ­¤æœºä¼šï¼Œæˆ‘å†³å®šå°†è¿™ä¸ªæœåŠ¡å™¨å…ˆåˆ©ç”¨èµ·æ¥ï¼Œäºæ˜¯å°±æœ‰äº†Astroblogã€‚</p>\n<h2 id=\"ç®€ä»‹\"><a href=\"#ç®€ä»‹\" class=\"headerlink\" title=\"ç®€ä»‹\"></a>ç®€ä»‹</h2><p>Astroblogä¸Šä¸»è¦å°†åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š</p>\n<ul>\n<li>åœ¨å­¦æ ¡çš„è¯¾ç¨‹æ€»ç»“ï¼Œç›®å‰è®¡åˆ’å°†æ€»ç»“çš„èµ„æ–™å…¨éƒ¨ç”µå­åŒ–ï¼ˆå°½é‡ï¼Œçœ‹å¿ƒæƒ…ğŸ˜‚ï¼‰</li>\n<li>ä¸è®¡ç®—æœºæŠ€æœ¯ç›¸å…³çš„æŠ€æœ¯æ€»ç»“ï¼Œæ¯”å¦‚ä¸€äº›æ•™ç¨‹æˆ–æ–¹æ³•ç­‰ï¼Œç”¨ä½œå¤‡å¿˜ï¼Œå¤§è‡´åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š<ul>\n<li>ç¼–ç¨‹è¯­è¨€çŸ¥è¯†æ€»ç»“</li>\n<li>è®¡ç®—æœºç½‘ç»œæŠ€æœ¯</li>\n<li>ç¨‹åºå¼€å‘</li>\n<li>é»‘è‹¹æœï£¿</li>\n</ul>\n</li>\n<li>ä¸ªäººæ‘„å½±ä½œå“ä»¥åŠå…¶ä»–ä¼˜ç§€æ‘„å½±ä½œå“çš„å±•è§ˆ</li>\n<li>ä¸€äº›æ°‘èˆªçŸ¥è¯†</li>\n<li>å…¶ä»–å†…å®¹</li>\n</ul>\n<h2 id=\"å…³äºAstrobear\"><a href=\"#å…³äºAstrobear\" class=\"headerlink\" title=\"å…³äºAstrobear\"></a>å…³äºAstrobear</h2><p>ç«™é•¿ç°åœ¨ï¼ˆ2020å¹´1æœˆï¼‰æ˜¯ä¸€ä¸ªå¤§ä¸‰å­¦ç”Ÿï¼Œä¸“ä¸šæ˜¯æ¢æµ‹åˆ¶å¯¼ä¸æ§åˆ¶æŠ€æœ¯ã€‚ä»å°çš„æ¢¦æƒ³æ˜¯æˆä¸ºä¸€åé£è¡Œå‘˜ï¼Œä½†æ˜¯é«˜ä¸‰ä½“æ£€è¢«åˆ·äº†ğŸ˜­ã€‚äºæ˜¯è¿·è¿·ç³Šç³Šå°±åˆ°äº†ç°åœ¨çš„å­¦æ ¡ï¼Œè¿›äº†ç°åœ¨çš„ä¸“ä¸šã€‚é«˜ä¸­çš„æ—¶å€™åŠ å…¥äº†å­¦æ ¡å¤©æ–‡ç¤¾ï¼Œä»é‚£æ—¶èµ·å–œæ¬¢ä¸Šäº†å¤©æ–‡ï¼ˆæ‘„å½±ï¼‰ã€‚æœ¬æ¸£åœ¨å¤§ä¸€ä¸Šå­¦æœŸå­¦äº†Cè¯­è¨€ä»¥åå¯¹è®¡ç®—æœºæŠ€æœ¯äº§ç”Ÿäº†å…´è¶£ï¼Œç„¶ååœ¨å¤§ä¸€æš‘å‡å‡ ä¹ç‹¬ç«‹å¼€å‘äº†<del>ï¼ˆåˆä¼™äººæ²‰è¿·acmå¤ªå¿™äº†ï¼‰</del>ä¸€ä¸ªç»™è‡ªå·±åœ¨çš„å­¦ç”Ÿç»„ç»‡ç”¨çš„ç®¡ç†ç±»å¾®ä¿¡å°ç¨‹åºï¼Œä¹‹ååœ¨â€œç¼–ç¨‹â€çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œã€‚</p>\n<p>æ€»ç»“ä¸€ä¸‹ï¼ŒAstrobearæ˜¯ï¼šå­¦æ§åˆ¶çš„æ¢¦æƒ³æˆä¸ºé£è¡Œå‘˜çš„å¤©æ–‡å’Œè®¡ç®—æœºçˆ±å¥½è€…ã€‚</p>\n<p>ç”±äºæˆ‘å¹¶ä¸æ˜¯è®¡ç®—æœºä¸“ä¸šçš„ï¼Œæ‰€ä»¥å…³äºè®¡ç®—æœºæŠ€æœ¯è¿™ä¸€å—æ˜¯æœ¬ç€â€œæ‹¿æ¥ä¸»ä¹‰â€çš„æ€åº¦å»å­¦çš„â€”â€”ä¼šç”¨å°±è¡Œã€‚å› æ­¤åœ¨è¿™æ–¹é¢éš¾å…ä¼šæœ‰ç–æ¼é”™è¯¯ä¹‹å¤„ï¼Œä¹Ÿè¯·å¤§å®¶æµ·æ¶µã€‚</p>\n<p>å¤šäºæœ‰äº†äº’è”ç½‘çš„å‘å±•ï¼ŒçŸ¥è¯†çš„ä¼ æ’­å¯ä»¥å¦‚æ­¤åœ°è¿…é€Ÿã€‚åœ¨æ­¤ï¼Œå¯¹ä¹‹å‰å¯¹æˆ‘æœ‰è¿‡å¸®åŠ©çš„åšä¸»è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼</p>\n"},{"title":"APIs of Multirotor in Airsim","date":"2020-01-15T15:40:00.000Z","thumbnail":"https://cn.bing.com/th?id=OIP.o6vbAWXSs3ffmE8NXNaZ4QHaEM&pid=Api&rs=1","_content":"\n### APIs of Multirotor in Airsim\n\nby Astrobear\n\n#### Preface\n\n- All APIs listed below need to add the suffix `.join()`. Actually, `.join()` is a call on Python's main process to wait for the thread to complete.\n- All APIs listed below has a hidden parameter, which is `vehicle_name`. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.\n- This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.\n\n#### Control APIs\n\n**`takeoffAsync(timeout_sec)`**: the multirotor will take off when this command is being executed. \n\n- `timeout_sec`: take off time, second. Better to greater than 3s but less than 10s.\n\n`hoverAsync()`: the multirotor will maintain its attitude when executed.\n\n**`landAsync(timeout_sec)`**: the multirotor will land when executed.\n\n- `timeout_sec`: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.\n\n**`goHomeAsync(timeout_sec)`**: the multirotor will fly back to its starting point automatically.\n\n- `timeout_sec`: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n\n**`moveByAngleZAsync(pitch, roll, z, yaw, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `z`: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.\n- `yaw`: angle of yaw, radian.\n- `duration`: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.\n\n**`moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `throttle`: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.\n- `yaw_rate`: angular velocity at yaw axis, radian per second.\n- `duration`: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.\n\n**`moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)`**: change the velocity of the multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `vz`: velocity projected at z axis, meter per second.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)`**: change the velocity at horizontal plane and the altitude of multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `z`: flight altitude, meter.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly according to several given coordinates.\n\n- `path`: a `Vector3r` array, which provides the route coordinates, meter. The form of it is `[airsim.Vector3r(x, y, z), ...]`.\n- `velocity`: flight velocity when traveling, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. \n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.\n\n- `x`: distance projected at x axis, meter.\n- `y`: distance projected at y axis, meter.\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.\n\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`rotateByYawRateAsync(yaw_rate, duration)`**: the multirotor will yaw at the given yaw rate.\n\n- `yaw_rate`: yawing angular velocity, degree per second. \n- `duration`: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.\n","source":"_posts/AirSimMultirotorAPIs.md","raw":"---\ntitle: APIs of Multirotor in Airsim\ndate: 2020-1-15 23:40:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- AirSim\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://cn.bing.com/th?id=OIP.o6vbAWXSs3ffmE8NXNaZ4QHaEM&pid=Api&rs=1\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\n#excerpt: ...\n\n#You can begin to input your article below now.\n\n---\n\n### APIs of Multirotor in Airsim\n\nby Astrobear\n\n#### Preface\n\n- All APIs listed below need to add the suffix `.join()`. Actually, `.join()` is a call on Python's main process to wait for the thread to complete.\n- All APIs listed below has a hidden parameter, which is `vehicle_name`. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.\n- This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.\n\n#### Control APIs\n\n**`takeoffAsync(timeout_sec)`**: the multirotor will take off when this command is being executed. \n\n- `timeout_sec`: take off time, second. Better to greater than 3s but less than 10s.\n\n`hoverAsync()`: the multirotor will maintain its attitude when executed.\n\n**`landAsync(timeout_sec)`**: the multirotor will land when executed.\n\n- `timeout_sec`: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.\n\n**`goHomeAsync(timeout_sec)`**: the multirotor will fly back to its starting point automatically.\n\n- `timeout_sec`: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n\n**`moveByAngleZAsync(pitch, roll, z, yaw, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `z`: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.\n- `yaw`: angle of yaw, radian.\n- `duration`: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.\n\n**`moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `throttle`: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.\n- `yaw_rate`: angular velocity at yaw axis, radian per second.\n- `duration`: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.\n\n**`moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)`**: change the velocity of the multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `vz`: velocity projected at z axis, meter per second.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)`**: change the velocity at horizontal plane and the altitude of multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `z`: flight altitude, meter.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly according to several given coordinates.\n\n- `path`: a `Vector3r` array, which provides the route coordinates, meter. The form of it is `[airsim.Vector3r(x, y, z), ...]`.\n- `velocity`: flight velocity when traveling, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. \n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.\n\n- `x`: distance projected at x axis, meter.\n- `y`: distance projected at y axis, meter.\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.\n\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`rotateByYawRateAsync(yaw_rate, duration)`**: the multirotor will yaw at the given yaw rate.\n\n- `yaw_rate`: yawing angular velocity, degree per second. \n- `duration`: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.\n","slug":"AirSimMultirotorAPIs","published":1,"updated":"2020-01-17T15:54:41.295Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck6ax1lg50001j1p27ayd7j78","content":"<h3 id=\"APIs-of-Multirotor-in-Airsim\"><a href=\"#APIs-of-Multirotor-in-Airsim\" class=\"headerlink\" title=\"APIs of Multirotor in Airsim\"></a>APIs of Multirotor in Airsim</h3><p>by Astrobear</p>\n<h4 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h4><ul>\n<li>All APIs listed below need to add the suffix <code>.join()</code>. Actually, <code>.join()</code> is a call on Pythonâ€™s main process to wait for the thread to complete.</li>\n<li>All APIs listed below has a hidden parameter, which is <code>vehicle_name</code>. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.</li>\n<li>This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.</li>\n</ul>\n<h4 id=\"Control-APIs\"><a href=\"#Control-APIs\" class=\"headerlink\" title=\"Control APIs\"></a>Control APIs</h4><p><strong><code>takeoffAsync(timeout_sec)</code></strong>: the multirotor will take off when this command is being executed. </p>\n<ul>\n<li><code>timeout_sec</code>: take off time, second. Better to greater than 3s but less than 10s.</li>\n</ul>\n<p><code>hoverAsync()</code>: the multirotor will maintain its attitude when executed.</p>\n<p><strong><code>landAsync(timeout_sec)</code></strong>: the multirotor will land when executed.</p>\n<ul>\n<li><code>timeout_sec</code>: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.</li>\n</ul>\n<p><strong><code>goHomeAsync(timeout_sec)</code></strong>: the multirotor will fly back to its starting point automatically.</p>\n<ul>\n<li><code>timeout_sec</code>: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n</ul>\n<p><strong><code>moveByAngleZAsync(pitch, roll, z, yaw, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>z</code>: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.</li>\n<li><code>yaw</code>: angle of yaw, radian.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.</li>\n</ul>\n<p><strong><code>moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>throttle</code>: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.</li>\n<li><code>yaw_rate</code>: angular velocity at yaw axis, radian per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.</li>\n</ul>\n<p><strong><code>moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)</code></strong>: change the velocity of the multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>vz</code>: velocity projected at z axis, meter per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)</code></strong>: change the velocity at horizontal plane and the altitude of multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly according to several given coordinates.</p>\n<ul>\n<li><code>path</code>: a <code>Vector3r</code> array, which provides the route coordinates, meter. The form of it is <code>[airsim.Vector3r(x, y, z), ...]</code>.</li>\n<li><code>velocity</code>: flight velocity when traveling, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. </li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.</p>\n<ul>\n<li><code>x</code>: distance projected at x axis, meter.</li>\n<li><code>y</code>: distance projected at y axis, meter.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.</p>\n<ul>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>rotateByYawRateAsync(yaw_rate, duration)</code></strong>: the multirotor will yaw at the given yaw rate.</p>\n<ul>\n<li><code>yaw_rate</code>: yawing angular velocity, degree per second. </li>\n<li><code>duration</code>: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"APIs-of-Multirotor-in-Airsim\"><a href=\"#APIs-of-Multirotor-in-Airsim\" class=\"headerlink\" title=\"APIs of Multirotor in Airsim\"></a>APIs of Multirotor in Airsim</h3><p>by Astrobear</p>\n<h4 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h4><ul>\n<li>All APIs listed below need to add the suffix <code>.join()</code>. Actually, <code>.join()</code> is a call on Pythonâ€™s main process to wait for the thread to complete.</li>\n<li>All APIs listed below has a hidden parameter, which is <code>vehicle_name</code>. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.</li>\n<li>This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.</li>\n</ul>\n<h4 id=\"Control-APIs\"><a href=\"#Control-APIs\" class=\"headerlink\" title=\"Control APIs\"></a>Control APIs</h4><p><strong><code>takeoffAsync(timeout_sec)</code></strong>: the multirotor will take off when this command is being executed. </p>\n<ul>\n<li><code>timeout_sec</code>: take off time, second. Better to greater than 3s but less than 10s.</li>\n</ul>\n<p><code>hoverAsync()</code>: the multirotor will maintain its attitude when executed.</p>\n<p><strong><code>landAsync(timeout_sec)</code></strong>: the multirotor will land when executed.</p>\n<ul>\n<li><code>timeout_sec</code>: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.</li>\n</ul>\n<p><strong><code>goHomeAsync(timeout_sec)</code></strong>: the multirotor will fly back to its starting point automatically.</p>\n<ul>\n<li><code>timeout_sec</code>: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n</ul>\n<p><strong><code>moveByAngleZAsync(pitch, roll, z, yaw, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>z</code>: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.</li>\n<li><code>yaw</code>: angle of yaw, radian.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.</li>\n</ul>\n<p><strong><code>moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>throttle</code>: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.</li>\n<li><code>yaw_rate</code>: angular velocity at yaw axis, radian per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.</li>\n</ul>\n<p><strong><code>moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)</code></strong>: change the velocity of the multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>vz</code>: velocity projected at z axis, meter per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)</code></strong>: change the velocity at horizontal plane and the altitude of multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly according to several given coordinates.</p>\n<ul>\n<li><code>path</code>: a <code>Vector3r</code> array, which provides the route coordinates, meter. The form of it is <code>[airsim.Vector3r(x, y, z), ...]</code>.</li>\n<li><code>velocity</code>: flight velocity when traveling, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. </li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.</p>\n<ul>\n<li><code>x</code>: distance projected at x axis, meter.</li>\n<li><code>y</code>: distance projected at y axis, meter.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.</p>\n<ul>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>rotateByYawRateAsync(yaw_rate, duration)</code></strong>: the multirotor will yaw at the given yaw rate.</p>\n<ul>\n<li><code>yaw_rate</code>: yawing angular velocity, degree per second. </li>\n<li><code>duration</code>: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.</li>\n</ul>\n"},{"title":"Summarize of Reinforcement Learning 3","date":"2020-02-01T09:12:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg","excerpt":"Introduction to MC and TD.","_content":"\n### Introduction\n\nIn the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss *model-free algorithms* in this article. \n\nThroughout this article, we will assume an *infinite horizon* as well as *stationary rewards, transition probabilities and policies*.\n\nFirst comes the definition of *history*: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: \n\n$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},...,s_{j,L_j})$, \n\nwhere $L_j$ is the length of the interaction (interaction between agent and environment). \n\nIn the article *Summarize of Reinforcement Learning 2* I introduced the *iterative solution* of value function, which is\n\n$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$\n\nâ€‹          $=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$.\n\nThis ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. \n\n### Monte Carlo on policy evaluation\n\nIn general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. \n\nIn reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: \n\n- Execute a rollout of policy until termination many times\n- Record the returns $G_t$ that we observe when starting at state $s$\n- Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. \n\nFigure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.\n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg)\n\n#### How to evaluate the good and bad of an algorithm\n\nWe use three quntities to evaluate the good and bad of an algorithm.\n\nConsider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and it's a function of observed data $x$. Then we have these quantities of the estimator: \n\nBias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, \n\nVariance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, \n\nMean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. \n\n#### First-Visit Monte Carlo\n\nHere is the algorithm of First-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $i$ `do`\n\nâ€‹\t `while` **first time $t$** that the state $s$ is visited in episode $h_i$ `do`\n\nâ€‹\t\t$N(s)=N(s)+1$\n\nâ€‹\t\t$G(s)=G(s)+G_{i,t}$\n\nâ€‹\t\t$V(s)=G(s)/N(s)$ \n\n`return` $V(s)$\n\nFirst-Visit Monte Carlo estimator is an unbised estimator.\n\n#### Every-Visit Monte Carlo\n\nHere is the algorithm of Every-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $h_i$ `do`\n\nâ€‹\t `while` **every time $t$** that the state $s$ is visited in episode $i$ `do`\n\nâ€‹\t\t$N(s)=N(s)+1$\n\nâ€‹\t\t$G(s)=G(s)+G_{i,t}$\n\nâ€‹\t\t$V(s)=G(s)/N(s)$ \n\n`return` $V(s)$\n\nEvery-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. \n\n#### Increment First-Visit/Every-Visit Monte Carlo\n\nWe can replace $V(s)=G(s)/N(s)$ in both two algorithms by \n\n$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nBecause\n\n${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nReplacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general *Incremental Monte Carlo on policy evaluation*. Setting $\\alpha > {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. \n\n### Temporal Difference (TD) Learning\n\nTD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. \n\nIn dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have \n\n$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, \n\nand this is the TD learning update. \n\nIn TD learning update, there are two concepts which are *TD error* and *TD target*. TD error is written as below: \n\n$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. \n\nAnd here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: \n\n$r_t+\\gamma V^\\pi(s_{t+1})$. \n\nThe algorithm of TD learning is shown below.\n\nInitialize $V^\\pi(s)=0,\\ s\\in S$\n\n`while` True `do`\n\nâ€‹\tSample tuple $(s_t,a_t,r_t,s_{t+1})$ \n\nâ€‹\t$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ \n\nIt is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that's why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.\n\nIn reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. \n\nFigure 2 shows a diagram expressing TD learning. \n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS3F2.png)\n\n### Summary\n\nTable below gives some fundamental properties of these three algorithms (DP, MC, TD). \n\n| Properties                                                   | DP   | MC                   | TD   |\n| ------------------------------------------------------------ | ---- | -------------------- | ---- |\n| Useble when no models of current domain                      | No   | Yes                  | Yes  |\n| Handles continuing domains (episodes will never terminate)   | Yes  | No                   | Yes  |\n| Handles Non-Markovian domains                                | No   | Yes                  | No   |\n| Coverges to true value in limit (satisfying some conditions) | Yes  | Yes                  | Yes  |\n| Unbised estimate of value                                    | N/A  | Yes (First-Visit MC) | No   |\n| Variance                                                     | N/A  | High                 | Low  |\n\nFigure 3 shows some other properties that may help us to choose the algorithm. \n\n![Figure 3](https://astrobear.top/resource/astroblog/content/RLS3F3.png)\n\n### Batch Monte Carlo and Temporal Difference\n\nThe batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. \n\nIn the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where\n\n![Figure 4](https://astrobear.top/resource/astroblog/content/RLS3F4.png). \n\nThe value function derived from the maximum likehood MDP model is known as the *certainty equivalence estimate*. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.","source":"_posts/RLSummarize3.md","raw":"---\ntitle: Summarize of Reinforcement Learning 3\ndate: 2020-2-1 17:12:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Introduction to MC and TD.\n\n#You can begin to input your article below now.\n\n---\n\n### Introduction\n\nIn the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss *model-free algorithms* in this article. \n\nThroughout this article, we will assume an *infinite horizon* as well as *stationary rewards, transition probabilities and policies*.\n\nFirst comes the definition of *history*: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: \n\n$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},...,s_{j,L_j})$, \n\nwhere $L_j$ is the length of the interaction (interaction between agent and environment). \n\nIn the article *Summarize of Reinforcement Learning 2* I introduced the *iterative solution* of value function, which is\n\n$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$\n\nâ€‹          $=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$.\n\nThis ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. \n\n### Monte Carlo on policy evaluation\n\nIn general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. \n\nIn reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: \n\n- Execute a rollout of policy until termination many times\n- Record the returns $G_t$ that we observe when starting at state $s$\n- Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. \n\nFigure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.\n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg)\n\n#### How to evaluate the good and bad of an algorithm\n\nWe use three quntities to evaluate the good and bad of an algorithm.\n\nConsider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and it's a function of observed data $x$. Then we have these quantities of the estimator: \n\nBias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, \n\nVariance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, \n\nMean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. \n\n#### First-Visit Monte Carlo\n\nHere is the algorithm of First-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $i$ `do`\n\nâ€‹\t `while` **first time $t$** that the state $s$ is visited in episode $h_i$ `do`\n\nâ€‹\t\t$N(s)=N(s)+1$\n\nâ€‹\t\t$G(s)=G(s)+G_{i,t}$\n\nâ€‹\t\t$V(s)=G(s)/N(s)$ \n\n`return` $V(s)$\n\nFirst-Visit Monte Carlo estimator is an unbised estimator.\n\n#### Every-Visit Monte Carlo\n\nHere is the algorithm of Every-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $h_i$ `do`\n\nâ€‹\t `while` **every time $t$** that the state $s$ is visited in episode $i$ `do`\n\nâ€‹\t\t$N(s)=N(s)+1$\n\nâ€‹\t\t$G(s)=G(s)+G_{i,t}$\n\nâ€‹\t\t$V(s)=G(s)/N(s)$ \n\n`return` $V(s)$\n\nEvery-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. \n\n#### Increment First-Visit/Every-Visit Monte Carlo\n\nWe can replace $V(s)=G(s)/N(s)$ in both two algorithms by \n\n$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nBecause\n\n${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nReplacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general *Incremental Monte Carlo on policy evaluation*. Setting $\\alpha > {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. \n\n### Temporal Difference (TD) Learning\n\nTD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. \n\nIn dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have \n\n$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, \n\nand this is the TD learning update. \n\nIn TD learning update, there are two concepts which are *TD error* and *TD target*. TD error is written as below: \n\n$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. \n\nAnd here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: \n\n$r_t+\\gamma V^\\pi(s_{t+1})$. \n\nThe algorithm of TD learning is shown below.\n\nInitialize $V^\\pi(s)=0,\\ s\\in S$\n\n`while` True `do`\n\nâ€‹\tSample tuple $(s_t,a_t,r_t,s_{t+1})$ \n\nâ€‹\t$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ \n\nIt is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that's why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.\n\nIn reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. \n\nFigure 2 shows a diagram expressing TD learning. \n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS3F2.png)\n\n### Summary\n\nTable below gives some fundamental properties of these three algorithms (DP, MC, TD). \n\n| Properties                                                   | DP   | MC                   | TD   |\n| ------------------------------------------------------------ | ---- | -------------------- | ---- |\n| Useble when no models of current domain                      | No   | Yes                  | Yes  |\n| Handles continuing domains (episodes will never terminate)   | Yes  | No                   | Yes  |\n| Handles Non-Markovian domains                                | No   | Yes                  | No   |\n| Coverges to true value in limit (satisfying some conditions) | Yes  | Yes                  | Yes  |\n| Unbised estimate of value                                    | N/A  | Yes (First-Visit MC) | No   |\n| Variance                                                     | N/A  | High                 | Low  |\n\nFigure 3 shows some other properties that may help us to choose the algorithm. \n\n![Figure 3](https://astrobear.top/resource/astroblog/content/RLS3F3.png)\n\n### Batch Monte Carlo and Temporal Difference\n\nThe batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. \n\nIn the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where\n\n![Figure 4](https://astrobear.top/resource/astroblog/content/RLS3F4.png). \n\nThe value function derived from the maximum likehood MDP model is known as the *certainty equivalence estimate*. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.","slug":"RLSummarize3","published":1,"updated":"2020-02-06T14:57:47.072Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck6ax1lg80003j1p26y1j8hbi","content":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss <em>model-free algorithms</em> in this article. </p>\n<p>Throughout this article, we will assume an <em>infinite horizon</em> as well as <em>stationary rewards, transition probabilities and policies</em>.</p>\n<p>First comes the definition of <em>history</em>: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: </p>\n<p>$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},â€¦,s_{j,L_j})$, </p>\n<p>where $L_j$ is the length of the interaction (interaction between agent and environment). </p>\n<p>In the article <em>Summarize of Reinforcement Learning 2</em> I introduced the <em>iterative solution</em> of value function, which is</p>\n<p>$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$</p>\n<p>â€‹          $=R(s)+\\gamma \\sum P(sâ€™|s)V_{t+1}(sâ€™), \\forall t=0,â€¦,H-1,V_H(s)=0$.</p>\n<p>This ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. </p>\n<h3 id=\"Monte-Carlo-on-policy-evaluation\"><a href=\"#Monte-Carlo-on-policy-evaluation\" class=\"headerlink\" title=\"Monte Carlo on policy evaluation\"></a>Monte Carlo on policy evaluation</h3><p>In general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. </p>\n<p>In reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: </p>\n<ul>\n<li>Execute a rollout of policy until termination many times</li>\n<li>Record the returns $G_t$ that we observe when starting at state $s$</li>\n<li>Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. </li>\n</ul>\n<p>Figure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg\" alt=\"Figure 1\"></p>\n<h4 id=\"How-to-evaluate-the-good-and-bad-of-an-algorithm\"><a href=\"#How-to-evaluate-the-good-and-bad-of-an-algorithm\" class=\"headerlink\" title=\"How to evaluate the good and bad of an algorithm\"></a>How to evaluate the good and bad of an algorithm</h4><p>We use three quntities to evaluate the good and bad of an algorithm.</p>\n<p>Consider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and itâ€™s a function of observed data $x$. Then we have these quantities of the estimator: </p>\n<p>Bias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, </p>\n<p>Variance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, </p>\n<p>Mean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. </p>\n<h4 id=\"First-Visit-Monte-Carlo\"><a href=\"#First-Visit-Monte-Carlo\" class=\"headerlink\" title=\"First-Visit Monte Carlo\"></a>First-Visit Monte Carlo</h4><p>Here is the algorithm of First-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>\n<p>â€‹     <code>while</code> <strong>first time $t$</strong> that the state $s$ is visited in episode $h_i$ <code>do</code></p>\n<p>â€‹        $N(s)=N(s)+1$</p>\n<p>â€‹        $G(s)=G(s)+G_{i,t}$</p>\n<p>â€‹        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$</p>\n<p>First-Visit Monte Carlo estimator is an unbised estimator.</p>\n<h4 id=\"Every-Visit-Monte-Carlo\"><a href=\"#Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Every-Visit Monte Carlo\"></a>Every-Visit Monte Carlo</h4><p>Here is the algorithm of Every-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $h_i$ <code>do</code></p>\n<p>â€‹     <code>while</code> <strong>every time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>\n<p>â€‹        $N(s)=N(s)+1$</p>\n<p>â€‹        $G(s)=G(s)+G_{i,t}$</p>\n<p>â€‹        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$</p>\n<p>Every-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. </p>\n<h4 id=\"Increment-First-Visit-Every-Visit-Monte-Carlo\"><a href=\"#Increment-First-Visit-Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Increment First-Visit/Every-Visit Monte Carlo\"></a>Increment First-Visit/Every-Visit Monte Carlo</h4><p>We can replace $V(s)=G(s)/N(s)$ in both two algorithms by </p>\n<p>$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Because</p>\n<p>${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Replacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general <em>Incremental Monte Carlo on policy evaluation</em>. Setting $\\alpha &gt; {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. </p>\n<h3 id=\"Temporal-Difference-TD-Learning\"><a href=\"#Temporal-Difference-TD-Learning\" class=\"headerlink\" title=\"Temporal Difference (TD) Learning\"></a>Temporal Difference (TD) Learning</h3><p>TD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. </p>\n<p>In dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have </p>\n<p>$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, </p>\n<p>and this is the TD learning update. </p>\n<p>In TD learning update, there are two concepts which are <em>TD error</em> and <em>TD target</em>. TD error is written as below: </p>\n<p>$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. </p>\n<p>And here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: </p>\n<p>$r_t+\\gamma V^\\pi(s_{t+1})$. </p>\n<p>The algorithm of TD learning is shown below.</p>\n<p>Initialize $V^\\pi(s)=0,\\ s\\in S$</p>\n<p><code>while</code> True <code>do</code></p>\n<p>â€‹    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ </p>\n<p>â€‹    $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ </p>\n<p>It is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus thatâ€™s why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.</p>\n<p>In reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. </p>\n<p>Figure 2 shows a diagram expressing TD learning. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F2.png\" alt=\"Figure 2\"></p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>Table below gives some fundamental properties of these three algorithms (DP, MC, TD). </p>\n<table>\n<thead>\n<tr>\n<th>Properties</th>\n<th>DP</th>\n<th>MC</th>\n<th>TD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Useble when no models of current domain</td>\n<td>No</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles continuing domains (episodes will never terminate)</td>\n<td>Yes</td>\n<td>No</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles Non-Markovian domains</td>\n<td>No</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Coverges to true value in limit (satisfying some conditions)</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Unbised estimate of value</td>\n<td>N/A</td>\n<td>Yes (First-Visit MC)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Variance</td>\n<td>N/A</td>\n<td>High</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>Figure 3 shows some other properties that may help us to choose the algorithm. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F3.png\" alt=\"Figure 3\"></p>\n<h3 id=\"Batch-Monte-Carlo-and-Temporal-Difference\"><a href=\"#Batch-Monte-Carlo-and-Temporal-Difference\" class=\"headerlink\" title=\"Batch Monte Carlo and Temporal Difference\"></a>Batch Monte Carlo and Temporal Difference</h3><p>The batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. </p>\n<p>In the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F4.png\" alt=\"Figure 4\">. </p>\n<p>The value function derived from the maximum likehood MDP model is known as the <em>certainty equivalence estimate</em>. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.</p>\n","site":{"data":{}},"more":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss <em>model-free algorithms</em> in this article. </p>\n<p>Throughout this article, we will assume an <em>infinite horizon</em> as well as <em>stationary rewards, transition probabilities and policies</em>.</p>\n<p>First comes the definition of <em>history</em>: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: </p>\n<p>$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},â€¦,s_{j,L_j})$, </p>\n<p>where $L_j$ is the length of the interaction (interaction between agent and environment). </p>\n<p>In the article <em>Summarize of Reinforcement Learning 2</em> I introduced the <em>iterative solution</em> of value function, which is</p>\n<p>$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$</p>\n<p>â€‹          $=R(s)+\\gamma \\sum P(sâ€™|s)V_{t+1}(sâ€™), \\forall t=0,â€¦,H-1,V_H(s)=0$.</p>\n<p>This ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. </p>\n<h3 id=\"Monte-Carlo-on-policy-evaluation\"><a href=\"#Monte-Carlo-on-policy-evaluation\" class=\"headerlink\" title=\"Monte Carlo on policy evaluation\"></a>Monte Carlo on policy evaluation</h3><p>In general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. </p>\n<p>In reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: </p>\n<ul>\n<li>Execute a rollout of policy until termination many times</li>\n<li>Record the returns $G_t$ that we observe when starting at state $s$</li>\n<li>Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. </li>\n</ul>\n<p>Figure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg\" alt=\"Figure 1\"></p>\n<h4 id=\"How-to-evaluate-the-good-and-bad-of-an-algorithm\"><a href=\"#How-to-evaluate-the-good-and-bad-of-an-algorithm\" class=\"headerlink\" title=\"How to evaluate the good and bad of an algorithm\"></a>How to evaluate the good and bad of an algorithm</h4><p>We use three quntities to evaluate the good and bad of an algorithm.</p>\n<p>Consider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and itâ€™s a function of observed data $x$. Then we have these quantities of the estimator: </p>\n<p>Bias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, </p>\n<p>Variance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, </p>\n<p>Mean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. </p>\n<h4 id=\"First-Visit-Monte-Carlo\"><a href=\"#First-Visit-Monte-Carlo\" class=\"headerlink\" title=\"First-Visit Monte Carlo\"></a>First-Visit Monte Carlo</h4><p>Here is the algorithm of First-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>\n<p>â€‹     <code>while</code> <strong>first time $t$</strong> that the state $s$ is visited in episode $h_i$ <code>do</code></p>\n<p>â€‹        $N(s)=N(s)+1$</p>\n<p>â€‹        $G(s)=G(s)+G_{i,t}$</p>\n<p>â€‹        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$</p>\n<p>First-Visit Monte Carlo estimator is an unbised estimator.</p>\n<h4 id=\"Every-Visit-Monte-Carlo\"><a href=\"#Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Every-Visit Monte Carlo\"></a>Every-Visit Monte Carlo</h4><p>Here is the algorithm of Every-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $h_i$ <code>do</code></p>\n<p>â€‹     <code>while</code> <strong>every time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>\n<p>â€‹        $N(s)=N(s)+1$</p>\n<p>â€‹        $G(s)=G(s)+G_{i,t}$</p>\n<p>â€‹        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$</p>\n<p>Every-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. </p>\n<h4 id=\"Increment-First-Visit-Every-Visit-Monte-Carlo\"><a href=\"#Increment-First-Visit-Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Increment First-Visit/Every-Visit Monte Carlo\"></a>Increment First-Visit/Every-Visit Monte Carlo</h4><p>We can replace $V(s)=G(s)/N(s)$ in both two algorithms by </p>\n<p>$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Because</p>\n<p>${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Replacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general <em>Incremental Monte Carlo on policy evaluation</em>. Setting $\\alpha &gt; {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. </p>\n<h3 id=\"Temporal-Difference-TD-Learning\"><a href=\"#Temporal-Difference-TD-Learning\" class=\"headerlink\" title=\"Temporal Difference (TD) Learning\"></a>Temporal Difference (TD) Learning</h3><p>TD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. </p>\n<p>In dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have </p>\n<p>$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, </p>\n<p>and this is the TD learning update. </p>\n<p>In TD learning update, there are two concepts which are <em>TD error</em> and <em>TD target</em>. TD error is written as below: </p>\n<p>$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. </p>\n<p>And here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: </p>\n<p>$r_t+\\gamma V^\\pi(s_{t+1})$. </p>\n<p>The algorithm of TD learning is shown below.</p>\n<p>Initialize $V^\\pi(s)=0,\\ s\\in S$</p>\n<p><code>while</code> True <code>do</code></p>\n<p>â€‹    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ </p>\n<p>â€‹    $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ </p>\n<p>It is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus thatâ€™s why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.</p>\n<p>In reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. </p>\n<p>Figure 2 shows a diagram expressing TD learning. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F2.png\" alt=\"Figure 2\"></p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>Table below gives some fundamental properties of these three algorithms (DP, MC, TD). </p>\n<table>\n<thead>\n<tr>\n<th>Properties</th>\n<th>DP</th>\n<th>MC</th>\n<th>TD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Useble when no models of current domain</td>\n<td>No</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles continuing domains (episodes will never terminate)</td>\n<td>Yes</td>\n<td>No</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles Non-Markovian domains</td>\n<td>No</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Coverges to true value in limit (satisfying some conditions)</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Unbised estimate of value</td>\n<td>N/A</td>\n<td>Yes (First-Visit MC)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Variance</td>\n<td>N/A</td>\n<td>High</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>Figure 3 shows some other properties that may help us to choose the algorithm. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F3.png\" alt=\"Figure 3\"></p>\n<h3 id=\"Batch-Monte-Carlo-and-Temporal-Difference\"><a href=\"#Batch-Monte-Carlo-and-Temporal-Difference\" class=\"headerlink\" title=\"Batch Monte Carlo and Temporal Difference\"></a>Batch Monte Carlo and Temporal Difference</h3><p>The batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. </p>\n<p>In the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F4.png\" alt=\"Figure 4\">. </p>\n<p>The value function derived from the maximum likehood MDP model is known as the <em>certainty equivalence estimate</em>. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.</p>\n"},{"title":"Summarize of Reinforcement Learning 2","date":"2020-01-18T13:06:00.000Z","thumbnail":"https://pic1.zhimg.com/80/v2-e1e894383536e4ff019f63e5507c2a18_hd.png","excerpt":"Introduction to MP, MRP, and MDP.","_content":"\n### Markov process (MP)\n\nMarkov process is a stochastic process that satisfies the Markov property, which means it is \"memoryless\" and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. \n\nWe need to make two assumptions before we define the Markov process. The first assumption is that *the state of MP is finite*, and we have $s_i\\in S, i\\in1,2,...$ , where $|S|<\\infty$. The second assumption is that *the transition probabilities are time independent*. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,...$.\n\nBase on these two assumption, we can define a *transition transform matrix*:\n\n![](https://astrobear.top/resource/astroblog/content/RLS2F0.png)\n\nThe size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.\n\nHenceforth, we can define a Markov process using a tuple $(S,\\bf P)$.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n\nBy calculating $S\\bf P$ we can get the distribution of the new state.\n\nFigure 1 shows a student MP example.\n\n![Figure 1](https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png)\n\n### Markov reward process (MRP)\n\nMRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explaintions.\n\n#### Reward function\n\nWhen we are moving from the current state $s$ to a *successor state* $s'$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s'$ ). For a state $s\\in S$, we define the expected reward by\n\n$R(s)=\\Bbb E[r_t|s_t=s]$. \n\nHere we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.\n\n#### Horizon\n\nIt is defined as the number of time steps in each episode of the process. An *episode* is the whole process of a round of training. The horizon can be finite or infinite.\n\n#### Return\n\nThe return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon *H*. We can calculate the return using\n\n$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.\n\n#### State value function\n\nThe state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression\n\n$V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\nIf the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.\n\n#### Discount factor\n\nWe design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma <1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.\n\nFigure 2 and 3 shows an example of how to calculate the return.\n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS2F2.png)\n\n![Figure 3](https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg)\n\nIt is significant to find out a value function while many problems of RL is how to get a value function essentially.\n\n#### Computing the value function\n\nWe have three ways to compute the value function.\n\n- Simulation. Through simulation, we can get the value function by averaing many returns of episodes.\n\n- Analytic solution. We have defined the state value function \n\n  $V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\n  Then, make a little transformation, see Figure 4 in detail. \n\n  ![Figure 4](https://astrobear.top/resource/astroblog/content/RLS2F4.png)\n\n  Then, we have\n\n  $V(s)=R(s)+\\gamma \\sum P(s'|s)V(s')$, \n\n  \n\n  $V=R+\\gamma\\bf P\\mit V$. \n\n  Therefore we have\n\n  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. \n\n  If $0<\\gamma<1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.\n\n  Notice that $s'$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.\n\n  ![Figure 5](https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png)\n\n- Iterative solution. \n\n  $V_t(s)=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$. \n\n  We can iterate it again and again and use $|V_t-V_{t-1}|<\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. \n\n### Markov decision process (MDP)\n\nMDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. \n\n- $S$: A finite state space.\n- $A$: A finite set of actions which are available from each state $s$.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explanations.\n\n#### Notifications\n\n- Both $S$ and $A$ are finite.\n\n- In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as\n\n  $P(s_{t+1}|s_t,a_t)$.\n\n- In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as\n\n  $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n- Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.\n\n#### Policy\n\nBefore we mention the state value function, we need to talk about the policy for the MDP first. \n\nA policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be *varying with time*, especially when the horizon is finite. A policy can be written as\n\n$\\bf \\pi\\mit(a|s)=P(a_t=a|s_t=s)$. \n\nIf given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: \n\n- $P^\\pi(s'|s)=\\sum_{a\\in A}\\pi(a|s) P(s'|s,a)$\n\n  When we have a policy $\\pi$, the probability of the state transforms from $s$ to $s'$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s'$ when executing an action $a$.\n\n- $R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$\n\n  When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.\n\n#### Value functions in MDP (Bellman expectation equations)\n\nGiven a policy $\\pi$ can define two quantities: *the state value function* and *the state-action value function*. These two value functions are both *Bellman expectation equations*.\n\n- State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression\n\n  $V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. \n\n  Frequently we will drop the subscript $\\pi$ in the expectation. \n\n- State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form\n\n  $Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. \n\n  It evaluates the value of acting the action $a$ under current state $s$. \n\nNow let's talk about the relationships between these two value functions.\n\nFigure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.\n\n![Figure 6](https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png)\n\nWe can discover that the value of a state can be denoted as\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.\n\nIn a similar way, Figure 7 shows what states that an action can lead to.\n\n![Figure 7](https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png)\n\nWe can also find that \n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^\\pi(s')$. \n\nOn the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s'$ and the probability of getting into that new state. \n\nIf we combine the two Bellman equation with each other, we can get\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^\\pi(s')]$\n\nâ€‹            $=R(s',\\pi(s'))+\\gamma\\sum_{s'\\in S}P(s'|s,\\pi(s)) V^\\pi(s')$, \n\nand\n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)\\sum_{a\\in A}\\pi(a'|s')Q_\\pi(s',a')$. \n\nThe example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.\n\n![Figure 8](https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png)\n\n#### Optimality value function (Bellman optimality equation)\n\n- Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. \n- Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.\n\nOptimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. \n\n### Find the best policy\n\nThe best policy is defined precisely as *optimal policy*  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.\n\nFor an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.\n\nWe can compute the optimal policy by\n\n$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,\n\nWhich means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. \n\nIf an optimal policy exists then its value function must be a fixed point of the operator $B^*$. \n\n#### Bellman optimality backup operator\n\nBellman optimality backup operator is written as $B^*$ with a value function behind it \n\n$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V(s')$. \n\nIf $\\gamma<1$, $B^*$ is a strict contraction and has a unique fixed point. This means \n\n$B^*V(s)\\geq V^\\pi(s)$.\n\nBellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.\n\nNext I'll briefly introduce some algorithms to compute the optimal value function and an optimal policy.\n\n#### Policy search\n\nThis algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a $V^*(s)$  and  $\\pi^*(s)$. \n\n#### Policy iteration\n\nThe algorithm of policy iteration is shown below: \n\n`while` True `do`\n\nâ€‹\t$V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)\n\nâ€‹\t$\\pi^*$ = Policy improvement $(M,V^\\pi)$\n\n`if` $\\pi^*(s)=\\pi(s)$ `then`\n\nâ€‹\t`break`\n\n`else`\n\nâ€‹\t$\\pi$ = $\\pi^*$\n\n$V^*$ = $V^\\pi$ . \n\nPolicy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute\n\n$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^{\\pi i}(s')$ \n\nfor all the $a$ and $s$ and then take the max\n\n`return` $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.\n\nNotice that there is a relationship\n\n$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.\n\nThis means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.\n\n#### Value iteration\n\nThe algorithm of value iteration is shown below:\n\n$V'(s)=0, V(s)=\\infty$, for all $s\\in S$\n\n`while` $||V-V'||_\\infty>\\epsilon$ `do`\n\nâ€‹\t$V=V'$\n\nâ€‹\t$V'(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V'(s)$, for all states $s\\in S$ \n\n$V^*=V$, for all $s\\in S$ \n\n$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^ *(s'),\\ \\forall s\\in S$ . \n\nThe idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.\n\n","source":"_posts/RLSummarize2.md","raw":"---\ntitle: Summarize of Reinforcement Learning 2\ndate: 2020-1-18 21:06:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://pic1.zhimg.com/80/v2-e1e894383536e4ff019f63e5507c2a18_hd.png\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Introduction to MP, MRP, and MDP.\n\n#You can begin to input your article below now.\n\n---\n\n### Markov process (MP)\n\nMarkov process is a stochastic process that satisfies the Markov property, which means it is \"memoryless\" and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. \n\nWe need to make two assumptions before we define the Markov process. The first assumption is that *the state of MP is finite*, and we have $s_i\\in S, i\\in1,2,...$ , where $|S|<\\infty$. The second assumption is that *the transition probabilities are time independent*. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,...$.\n\nBase on these two assumption, we can define a *transition transform matrix*:\n\n![](https://astrobear.top/resource/astroblog/content/RLS2F0.png)\n\nThe size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.\n\nHenceforth, we can define a Markov process using a tuple $(S,\\bf P)$.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n\nBy calculating $S\\bf P$ we can get the distribution of the new state.\n\nFigure 1 shows a student MP example.\n\n![Figure 1](https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png)\n\n### Markov reward process (MRP)\n\nMRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explaintions.\n\n#### Reward function\n\nWhen we are moving from the current state $s$ to a *successor state* $s'$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s'$ ). For a state $s\\in S$, we define the expected reward by\n\n$R(s)=\\Bbb E[r_t|s_t=s]$. \n\nHere we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.\n\n#### Horizon\n\nIt is defined as the number of time steps in each episode of the process. An *episode* is the whole process of a round of training. The horizon can be finite or infinite.\n\n#### Return\n\nThe return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon *H*. We can calculate the return using\n\n$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.\n\n#### State value function\n\nThe state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression\n\n$V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\nIf the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.\n\n#### Discount factor\n\nWe design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma <1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.\n\nFigure 2 and 3 shows an example of how to calculate the return.\n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS2F2.png)\n\n![Figure 3](https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg)\n\nIt is significant to find out a value function while many problems of RL is how to get a value function essentially.\n\n#### Computing the value function\n\nWe have three ways to compute the value function.\n\n- Simulation. Through simulation, we can get the value function by averaing many returns of episodes.\n\n- Analytic solution. We have defined the state value function \n\n  $V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\n  Then, make a little transformation, see Figure 4 in detail. \n\n  ![Figure 4](https://astrobear.top/resource/astroblog/content/RLS2F4.png)\n\n  Then, we have\n\n  $V(s)=R(s)+\\gamma \\sum P(s'|s)V(s')$, \n\n  \n\n  $V=R+\\gamma\\bf P\\mit V$. \n\n  Therefore we have\n\n  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. \n\n  If $0<\\gamma<1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.\n\n  Notice that $s'$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.\n\n  ![Figure 5](https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png)\n\n- Iterative solution. \n\n  $V_t(s)=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$. \n\n  We can iterate it again and again and use $|V_t-V_{t-1}|<\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. \n\n### Markov decision process (MDP)\n\nMDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. \n\n- $S$: A finite state space.\n- $A$: A finite set of actions which are available from each state $s$.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explanations.\n\n#### Notifications\n\n- Both $S$ and $A$ are finite.\n\n- In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as\n\n  $P(s_{t+1}|s_t,a_t)$.\n\n- In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as\n\n  $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n- Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.\n\n#### Policy\n\nBefore we mention the state value function, we need to talk about the policy for the MDP first. \n\nA policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be *varying with time*, especially when the horizon is finite. A policy can be written as\n\n$\\bf \\pi\\mit(a|s)=P(a_t=a|s_t=s)$. \n\nIf given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: \n\n- $P^\\pi(s'|s)=\\sum_{a\\in A}\\pi(a|s) P(s'|s,a)$\n\n  When we have a policy $\\pi$, the probability of the state transforms from $s$ to $s'$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s'$ when executing an action $a$.\n\n- $R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$\n\n  When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.\n\n#### Value functions in MDP (Bellman expectation equations)\n\nGiven a policy $\\pi$ can define two quantities: *the state value function* and *the state-action value function*. These two value functions are both *Bellman expectation equations*.\n\n- State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression\n\n  $V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. \n\n  Frequently we will drop the subscript $\\pi$ in the expectation. \n\n- State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form\n\n  $Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. \n\n  It evaluates the value of acting the action $a$ under current state $s$. \n\nNow let's talk about the relationships between these two value functions.\n\nFigure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.\n\n![Figure 6](https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png)\n\nWe can discover that the value of a state can be denoted as\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.\n\nIn a similar way, Figure 7 shows what states that an action can lead to.\n\n![Figure 7](https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png)\n\nWe can also find that \n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^\\pi(s')$. \n\nOn the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s'$ and the probability of getting into that new state. \n\nIf we combine the two Bellman equation with each other, we can get\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^\\pi(s')]$\n\nâ€‹            $=R(s',\\pi(s'))+\\gamma\\sum_{s'\\in S}P(s'|s,\\pi(s)) V^\\pi(s')$, \n\nand\n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)\\sum_{a\\in A}\\pi(a'|s')Q_\\pi(s',a')$. \n\nThe example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.\n\n![Figure 8](https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png)\n\n#### Optimality value function (Bellman optimality equation)\n\n- Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. \n- Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.\n\nOptimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. \n\n### Find the best policy\n\nThe best policy is defined precisely as *optimal policy*  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.\n\nFor an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.\n\nWe can compute the optimal policy by\n\n$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,\n\nWhich means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. \n\nIf an optimal policy exists then its value function must be a fixed point of the operator $B^*$. \n\n#### Bellman optimality backup operator\n\nBellman optimality backup operator is written as $B^*$ with a value function behind it \n\n$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V(s')$. \n\nIf $\\gamma<1$, $B^*$ is a strict contraction and has a unique fixed point. This means \n\n$B^*V(s)\\geq V^\\pi(s)$.\n\nBellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.\n\nNext I'll briefly introduce some algorithms to compute the optimal value function and an optimal policy.\n\n#### Policy search\n\nThis algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a $V^*(s)$  and  $\\pi^*(s)$. \n\n#### Policy iteration\n\nThe algorithm of policy iteration is shown below: \n\n`while` True `do`\n\nâ€‹\t$V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)\n\nâ€‹\t$\\pi^*$ = Policy improvement $(M,V^\\pi)$\n\n`if` $\\pi^*(s)=\\pi(s)$ `then`\n\nâ€‹\t`break`\n\n`else`\n\nâ€‹\t$\\pi$ = $\\pi^*$\n\n$V^*$ = $V^\\pi$ . \n\nPolicy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute\n\n$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^{\\pi i}(s')$ \n\nfor all the $a$ and $s$ and then take the max\n\n`return` $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.\n\nNotice that there is a relationship\n\n$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.\n\nThis means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.\n\n#### Value iteration\n\nThe algorithm of value iteration is shown below:\n\n$V'(s)=0, V(s)=\\infty$, for all $s\\in S$\n\n`while` $||V-V'||_\\infty>\\epsilon$ `do`\n\nâ€‹\t$V=V'$\n\nâ€‹\t$V'(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V'(s)$, for all states $s\\in S$ \n\n$V^*=V$, for all $s\\in S$ \n\n$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^ *(s'),\\ \\forall s\\in S$ . \n\nThe idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.\n\n","slug":"RLSummarize2","published":1,"updated":"2020-02-06T16:11:35.224Z","_id":"ck6ax1lgh0006j1p22s0p8u3s","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"Markov-process-MP\"><a href=\"#Markov-process-MP\" class=\"headerlink\" title=\"Markov process (MP)\"></a>Markov process (MP)</h3><p>Markov process is a stochastic process that satisfies the Markov property, which means it is â€œmemorylessâ€ and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. </p>\n<p>We need to make two assumptions before we define the Markov process. The first assumption is that <em>the state of MP is finite</em>, and we have $s_i\\in S, i\\in1,2,â€¦$ , where $|S|&lt;\\infty$. The second assumption is that <em>the transition probabilities are time independent</em>. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,â€¦$.</p>\n<p>Base on these two assumption, we can define a <em>transition transform matrix</em>:</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F0.png\" alt=\"\"></p>\n<p>The size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.</p>\n<p>Henceforth, we can define a Markov process using a tuple $(S,\\bf P)$.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n</ul>\n<p>By calculating $S\\bf P$ we can get the distribution of the new state.</p>\n<p>Figure 1 shows a student MP example.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png\" alt=\"Figure 1\"></p>\n<h3 id=\"Markov-reward-process-MRP\"><a href=\"#Markov-reward-process-MRP\" class=\"headerlink\" title=\"Markov reward process (MRP)\"></a>Markov reward process (MRP)</h3><p>MRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explaintions.</p>\n<h4 id=\"Reward-function\"><a href=\"#Reward-function\" class=\"headerlink\" title=\"Reward function\"></a>Reward function</h4><p>When we are moving from the current state $s$ to a <em>successor state</em> $sâ€™$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $sâ€™$ ). For a state $s\\in S$, we define the expected reward by</p>\n<p>$R(s)=\\Bbb E[r_t|s_t=s]$. </p>\n<p>Here we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.</p>\n<h4 id=\"Horizon\"><a href=\"#Horizon\" class=\"headerlink\" title=\"Horizon\"></a>Horizon</h4><p>It is defined as the number of time steps in each episode of the process. An <em>episode</em> is the whole process of a round of training. The horizon can be finite or infinite.</p>\n<h4 id=\"Return\"><a href=\"#Return\" class=\"headerlink\" title=\"Return\"></a>Return</h4><p>The return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon <em>H</em>. We can calculate the return using</p>\n<p>$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.</p>\n<h4 id=\"State-value-function\"><a href=\"#State-value-function\" class=\"headerlink\" title=\"State value function\"></a>State value function</h4><p>The state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression</p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>If the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.</p>\n<h4 id=\"Discount-factor\"><a href=\"#Discount-factor\" class=\"headerlink\" title=\"Discount factor\"></a>Discount factor</h4><p>We design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma &lt;1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.</p>\n<p>Figure 2 and 3 shows an example of how to calculate the return.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F2.png\" alt=\"Figure 2\"></p>\n<p><img src=\"https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg\" alt=\"Figure 3\"></p>\n<p>It is significant to find out a value function while many problems of RL is how to get a value function essentially.</p>\n<h4 id=\"Computing-the-value-function\"><a href=\"#Computing-the-value-function\" class=\"headerlink\" title=\"Computing the value function\"></a>Computing the value function</h4><p>We have three ways to compute the value function.</p>\n<ul>\n<li><p>Simulation. Through simulation, we can get the value function by averaing many returns of episodes.</p>\n</li>\n<li><p>Analytic solution. We have defined the state value function </p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>Then, make a little transformation, see Figure 4 in detail. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F4.png\" alt=\"Figure 4\"></p>\n<p>Then, we have</p>\n<p>$V(s)=R(s)+\\gamma \\sum P(sâ€™|s)V(sâ€™)$, </p>\n</li>\n</ul>\n<p>  $V=R+\\gamma\\bf P\\mit V$. </p>\n<p>  Therefore we have</p>\n<p>  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. </p>\n<p>  If $0&lt;\\gamma&lt;1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.</p>\n<p>  Notice that $sâ€™$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.</p>\n<p>  <img src=\"https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png\" alt=\"Figure 5\"></p>\n<ul>\n<li><p>Iterative solution. </p>\n<p>$V_t(s)=R(s)+\\gamma \\sum P(sâ€™|s)V_{t+1}(sâ€™), \\forall t=0,â€¦,H-1,V_H(s)=0$. </p>\n<p>We can iterate it again and again and use $|V_t-V_{t-1}|&lt;\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. </p>\n</li>\n</ul>\n<h3 id=\"Markov-decision-process-MDP\"><a href=\"#Markov-decision-process-MDP\" class=\"headerlink\" title=\"Markov decision process (MDP)\"></a>Markov decision process (MDP)</h3><p>MDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. </p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$A$: A finite set of actions which are available from each state $s$.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explanations.</p>\n<h4 id=\"Notifications\"><a href=\"#Notifications\" class=\"headerlink\" title=\"Notifications\"></a>Notifications</h4><ul>\n<li><p>Both $S$ and $A$ are finite.</p>\n</li>\n<li><p>In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as</p>\n<p>$P(s_{t+1}|s_t,a_t)$.</p>\n</li>\n<li><p>In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as</p>\n<p>$R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n</li>\n<li><p>Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.</p>\n</li>\n</ul>\n<h4 id=\"Policy\"><a href=\"#Policy\" class=\"headerlink\" title=\"Policy\"></a>Policy</h4><p>Before we mention the state value function, we need to talk about the policy for the MDP first. </p>\n<p>A policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be <em>varying with time</em>, especially when the horizon is finite. A policy can be written as</p>\n<p>$\\bf \\pi\\mit(a|s)=P(a_t=a|s_t=s)$. </p>\n<p>If given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: </p>\n<ul>\n<li><p>$P^\\pi(sâ€™|s)=\\sum_{a\\in A}\\pi(a|s) P(sâ€™|s,a)$</p>\n<p>When we have a policy $\\pi$, the probability of the state transforms from $s$ to $sâ€™$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $sâ€™$ when executing an action $a$.</p>\n</li>\n<li><p>$R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$</p>\n<p>When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.</p>\n</li>\n</ul>\n<h4 id=\"Value-functions-in-MDP-Bellman-expectation-equations\"><a href=\"#Value-functions-in-MDP-Bellman-expectation-equations\" class=\"headerlink\" title=\"Value functions in MDP (Bellman expectation equations)\"></a>Value functions in MDP (Bellman expectation equations)</h4><p>Given a policy $\\pi$ can define two quantities: <em>the state value function</em> and <em>the state-action value function</em>. These two value functions are both <em>Bellman expectation equations</em>.</p>\n<ul>\n<li><p>State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression</p>\n<p>$V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. </p>\n<p>Frequently we will drop the subscript $\\pi$ in the expectation. </p>\n</li>\n<li><p>State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form</p>\n<p>$Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. </p>\n<p>It evaluates the value of acting the action $a$ under current state $s$. </p>\n</li>\n</ul>\n<p>Now letâ€™s talk about the relationships between these two value functions.</p>\n<p>Figure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png\" alt=\"Figure 6\"></p>\n<p>We can discover that the value of a state can be denoted as</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.</p>\n<p>In a similar way, Figure 7 shows what states that an action can lead to.</p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png\" alt=\"Figure 7\"></p>\n<p>We can also find that </p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^\\pi(sâ€™)$. </p>\n<p>On the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $sâ€™$ and the probability of getting into that new state. </p>\n<p>If we combine the two Bellman equation with each other, we can get</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V^\\pi(sâ€™)]$</p>\n<p>â€‹            $=R(sâ€™,\\pi(sâ€™))+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,\\pi(s)) V^\\pi(sâ€™)$, </p>\n<p>and</p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)\\sum_{a\\in A}\\pi(aâ€™|sâ€™)Q_\\pi(sâ€™,aâ€™)$. </p>\n<p>The example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png\" alt=\"Figure 8\"></p>\n<h4 id=\"Optimality-value-function-Bellman-optimality-equation\"><a href=\"#Optimality-value-function-Bellman-optimality-equation\" class=\"headerlink\" title=\"Optimality value function (Bellman optimality equation)\"></a>Optimality value function (Bellman optimality equation)</h4><ul>\n<li>Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. </li>\n<li>Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.</li>\n</ul>\n<p>Optimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. </p>\n<h3 id=\"Find-the-best-policy\"><a href=\"#Find-the-best-policy\" class=\"headerlink\" title=\"Find the best policy\"></a>Find the best policy</h3><p>The best policy is defined precisely as <em>optimal policy</em>  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.</p>\n<p>For an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.</p>\n<p>We can compute the optimal policy by</p>\n<p>$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,</p>\n<p>Which means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. </p>\n<p>If an optimal policy exists then its value function must be a fixed point of the operator $B^*$. </p>\n<h4 id=\"Bellman-optimality-backup-operator\"><a href=\"#Bellman-optimality-backup-operator\" class=\"headerlink\" title=\"Bellman optimality backup operator\"></a>Bellman optimality backup operator</h4><p>Bellman optimality backup operator is written as $B^*$ with a value function behind it </p>\n<p>$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V(sâ€™)$. </p>\n<p>If $\\gamma&lt;1$, $B^*$ is a strict contraction and has a unique fixed point. This means </p>\n<p>$B^*V(s)\\geq V^\\pi(s)$.</p>\n<p>Bellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.</p>\n<p>Next Iâ€™ll briefly introduce some algorithms to compute the optimal value function and an optimal policy.</p>\n<h4 id=\"Policy-search\"><a href=\"#Policy-search\" class=\"headerlink\" title=\"Policy search\"></a>Policy search</h4><p>This algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a $V^<em>(s)$  and  $\\pi^</em>(s)$. </p>\n<h4 id=\"Policy-iteration\"><a href=\"#Policy-iteration\" class=\"headerlink\" title=\"Policy iteration\"></a>Policy iteration</h4><p>The algorithm of policy iteration is shown below: </p>\n<p><code>while</code> True <code>do</code></p>\n<p>â€‹    $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)</p>\n<p>â€‹    $\\pi^*$ = Policy improvement $(M,V^\\pi)$</p>\n<p><code>if</code> $\\pi^*(s)=\\pi(s)$ <code>then</code></p>\n<p>â€‹    <code>break</code></p>\n<p><code>else</code></p>\n<p>â€‹    $\\pi$ = $\\pi^*$</p>\n<p>$V^*$ = $V^\\pi$ . </p>\n<p>Policy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute</p>\n<p>$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^{\\pi i}(sâ€™)$ </p>\n<p>for all the $a$ and $s$ and then take the max</p>\n<p><code>return</code> $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.</p>\n<p>Notice that there is a relationship</p>\n<p>$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.</p>\n<p>This means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.</p>\n<h4 id=\"Value-iteration\"><a href=\"#Value-iteration\" class=\"headerlink\" title=\"Value iteration\"></a>Value iteration</h4><p>The algorithm of value iteration is shown below:</p>\n<p>$Vâ€™(s)=0, V(s)=\\infty$, for all $s\\in S$</p>\n<p><code>while</code> $||V-Vâ€™||_\\infty&gt;\\epsilon$ <code>do</code></p>\n<p>â€‹    $V=Vâ€™$</p>\n<p>â€‹    $Vâ€™(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)Vâ€™(s)$, for all states $s\\in S$ </p>\n<p>$V^*=V$, for all $s\\in S$ </p>\n<p>$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V^ *(sâ€™),\\ \\forall s\\in S$ . </p>\n<p>The idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.</p>\n","site":{"data":{}},"more":"<h3 id=\"Markov-process-MP\"><a href=\"#Markov-process-MP\" class=\"headerlink\" title=\"Markov process (MP)\"></a>Markov process (MP)</h3><p>Markov process is a stochastic process that satisfies the Markov property, which means it is â€œmemorylessâ€ and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. </p>\n<p>We need to make two assumptions before we define the Markov process. The first assumption is that <em>the state of MP is finite</em>, and we have $s_i\\in S, i\\in1,2,â€¦$ , where $|S|&lt;\\infty$. The second assumption is that <em>the transition probabilities are time independent</em>. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,â€¦$.</p>\n<p>Base on these two assumption, we can define a <em>transition transform matrix</em>:</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F0.png\" alt=\"\"></p>\n<p>The size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.</p>\n<p>Henceforth, we can define a Markov process using a tuple $(S,\\bf P)$.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n</ul>\n<p>By calculating $S\\bf P$ we can get the distribution of the new state.</p>\n<p>Figure 1 shows a student MP example.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png\" alt=\"Figure 1\"></p>\n<h3 id=\"Markov-reward-process-MRP\"><a href=\"#Markov-reward-process-MRP\" class=\"headerlink\" title=\"Markov reward process (MRP)\"></a>Markov reward process (MRP)</h3><p>MRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explaintions.</p>\n<h4 id=\"Reward-function\"><a href=\"#Reward-function\" class=\"headerlink\" title=\"Reward function\"></a>Reward function</h4><p>When we are moving from the current state $s$ to a <em>successor state</em> $sâ€™$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $sâ€™$ ). For a state $s\\in S$, we define the expected reward by</p>\n<p>$R(s)=\\Bbb E[r_t|s_t=s]$. </p>\n<p>Here we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.</p>\n<h4 id=\"Horizon\"><a href=\"#Horizon\" class=\"headerlink\" title=\"Horizon\"></a>Horizon</h4><p>It is defined as the number of time steps in each episode of the process. An <em>episode</em> is the whole process of a round of training. The horizon can be finite or infinite.</p>\n<h4 id=\"Return\"><a href=\"#Return\" class=\"headerlink\" title=\"Return\"></a>Return</h4><p>The return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon <em>H</em>. We can calculate the return using</p>\n<p>$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.</p>\n<h4 id=\"State-value-function\"><a href=\"#State-value-function\" class=\"headerlink\" title=\"State value function\"></a>State value function</h4><p>The state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression</p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>If the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.</p>\n<h4 id=\"Discount-factor\"><a href=\"#Discount-factor\" class=\"headerlink\" title=\"Discount factor\"></a>Discount factor</h4><p>We design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma &lt;1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.</p>\n<p>Figure 2 and 3 shows an example of how to calculate the return.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F2.png\" alt=\"Figure 2\"></p>\n<p><img src=\"https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg\" alt=\"Figure 3\"></p>\n<p>It is significant to find out a value function while many problems of RL is how to get a value function essentially.</p>\n<h4 id=\"Computing-the-value-function\"><a href=\"#Computing-the-value-function\" class=\"headerlink\" title=\"Computing the value function\"></a>Computing the value function</h4><p>We have three ways to compute the value function.</p>\n<ul>\n<li><p>Simulation. Through simulation, we can get the value function by averaing many returns of episodes.</p>\n</li>\n<li><p>Analytic solution. We have defined the state value function </p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>Then, make a little transformation, see Figure 4 in detail. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F4.png\" alt=\"Figure 4\"></p>\n<p>Then, we have</p>\n<p>$V(s)=R(s)+\\gamma \\sum P(sâ€™|s)V(sâ€™)$, </p>\n</li>\n</ul>\n<p>  $V=R+\\gamma\\bf P\\mit V$. </p>\n<p>  Therefore we have</p>\n<p>  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. </p>\n<p>  If $0&lt;\\gamma&lt;1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.</p>\n<p>  Notice that $sâ€™$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.</p>\n<p>  <img src=\"https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png\" alt=\"Figure 5\"></p>\n<ul>\n<li><p>Iterative solution. </p>\n<p>$V_t(s)=R(s)+\\gamma \\sum P(sâ€™|s)V_{t+1}(sâ€™), \\forall t=0,â€¦,H-1,V_H(s)=0$. </p>\n<p>We can iterate it again and again and use $|V_t-V_{t-1}|&lt;\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. </p>\n</li>\n</ul>\n<h3 id=\"Markov-decision-process-MDP\"><a href=\"#Markov-decision-process-MDP\" class=\"headerlink\" title=\"Markov decision process (MDP)\"></a>Markov decision process (MDP)</h3><p>MDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. </p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$A$: A finite set of actions which are available from each state $s$.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explanations.</p>\n<h4 id=\"Notifications\"><a href=\"#Notifications\" class=\"headerlink\" title=\"Notifications\"></a>Notifications</h4><ul>\n<li><p>Both $S$ and $A$ are finite.</p>\n</li>\n<li><p>In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as</p>\n<p>$P(s_{t+1}|s_t,a_t)$.</p>\n</li>\n<li><p>In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as</p>\n<p>$R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n</li>\n<li><p>Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.</p>\n</li>\n</ul>\n<h4 id=\"Policy\"><a href=\"#Policy\" class=\"headerlink\" title=\"Policy\"></a>Policy</h4><p>Before we mention the state value function, we need to talk about the policy for the MDP first. </p>\n<p>A policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be <em>varying with time</em>, especially when the horizon is finite. A policy can be written as</p>\n<p>$\\bf \\pi\\mit(a|s)=P(a_t=a|s_t=s)$. </p>\n<p>If given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: </p>\n<ul>\n<li><p>$P^\\pi(sâ€™|s)=\\sum_{a\\in A}\\pi(a|s) P(sâ€™|s,a)$</p>\n<p>When we have a policy $\\pi$, the probability of the state transforms from $s$ to $sâ€™$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $sâ€™$ when executing an action $a$.</p>\n</li>\n<li><p>$R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$</p>\n<p>When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.</p>\n</li>\n</ul>\n<h4 id=\"Value-functions-in-MDP-Bellman-expectation-equations\"><a href=\"#Value-functions-in-MDP-Bellman-expectation-equations\" class=\"headerlink\" title=\"Value functions in MDP (Bellman expectation equations)\"></a>Value functions in MDP (Bellman expectation equations)</h4><p>Given a policy $\\pi$ can define two quantities: <em>the state value function</em> and <em>the state-action value function</em>. These two value functions are both <em>Bellman expectation equations</em>.</p>\n<ul>\n<li><p>State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression</p>\n<p>$V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. </p>\n<p>Frequently we will drop the subscript $\\pi$ in the expectation. </p>\n</li>\n<li><p>State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form</p>\n<p>$Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. </p>\n<p>It evaluates the value of acting the action $a$ under current state $s$. </p>\n</li>\n</ul>\n<p>Now letâ€™s talk about the relationships between these two value functions.</p>\n<p>Figure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png\" alt=\"Figure 6\"></p>\n<p>We can discover that the value of a state can be denoted as</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.</p>\n<p>In a similar way, Figure 7 shows what states that an action can lead to.</p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png\" alt=\"Figure 7\"></p>\n<p>We can also find that </p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^\\pi(sâ€™)$. </p>\n<p>On the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $sâ€™$ and the probability of getting into that new state. </p>\n<p>If we combine the two Bellman equation with each other, we can get</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V^\\pi(sâ€™)]$</p>\n<p>â€‹            $=R(sâ€™,\\pi(sâ€™))+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,\\pi(s)) V^\\pi(sâ€™)$, </p>\n<p>and</p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)\\sum_{a\\in A}\\pi(aâ€™|sâ€™)Q_\\pi(sâ€™,aâ€™)$. </p>\n<p>The example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png\" alt=\"Figure 8\"></p>\n<h4 id=\"Optimality-value-function-Bellman-optimality-equation\"><a href=\"#Optimality-value-function-Bellman-optimality-equation\" class=\"headerlink\" title=\"Optimality value function (Bellman optimality equation)\"></a>Optimality value function (Bellman optimality equation)</h4><ul>\n<li>Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. </li>\n<li>Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.</li>\n</ul>\n<p>Optimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. </p>\n<h3 id=\"Find-the-best-policy\"><a href=\"#Find-the-best-policy\" class=\"headerlink\" title=\"Find the best policy\"></a>Find the best policy</h3><p>The best policy is defined precisely as <em>optimal policy</em>  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.</p>\n<p>For an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.</p>\n<p>We can compute the optimal policy by</p>\n<p>$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,</p>\n<p>Which means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. </p>\n<p>If an optimal policy exists then its value function must be a fixed point of the operator $B^*$. </p>\n<h4 id=\"Bellman-optimality-backup-operator\"><a href=\"#Bellman-optimality-backup-operator\" class=\"headerlink\" title=\"Bellman optimality backup operator\"></a>Bellman optimality backup operator</h4><p>Bellman optimality backup operator is written as $B^*$ with a value function behind it </p>\n<p>$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V(sâ€™)$. </p>\n<p>If $\\gamma&lt;1$, $B^*$ is a strict contraction and has a unique fixed point. This means </p>\n<p>$B^*V(s)\\geq V^\\pi(s)$.</p>\n<p>Bellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.</p>\n<p>Next Iâ€™ll briefly introduce some algorithms to compute the optimal value function and an optimal policy.</p>\n<h4 id=\"Policy-search\"><a href=\"#Policy-search\" class=\"headerlink\" title=\"Policy search\"></a>Policy search</h4><p>This algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a $V^<em>(s)$  and  $\\pi^</em>(s)$. </p>\n<h4 id=\"Policy-iteration\"><a href=\"#Policy-iteration\" class=\"headerlink\" title=\"Policy iteration\"></a>Policy iteration</h4><p>The algorithm of policy iteration is shown below: </p>\n<p><code>while</code> True <code>do</code></p>\n<p>â€‹    $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)</p>\n<p>â€‹    $\\pi^*$ = Policy improvement $(M,V^\\pi)$</p>\n<p><code>if</code> $\\pi^*(s)=\\pi(s)$ <code>then</code></p>\n<p>â€‹    <code>break</code></p>\n<p><code>else</code></p>\n<p>â€‹    $\\pi$ = $\\pi^*$</p>\n<p>$V^*$ = $V^\\pi$ . </p>\n<p>Policy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute</p>\n<p>$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^{\\pi i}(sâ€™)$ </p>\n<p>for all the $a$ and $s$ and then take the max</p>\n<p><code>return</code> $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.</p>\n<p>Notice that there is a relationship</p>\n<p>$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.</p>\n<p>This means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.</p>\n<h4 id=\"Value-iteration\"><a href=\"#Value-iteration\" class=\"headerlink\" title=\"Value iteration\"></a>Value iteration</h4><p>The algorithm of value iteration is shown below:</p>\n<p>$Vâ€™(s)=0, V(s)=\\infty$, for all $s\\in S$</p>\n<p><code>while</code> $||V-Vâ€™||_\\infty&gt;\\epsilon$ <code>do</code></p>\n<p>â€‹    $V=Vâ€™$</p>\n<p>â€‹    $Vâ€™(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)Vâ€™(s)$, for all states $s\\in S$ </p>\n<p>$V^*=V$, for all $s\\in S$ </p>\n<p>$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V^ *(sâ€™),\\ \\forall s\\in S$ . </p>\n<p>The idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.</p>\n"},{"title":"Pythonå­¦ä¹ ç¬”è®°","date":"2020-01-06T09:00:00.000Z","thumbnail":"https://netmaxtech.com/wp-content/uploads/2017/05/Python-Logo-PNG-Image.png","excerpt":"è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™é—¨è¯­è¨€çš„ç‰¹æ€§ã€‚","_content":"\n> è¿™ç¯‡æ–‡ç« ä¸»è¦è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™ä¸ªè¯­è¨€çš„ä¸€äº›ç‰¹æ€§ï¼Œå†…å®¹ä»¥æ—¶é—´é¡ºåºæ•´ç†ï¼Œæ¯”è¾ƒé›¶æ•£æ‚ä¹±ã€‚å¯¹äºä»é›¶å¼€å§‹çš„åŒå­¦ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£[Python 3.8.1 ä¸­æ–‡æ–‡æ¡£](https://docs.python.org/zh-cn/3/)æˆ–å…¶ä»–ç½‘ç»œä¸Šçš„æ•™ç¨‹ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚\n\n### 19/9/14\n\n- æ³¨é‡Šæ–¹æ³•ï¼š`#ï¼ˆä¸€è¡Œæ³¨é‡Šï¼‰`ï¼Œ`â€œâ€â€œ â€â€œâ€ï¼ˆå¤šè¡Œæ³¨é‡Šï¼‰`\n- forå¾ªç¯ï¼š`for ï¼ˆå˜é‡ï¼‰ in ï¼ˆèŒƒå›´ï¼‰`ï¼ŒèŒƒå›´å¯ä»¥ç”¨`range`å‡½æ•°\n- `Input`å‡½æ•°çš„è¾“å…¥æ˜¯`char`ç±»å‹çš„\n- `// `æ˜¯æ•´é™¤è¿ç®—\n- é€—å·ä¸å¯ä»¥ç”¨æ¥åˆ†éš”è¯­å¥\n- ä½¿ç”¨ç¼©è¿›ï¼ˆ4ä¸ªç©ºæ ¼ï¼‰æ¥ä»£æ›¿C/C++ä¸­çš„å¤§æ‹¬å·\n\n### 19/9/15\n\n- `for...in`å¾ªç¯ä¸­ï¼Œ`_ `å¯ä»¥ä½œä¸ºå¾ªç¯å˜é‡ï¼Œè¿™æ—¶å€™ä»…å¾ªç¯æŒ‡å®šæ¬¡æ•°ï¼Œè€Œä¸éœ€è¦å…³å¿ƒå¾ªç¯å˜é‡çš„å€¼ï¼›äº‹å®ä¸Šï¼Œ`_ `æ˜¯ä¸€ä¸ªåˆæ³•çš„æ ‡è¯†ç¬¦ï¼Œå¦‚æœä¸å…³å¿ƒè¿™ä¸ªå˜é‡ï¼Œå°±å¯ä»¥å°†å…¶å®šä¹‰æˆè¿™ä¸ªå€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ªåƒåœ¾æ¡¶\n- å®šä¹‰å‡½æ•°æ—¶ï¼Œä½¿ç”¨`å‡½æ•°å(*å‚æ•°å)`çš„å®šä¹‰æ–¹å¼ï¼Œ `*` ä»£è¡¨å‡½æ•°çš„å‚æ•°æ˜¯å¯å˜å‚æ•°ï¼Œå¯ä»¥æœ‰0åˆ°å¤šä¸ªå‚æ•°\n- ä¸€ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ¨¡å—(module)ï¼Œè‹¥åœ¨ä¸åŒçš„æ¨¡å—ä¸­å«æœ‰åŒåå‡½æ•°ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡`import`å¯¼å…¥æŒ‡å®šçš„æ¨¡å—ä¸­çš„å‡½æ•°ï¼Œå¦‚`from æ¨¡å— import å‡½æ•°`ï¼Œæˆ–è€…`import æ¨¡å— as è‡ªå®šä¹‰æ¨¡å—åç§°`ï¼Œå†é€šè¿‡`è‡ªå®šä¹‰æ¨¡å—åç§°.å‡½æ•°`çš„æ–¹å¼è°ƒç”¨\n- `__name__`æ˜¯Pythonä¸­ä¸€ä¸ªéšå«çš„å˜é‡ï¼Œä»£è¡¨äº†æ¨¡å—çš„åå­—ï¼Œåªç”¨ç›´æ¥æ‰§è¡Œçš„æ¨¡å—çš„åå­—æ‰æ˜¯`__main__`\n- å¯ä»¥ä½¿ç”¨`global`æŒ‡å®šä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œå¦‚æœå…¨å±€å˜é‡ä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ï¼Œé‚£ä¹ˆä¼šå®šä¹‰ä¸€ä¸ªæ–°çš„å…¨å±€å˜é‡\n- åµŒå¥—ä½œç”¨åŸŸï¼šå¯¹äºå‡½æ•°aå†…éƒ¨çš„å‡½æ•°bè€Œè¨€ï¼Œaä¸­å®šä¹‰çš„å˜é‡å¯¹bæ¥è¯´æ˜¯åœ¨åµŒå¥—ä½œç”¨åŸŸä¸­çš„ï¼Œè‹¥è¦æŒ‡å®šä¿®æ”¹åµŒå¥—ä½œç”¨åŸŸä¸­çš„å˜é‡ï¼Œå¯ä»¥ä½¿ç”¨`nonlocal`æŒ‡ç¤ºå˜é‡æ¥è‡ªåµŒå¥—ä½œç”¨åŸŸ\n- `pass`æ˜¯ä¸€ä¸ªç©ºè¯­å¥ï¼Œåªèµ·åˆ°å ä½ä½œç”¨\n- å¯ä»¥å®šä¹‰ä¸€ä¸ª`main`å‡½æ•°ï¼ˆæˆ–è€…ä¸æ¨¡å—åå­—ç›¸åŒçš„å‡½æ•°ï¼‰ï¼Œå†æŒ‰ç…§`if __name__ = '__main__'`çš„æ ¼å¼ä½¿è„šæœ¬æ‰§è¡Œ\n\n### 19/9/17\n\n- ä¸å­—ç¬¦ä¸²æœ‰å…³çš„å‡½æ•°çš„è°ƒç”¨æ–¹å¼ä¸ºï¼š`å­—ç¬¦ä¸²åç§°.å­—ç¬¦ä¸²æ“ä½œå‡½æ•°()`ï¼Œåœ¨æ­¤æ—¶å­—ç¬¦ä¸²æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå­—ç¬¦ä¸²æ“ä½œå‡½æ•°çš„ä½œç”¨æ˜¯å‘å­—ç¬¦ä¸²å¯¹è±¡å‘é€ä¸€ä¸ªæ¶ˆæ¯\n- å­—ç¬¦ä¸²å®è´¨ä¸Šæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯ä»¥è¿›è¡Œä¸‹æ ‡è¿ç®—\n- å­—ç¬¦ä¸²åˆ‡ç‰‡å¯ä»¥åœ¨ä¸‹æ ‡è¿ç®—ä¸­ä½¿ç”¨å†’å·è¿›è¡Œè¿ç®—ï¼Œ`[èµ·å§‹å­—ç¬¦:ç»“æŸå­—ç¬¦:é—´éš”]`ï¼Œè‹¥ä¸å®šä¹‰èµ·å§‹ä¸ç»ˆæ­¢å­—ç¬¦ï¼Œåˆ™é»˜è®¤ä¸ºæ•´ä¸ªå­—ç¬¦ä¸²ï¼Œå½“é—´éš”ä¸ºè´Ÿå€¼æ—¶ï¼Œä»¥ä¸ºç€åˆ‡ç‰‡æ“ä½œåå‘\n- å­—ç¬¦ä¸²çš„ç´¢å¼•ä¸ºè´Ÿå€¼æ—¶ï¼Œæ„å‘³ç€ç´¢å¼•ä»å³åˆ°å·¦æ•°\n- åˆ—è¡¨å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶æ“ä½œä¸å­—ç¬¦ä¸²ç±»ä¼¼\n- å¯ä½¿ç”¨`sorted`å‡½æ•°å¯¹åˆ—è¡¨è¿›è¡Œæ’åº\n- å¯ä»¥ä½¿ç”¨ç”Ÿæˆå¼è¯­æ³•åˆ›å»ºåˆ—è¡¨ï¼š`f = [x for x in range(1, 10)]`ï¼ˆæ­¤æ–¹æ³•åœ¨åˆ›å»ºåˆ—è¡¨åå…ƒç´ å·²ç»å‡†å¤‡å°±ç»ªï¼Œè€—è´¹è¾ƒå¤šå†…å­˜ï¼‰ï¼Œæˆ–`f = (x for x in range(1, 10))`ï¼ˆæ­¤æ–¹æ³•åˆ›å»ºçš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼Œéœ€è¦æ•°æ®æ—¶åˆ—è¡¨é€šè¿‡ç”Ÿæˆå™¨äº§ç”Ÿï¼ŒèŠ‚çœå†…å­˜ä½†æ˜¯è€—è´¹è¾ƒå¤šæ—¶é—´ï¼‰\n- å¯ä»¥ä½¿ç”¨`yield`å…³é”®å­—æ¥å®ç°è¿­ä»£ï¼Œä½¿ç”¨`yield`å°±æ˜¯äº§ç”Ÿäº†ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡é‡åˆ°` yield `æ—¶å‡½æ•°ä¼šæš‚åœå¹¶ä¿å­˜å½“å‰æ‰€æœ‰çš„è¿è¡Œä¿¡æ¯ï¼Œè¿”å›` yield `çš„å€¼ï¼Œå¹¶åœ¨ä¸‹ä¸€æ¬¡æ‰§è¡Œæ­¤æ–¹æ³•æ˜¯ä»å½“å‰ä½ç½®å¼€å§‹è¿è¡Œ\n- å¯ä»¥å®šä¹‰å…ƒç»„ï¼Œå…¶ç›¸å½“äºä¸èƒ½ä¿®æ”¹çš„æ•°ç»„ï¼Œä¸€ä¸ªå…ƒç»„ä¸­çš„å…ƒç´ æ•°æ®ç±»å‹å¯ä»¥ä¸åŒï¼Œå®šä¹‰å…ƒç»„ä½¿ç”¨`t = ()`\n- åˆ—è¡¨å’Œå…ƒç»„å¯ä»¥äº’ç›¸è½¬æ¢\n- å¯ä»¥å®šä¹‰é›†åˆï¼Œå®šä¹‰é›†åˆå¯ä»¥ä½¿ç”¨`set = {}`ï¼Œå…ƒç»„å¯ä»¥è½¬æ¢ä¸ºé›†åˆ\n- å­—å…¸ç±»ä¼¼äºæ•°ç»„ï¼Œä½†æ˜¯å®ƒæ˜¯ç”±å¤šç»„é”®å€¼å¯¹ç»„æˆçš„\n\n### 19/9/19\n\n- ä½¿ç”¨classå…³é”®å­—å®šä¹‰ç±»ï¼Œå†åœ¨ç±»ä¸­å®šä¹‰å‡½æ•°ï¼Œå¦‚ï¼š`class ç±»å(object)`\n- `__init__`å‡½æ•°æ˜¯ç”¨äºåœ¨åˆ›å»ºå¯¹è±¡æ—¶è¿›è¡Œçš„åˆå§‹åŒ–æ“ä½œ\n- selfæ˜¯ç±»çš„æœ¬èº«ï¼Œæ˜¯å®ƒçš„å®ä¾‹å˜é‡ï¼Œåœ¨ç±»ä¸­æ‰€æœ‰å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°å°±æ˜¯selfï¼Œåœ¨ç±»ä¸­ä¿®æ”¹å±æ€§å€¼éœ€ä½¿ç”¨`self.å±æ€§å€¼ = x`çš„è¯­æ³•\n- å®ä¾‹åŒ–ç±»çš„æ–¹æ³•ï¼š`å¯¹è±¡å = ç±»å(åˆå§‹åŒ–å‡½æ•°å‚æ•°)`\n- å¯¹è±¡ä¸­æ–¹æ³•çš„å¼•ç”¨å¯ä»¥é‡‡ç”¨`å¯¹è±¡.æ–¹æ³•ï¼ˆä¹Ÿå³å‡½æ•°ï¼‰`çš„è¯­å¥ï¼Œé€šè¿‡æ­¤æ–¹å¼å‘å¯¹è±¡å‘é€æ¶ˆæ¯\n- Pythonä¸­ï¼Œå±æ€§å’Œæ–¹æ³•çš„è®¿é—®æƒé™åªæœ‰`public`å’Œ`private`ï¼Œè‹¥å¸Œæœ›å±æ€§æˆ–æ–¹æ³•æ˜¯ç§æœ‰çš„ï¼Œåœ¨ç»™å®ƒä»¬å‘½åçš„æ—¶å€™è¦ä½¿ç”¨`__`å¼€å¤´ï¼Œä½†æ˜¯ä¸å»ºè®®å°†å±æ€§è®¾ç½®ä¸ºç§æœ‰çš„\n- ä½¿ç”¨`_`å¼€å¤´æš—ç¤ºå±æ€§æˆ–æ–¹æ³•æ˜¯å—ä¿æŠ¤(protected)çš„ï¼Œè®¿é—®å®ƒä»¬å»ºè®®é€šè¿‡ç‰¹å®šçš„æ–¹æ³•ï¼Œä½†å®é™…ä¸Šå®ƒä»¬è¿˜æ˜¯å¯ä»¥ç›´æ¥è¢«å¤–éƒ¨è®¿é—®\n- å¯ä»¥é€šè¿‡åœ¨ç±»ä¸­å®šä¹‰æ–¹æ³•ä»¥è®¿é—®å¯¹è±¡å—ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨å®šä¹‰è¿™äº›æ–¹æ³•ï¼ˆå‡½æ•°ï¼‰æ—¶ï¼Œè¦åœ¨ä¸Šä¸€è¡Œä½¿ç”¨`@property`åŒ…è£…è¿™äº›æ–¹æ³•\n- å¯¹äºè¢«ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨è®¿é—®å®ƒä»¬æ—¶é‡‡ç”¨`getter`æ–¹æ³•ï¼Œéœ€æ·»åŠ `@property`ï¼Œåœ¨ä¿®æ”¹å®ƒä»¬æ—¶é‡‡ç”¨`setter`æ–¹æ³•ï¼Œéœ€æ·»åŠ `@å‡½æ•°ï¼ˆå³æ–¹æ³•ï¼‰å.setter`\n- Pythonå¯ä»¥å¯¹å¯¹è±¡åŠ¨æ€ç»‘å®šæ–°çš„å±æ€§æˆ–æ–¹æ³•\n- å¯ä»¥ä½¿ç”¨`__slots__`é™å®šå¯¹è±¡åªèƒ½ç»‘å®šæŸäº›å±æ€§ï¼Œä½†æ˜¯å®ƒåªå¯¹å½“å‰ç±»çš„å¯¹è±¡ç”Ÿæ•ˆï¼Œå¯¹å­ç±»ä¸èµ·ä½œç”¨\n- å¯ä»¥é€šè¿‡ç»™ç±»å‘é€æ¶ˆæ¯ï¼Œåœ¨ç±»çš„å¯¹è±¡è¢«åˆ›å»ºå‡ºæ¥ä¹‹å‰ç›´æ¥ä½¿ç”¨å…¶ä¸­çš„æ–¹æ³•ï¼Œæ­¤ç§æ–¹æ³•è¢«ç§°ä¸ºé™æ€æ–¹æ³•ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ `@staticmethod`ï¼Œæ­¤ç±»æ–¹æ³•çš„å‚æ•°ä¸å«æœ‰`self`\n- é€šè¿‡ç±»æ–¹æ³•å¯ä»¥è·å–ç±»ç›¸å…³çš„ä¿¡æ¯å¹¶ä¸”å¯ä»¥__åˆ›å»ºå‡ºç±»çš„å¯¹è±¡__ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ `@classmethod`ï¼Œç±»æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯`cls`ï¼Œè¿™ä¸ª`cls`ç›¸å½“äºå°±æ˜¯åœ¨å¤–éƒ¨å®ä¾‹åŒ–ç±»æ—¶å®šä¹‰çš„å¯¹è±¡åï¼Œåªä¸è¿‡å®ƒæ˜¯æ”¾åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨äº†ï¼Œå…¶åŠŸèƒ½å°±æ˜¯å¯ä»¥åƒåœ¨å¤–éƒ¨è°ƒç”¨å¯¹è±¡çš„å±æ€§å’Œæ–¹æ³•ä¸€æ ·åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨å¯¹è±¡ï¼ˆç±»ï¼‰çš„å±æ€§å’Œæ–¹æ³•\n\n### 19/9/20\n\n- ç±»ä¹‹é—´çš„å…³ç³»ï¼š\n  - is-aï¼šç»§æ‰¿æˆ–è€…æ³›åŒ–ï¼Œå¦‚ï¼š__student__ is a __human being__ï¼Œ__cell phone__ is a __electronic device__\n  - has-aï¼šå…³è”ï¼Œå¦‚ __department__ has an __employee__\n  - use-aï¼šä¾èµ–ï¼Œå¦‚ __driver__ use a __car__ \n- ç±»ä¸ç±»ä¹‹é—´å¯ä»¥ç»§æ‰¿ï¼Œæä¾›ç»§æ‰¿ä¿¡æ¯çš„æˆä¸ºçˆ¶ç±»ï¼ˆè¶…ç±»æˆ–è€…åŸºç±»ï¼‰ï¼Œå¾—åˆ°ç»§æ‰¿çš„ç§°ä¸ºå­ç±»ï¼ˆæ´¾ç”Ÿç±»æˆ–è€…è¡ç”Ÿç±»ï¼‰\n- Pythonä¸­ç»§æ‰¿çš„å†™æ³•ï¼š`class å­ç±»å(åŸºç±»å)`\n- åœ¨ç¼–ç¨‹ä¸­ä¸€èˆ¬ä½¿ç”¨å­ç±»å»æ›¿ä»£åŸºç±»\n- åœ¨å­ç±»ä¸­ï¼Œé€šè¿‡é‡æ–°å®šä¹‰çˆ¶ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥è®©åŒä¸€ç§æ–¹æ³•åœ¨ä¸åŒçš„å­ç±»ä¸­æœ‰ä¸åŒçš„è¡Œä¸ºï¼Œè¿™ç§°ä¸ºé‡å†™\n\n### 20/1/11\n\n- Pythonä¸­æä¾›ä¸¤ä¸ªé‡è¦çš„åŠŸèƒ½ï¼šå¼‚å¸¸å¤„ç†å’Œæ–­è¨€ï¼ˆAssertionsï¼‰æ¥å¤„ç†è¿è¡Œä¸­å‡ºç°çš„å¼‚å¸¸å’Œé”™è¯¯ï¼Œä»–ä»¬çš„åŠŸèƒ½æ˜¯ç”¨äºè°ƒè¯•Pythonç¨‹åº\n- å¼‚å¸¸ï¼šæ— æ³•æ­£å¸¸å¤„ç†ç¨‹åºæ—¶ä¼šå‘ç”Ÿå¼‚å¸¸ï¼Œæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚æœä¸æ•è·å¼‚å¸¸ï¼Œç¨‹åºä¼šç»ˆæ­¢æ‰§è¡Œ\n- Pythonä¸­å¼‚å¸¸å¤„ç†çš„å†™æ³•ï¼š\n\n```python\ntry: \n\t#operation1\nexcept exception_type, argument:\n\t#if error occurs in operation1, execute operation2\n  #operation2\nelse: \n\t#if no error occurs in operation1, execute operation3\n  #operation3\n```\n\n- ä½¿ç”¨`except`å¯ä»¥ä¸å¸¦å¼‚å¸¸ç±»å‹ï¼Œä½†æ˜¯ä¼šè®©`try-except`è¯­å¥æ•è·æ‰€æœ‰çš„å¼‚å¸¸ï¼Œä¸å»ºè®®è¿™æ ·å†™\n- å¯ä»¥ä½¿ç”¨`expect(exception1[, expection2[, expection3]])`æ¥æ·»åŠ å¤šä¸ªå¼‚å¸¸ç±»å‹\n- `argument`ä¸ºå¼‚å¸¸çš„å‚æ•°ï¼Œå¯ä»¥ç”¨äºè¾“å‡ºå¼‚å¸¸ä¿¡æ¯çš„å¼‚å¸¸å€¼\n- ä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ï¼Œä½†æ˜¯ä¸`try-except`æœ‰æ‰€ä¸åŒï¼š\n\n```python\ntry:\n\t#operation1\nfinally:\n\t#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished\n```\n\n- `finally`å’Œ`except`ä¸å¯ä»¥åŒæ—¶ä½¿ç”¨\n\n- å¯ä»¥ä½¿ç”¨`raise`è§¦å‘å¼‚å¸¸\n- `append()`æ–¹æ³•ç”¨äºåœ¨åˆ—è¡¨æœ«å°¾æ·»åŠ æ–°çš„å¯¹è±¡ï¼Œå¯¹äºä¸€ä¸ªæ•°ç»„`list`ï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š`list.append()`\n- å¤šçº¿ç¨‹ç”¨äºåŒæ—¶æ‰§è¡Œå¤šä¸ªä¸åŒçš„ç¨‹åºï¼Œå¯ä»¥æŠŠå æ®é•¿æ—¶é—´çš„ç¨‹åºä¸­çš„ä»»åŠ¡æ”¾åˆ°åå°å¤„ç†\n- çº¿ç¨‹ä¸è¿›ç¨‹ï¼šç‹¬ç«‹çš„çº¿ç¨‹æœ‰è‡ªå·±çš„ç¨‹åºå…¥å£ã€æ‰§è¡Œåºåˆ—ã€ç¨‹åºå‡ºå£ï¼Œä½†æ˜¯çº¿ç¨‹ä¸å¯ä»¥ç‹¬ç«‹æ‰§è¡Œï¼Œå¿…é¡»ä¾å­˜åœ¨åº”ç”¨ç¨‹åºä¸­ï¼Œç”±åº”ç”¨ç¨‹åºæä¾›å¤šä¸ªçº¿ç¨‹æ‰§è¡Œæ§åˆ¶\n- åœ¨Pythonä¸­ä½¿ç”¨çº¿ç¨‹ï¼š`thread.start_new_thread(function, args[, kwargs])`ï¼Œå…¶ä¸­`function`ä¸ºçº¿ç¨‹å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°éœ€è¦æå‰å®šä¹‰å¥½ï¼Œ`args`ä¸ºä¼ é€’ç»™çº¿ç¨‹å‡½æ•°çš„å‚æ•°ï¼Œæ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œ`kwargs`ä¸ºå¯é€‰å‚æ•°ï¼Œæ­¤ç§æ–¹å¼ç§°ä¸ºå‡½æ•°å¼ï¼Œçº¿ç¨‹çš„ç»“æŸä¸€èˆ¬é å‡½æ•°çš„è‡ªç„¶ç»“æŸ\n- æ­¤å¤–è¿˜å¯ä»¥ä½¿ç”¨Pythonæ‰€æä¾›çš„`threading`æ¨¡å—ï¼Œç›´æ¥ä»`threading.Thread`ç»§æ‰¿ï¼š`class myThread(threading.Thread)`ï¼Œç„¶åé‡å†™`__init__`å’Œ`run`æ–¹æ³•ï¼ŒæŠŠéœ€è¦æ‰§è¡Œçš„ä»£ç å†™åˆ°`run`æ–¹æ³•é‡Œé¢ï¼Œ`__init__`çš„é‡å†™æ–¹æ³•å¦‚ä¸‹ï¼š\n\n```python\ndef __init__(self, threadID, name, counter):\n\tthreading.Thread.__init__(self)\n\tself.threadID = threadID\n\tself.name = name\n\tself.counter = counter\n```\n\n- ä¸Šè¿°`thread`ç±»æä¾›äº†ä»¥ä¸‹æ–¹æ³•ï¼š\n  - `run()`ï¼šè¡¨ç¤ºçº¿ç¨‹æ´»åŠ¨çš„æ–¹æ³•\n  - `start`ï¼šå¯åŠ¨çº¿ç¨‹\n  - `join()`ï¼šç­‰å¾…ç›´åˆ°çº¿ç¨‹ç»ˆæ­¢\n  - `isAlive()`ï¼šæŸ¥è¯¢çº¿ç¨‹æ˜¯å¦æ´»åŠ¨\n  - `getName()`ï¼šè¿”å›çº¿ç¨‹å\n  - `setName()`ï¼šè®¾ç½®çº¿ç¨‹å\n- ä¸ºäº†é¿å…ä¸¤ä¸ªæˆ–å¤šä¸ªçº¿ç¨‹åŒæ—¶è¿è¡Œï¼Œäº§ç”Ÿå†²çªï¼Œå¯ä»¥ä½¿ç”¨çº¿ç¨‹é”æ¥æ§åˆ¶çº¿ç¨‹æ‰§è¡Œçš„ä¼˜å…ˆé¡ºåºï¼Œè¢«é”å®šçš„çº¿ç¨‹ä¼˜å…ˆæ‰§è¡Œï¼Œå…¶ä»–è¿›ç¨‹å¿…é¡»åœæ­¢\n- å¯ä»¥ä½¿ç”¨`threading.Lock().acquire()`å’Œ`threading.Lock().release()`æ¥é”å®šå’Œé‡Šæ”¾çº¿ç¨‹\n- å¯ä»¥å»ºç«‹ä¸€ä¸ªç©ºæ•°ç»„ç”¨äºå­˜æ”¾çº¿ç¨‹ï¼Œå†é€šè¿‡`append`æ–¹æ³•å°†çº¿ç¨‹æ·»åŠ è‡³è¯¥æ•°ç»„ä¸­ï¼Œé€šè¿‡éå†æ•°ç»„å¯ä»¥å¯¹å…¶ä¸­çš„çº¿ç¨‹åšä¸€æ ·çš„æ“ä½œ\n\n### 20/1/13\n\n- åœ¨ç»§æ‰¿çš„æ—¶å€™ï¼Œ","source":"_posts/Pythonå­¦ä¹ ç¬”è®°.md","raw":"---\ntitle: Pythonå­¦ä¹ ç¬”è®°\ndate: 2020-1-6 17:00:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- Python\n\t- Programming Language\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://netmaxtech.com/wp-content/uploads/2017/05/Python-Logo-PNG-Image.png\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™é—¨è¯­è¨€çš„ç‰¹æ€§ã€‚\n\n#You can begin to input your article below now.\n\n---\n\n> è¿™ç¯‡æ–‡ç« ä¸»è¦è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™ä¸ªè¯­è¨€çš„ä¸€äº›ç‰¹æ€§ï¼Œå†…å®¹ä»¥æ—¶é—´é¡ºåºæ•´ç†ï¼Œæ¯”è¾ƒé›¶æ•£æ‚ä¹±ã€‚å¯¹äºä»é›¶å¼€å§‹çš„åŒå­¦ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£[Python 3.8.1 ä¸­æ–‡æ–‡æ¡£](https://docs.python.org/zh-cn/3/)æˆ–å…¶ä»–ç½‘ç»œä¸Šçš„æ•™ç¨‹ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚\n\n### 19/9/14\n\n- æ³¨é‡Šæ–¹æ³•ï¼š`#ï¼ˆä¸€è¡Œæ³¨é‡Šï¼‰`ï¼Œ`â€œâ€â€œ â€â€œâ€ï¼ˆå¤šè¡Œæ³¨é‡Šï¼‰`\n- forå¾ªç¯ï¼š`for ï¼ˆå˜é‡ï¼‰ in ï¼ˆèŒƒå›´ï¼‰`ï¼ŒèŒƒå›´å¯ä»¥ç”¨`range`å‡½æ•°\n- `Input`å‡½æ•°çš„è¾“å…¥æ˜¯`char`ç±»å‹çš„\n- `// `æ˜¯æ•´é™¤è¿ç®—\n- é€—å·ä¸å¯ä»¥ç”¨æ¥åˆ†éš”è¯­å¥\n- ä½¿ç”¨ç¼©è¿›ï¼ˆ4ä¸ªç©ºæ ¼ï¼‰æ¥ä»£æ›¿C/C++ä¸­çš„å¤§æ‹¬å·\n\n### 19/9/15\n\n- `for...in`å¾ªç¯ä¸­ï¼Œ`_ `å¯ä»¥ä½œä¸ºå¾ªç¯å˜é‡ï¼Œè¿™æ—¶å€™ä»…å¾ªç¯æŒ‡å®šæ¬¡æ•°ï¼Œè€Œä¸éœ€è¦å…³å¿ƒå¾ªç¯å˜é‡çš„å€¼ï¼›äº‹å®ä¸Šï¼Œ`_ `æ˜¯ä¸€ä¸ªåˆæ³•çš„æ ‡è¯†ç¬¦ï¼Œå¦‚æœä¸å…³å¿ƒè¿™ä¸ªå˜é‡ï¼Œå°±å¯ä»¥å°†å…¶å®šä¹‰æˆè¿™ä¸ªå€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ªåƒåœ¾æ¡¶\n- å®šä¹‰å‡½æ•°æ—¶ï¼Œä½¿ç”¨`å‡½æ•°å(*å‚æ•°å)`çš„å®šä¹‰æ–¹å¼ï¼Œ `*` ä»£è¡¨å‡½æ•°çš„å‚æ•°æ˜¯å¯å˜å‚æ•°ï¼Œå¯ä»¥æœ‰0åˆ°å¤šä¸ªå‚æ•°\n- ä¸€ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ¨¡å—(module)ï¼Œè‹¥åœ¨ä¸åŒçš„æ¨¡å—ä¸­å«æœ‰åŒåå‡½æ•°ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡`import`å¯¼å…¥æŒ‡å®šçš„æ¨¡å—ä¸­çš„å‡½æ•°ï¼Œå¦‚`from æ¨¡å— import å‡½æ•°`ï¼Œæˆ–è€…`import æ¨¡å— as è‡ªå®šä¹‰æ¨¡å—åç§°`ï¼Œå†é€šè¿‡`è‡ªå®šä¹‰æ¨¡å—åç§°.å‡½æ•°`çš„æ–¹å¼è°ƒç”¨\n- `__name__`æ˜¯Pythonä¸­ä¸€ä¸ªéšå«çš„å˜é‡ï¼Œä»£è¡¨äº†æ¨¡å—çš„åå­—ï¼Œåªç”¨ç›´æ¥æ‰§è¡Œçš„æ¨¡å—çš„åå­—æ‰æ˜¯`__main__`\n- å¯ä»¥ä½¿ç”¨`global`æŒ‡å®šä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œå¦‚æœå…¨å±€å˜é‡ä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ï¼Œé‚£ä¹ˆä¼šå®šä¹‰ä¸€ä¸ªæ–°çš„å…¨å±€å˜é‡\n- åµŒå¥—ä½œç”¨åŸŸï¼šå¯¹äºå‡½æ•°aå†…éƒ¨çš„å‡½æ•°bè€Œè¨€ï¼Œaä¸­å®šä¹‰çš„å˜é‡å¯¹bæ¥è¯´æ˜¯åœ¨åµŒå¥—ä½œç”¨åŸŸä¸­çš„ï¼Œè‹¥è¦æŒ‡å®šä¿®æ”¹åµŒå¥—ä½œç”¨åŸŸä¸­çš„å˜é‡ï¼Œå¯ä»¥ä½¿ç”¨`nonlocal`æŒ‡ç¤ºå˜é‡æ¥è‡ªåµŒå¥—ä½œç”¨åŸŸ\n- `pass`æ˜¯ä¸€ä¸ªç©ºè¯­å¥ï¼Œåªèµ·åˆ°å ä½ä½œç”¨\n- å¯ä»¥å®šä¹‰ä¸€ä¸ª`main`å‡½æ•°ï¼ˆæˆ–è€…ä¸æ¨¡å—åå­—ç›¸åŒçš„å‡½æ•°ï¼‰ï¼Œå†æŒ‰ç…§`if __name__ = '__main__'`çš„æ ¼å¼ä½¿è„šæœ¬æ‰§è¡Œ\n\n### 19/9/17\n\n- ä¸å­—ç¬¦ä¸²æœ‰å…³çš„å‡½æ•°çš„è°ƒç”¨æ–¹å¼ä¸ºï¼š`å­—ç¬¦ä¸²åç§°.å­—ç¬¦ä¸²æ“ä½œå‡½æ•°()`ï¼Œåœ¨æ­¤æ—¶å­—ç¬¦ä¸²æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå­—ç¬¦ä¸²æ“ä½œå‡½æ•°çš„ä½œç”¨æ˜¯å‘å­—ç¬¦ä¸²å¯¹è±¡å‘é€ä¸€ä¸ªæ¶ˆæ¯\n- å­—ç¬¦ä¸²å®è´¨ä¸Šæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯ä»¥è¿›è¡Œä¸‹æ ‡è¿ç®—\n- å­—ç¬¦ä¸²åˆ‡ç‰‡å¯ä»¥åœ¨ä¸‹æ ‡è¿ç®—ä¸­ä½¿ç”¨å†’å·è¿›è¡Œè¿ç®—ï¼Œ`[èµ·å§‹å­—ç¬¦:ç»“æŸå­—ç¬¦:é—´éš”]`ï¼Œè‹¥ä¸å®šä¹‰èµ·å§‹ä¸ç»ˆæ­¢å­—ç¬¦ï¼Œåˆ™é»˜è®¤ä¸ºæ•´ä¸ªå­—ç¬¦ä¸²ï¼Œå½“é—´éš”ä¸ºè´Ÿå€¼æ—¶ï¼Œä»¥ä¸ºç€åˆ‡ç‰‡æ“ä½œåå‘\n- å­—ç¬¦ä¸²çš„ç´¢å¼•ä¸ºè´Ÿå€¼æ—¶ï¼Œæ„å‘³ç€ç´¢å¼•ä»å³åˆ°å·¦æ•°\n- åˆ—è¡¨å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶æ“ä½œä¸å­—ç¬¦ä¸²ç±»ä¼¼\n- å¯ä½¿ç”¨`sorted`å‡½æ•°å¯¹åˆ—è¡¨è¿›è¡Œæ’åº\n- å¯ä»¥ä½¿ç”¨ç”Ÿæˆå¼è¯­æ³•åˆ›å»ºåˆ—è¡¨ï¼š`f = [x for x in range(1, 10)]`ï¼ˆæ­¤æ–¹æ³•åœ¨åˆ›å»ºåˆ—è¡¨åå…ƒç´ å·²ç»å‡†å¤‡å°±ç»ªï¼Œè€—è´¹è¾ƒå¤šå†…å­˜ï¼‰ï¼Œæˆ–`f = (x for x in range(1, 10))`ï¼ˆæ­¤æ–¹æ³•åˆ›å»ºçš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼Œéœ€è¦æ•°æ®æ—¶åˆ—è¡¨é€šè¿‡ç”Ÿæˆå™¨äº§ç”Ÿï¼ŒèŠ‚çœå†…å­˜ä½†æ˜¯è€—è´¹è¾ƒå¤šæ—¶é—´ï¼‰\n- å¯ä»¥ä½¿ç”¨`yield`å…³é”®å­—æ¥å®ç°è¿­ä»£ï¼Œä½¿ç”¨`yield`å°±æ˜¯äº§ç”Ÿäº†ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡é‡åˆ°` yield `æ—¶å‡½æ•°ä¼šæš‚åœå¹¶ä¿å­˜å½“å‰æ‰€æœ‰çš„è¿è¡Œä¿¡æ¯ï¼Œè¿”å›` yield `çš„å€¼ï¼Œå¹¶åœ¨ä¸‹ä¸€æ¬¡æ‰§è¡Œæ­¤æ–¹æ³•æ˜¯ä»å½“å‰ä½ç½®å¼€å§‹è¿è¡Œ\n- å¯ä»¥å®šä¹‰å…ƒç»„ï¼Œå…¶ç›¸å½“äºä¸èƒ½ä¿®æ”¹çš„æ•°ç»„ï¼Œä¸€ä¸ªå…ƒç»„ä¸­çš„å…ƒç´ æ•°æ®ç±»å‹å¯ä»¥ä¸åŒï¼Œå®šä¹‰å…ƒç»„ä½¿ç”¨`t = ()`\n- åˆ—è¡¨å’Œå…ƒç»„å¯ä»¥äº’ç›¸è½¬æ¢\n- å¯ä»¥å®šä¹‰é›†åˆï¼Œå®šä¹‰é›†åˆå¯ä»¥ä½¿ç”¨`set = {}`ï¼Œå…ƒç»„å¯ä»¥è½¬æ¢ä¸ºé›†åˆ\n- å­—å…¸ç±»ä¼¼äºæ•°ç»„ï¼Œä½†æ˜¯å®ƒæ˜¯ç”±å¤šç»„é”®å€¼å¯¹ç»„æˆçš„\n\n### 19/9/19\n\n- ä½¿ç”¨classå…³é”®å­—å®šä¹‰ç±»ï¼Œå†åœ¨ç±»ä¸­å®šä¹‰å‡½æ•°ï¼Œå¦‚ï¼š`class ç±»å(object)`\n- `__init__`å‡½æ•°æ˜¯ç”¨äºåœ¨åˆ›å»ºå¯¹è±¡æ—¶è¿›è¡Œçš„åˆå§‹åŒ–æ“ä½œ\n- selfæ˜¯ç±»çš„æœ¬èº«ï¼Œæ˜¯å®ƒçš„å®ä¾‹å˜é‡ï¼Œåœ¨ç±»ä¸­æ‰€æœ‰å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°å°±æ˜¯selfï¼Œåœ¨ç±»ä¸­ä¿®æ”¹å±æ€§å€¼éœ€ä½¿ç”¨`self.å±æ€§å€¼ = x`çš„è¯­æ³•\n- å®ä¾‹åŒ–ç±»çš„æ–¹æ³•ï¼š`å¯¹è±¡å = ç±»å(åˆå§‹åŒ–å‡½æ•°å‚æ•°)`\n- å¯¹è±¡ä¸­æ–¹æ³•çš„å¼•ç”¨å¯ä»¥é‡‡ç”¨`å¯¹è±¡.æ–¹æ³•ï¼ˆä¹Ÿå³å‡½æ•°ï¼‰`çš„è¯­å¥ï¼Œé€šè¿‡æ­¤æ–¹å¼å‘å¯¹è±¡å‘é€æ¶ˆæ¯\n- Pythonä¸­ï¼Œå±æ€§å’Œæ–¹æ³•çš„è®¿é—®æƒé™åªæœ‰`public`å’Œ`private`ï¼Œè‹¥å¸Œæœ›å±æ€§æˆ–æ–¹æ³•æ˜¯ç§æœ‰çš„ï¼Œåœ¨ç»™å®ƒä»¬å‘½åçš„æ—¶å€™è¦ä½¿ç”¨`__`å¼€å¤´ï¼Œä½†æ˜¯ä¸å»ºè®®å°†å±æ€§è®¾ç½®ä¸ºç§æœ‰çš„\n- ä½¿ç”¨`_`å¼€å¤´æš—ç¤ºå±æ€§æˆ–æ–¹æ³•æ˜¯å—ä¿æŠ¤(protected)çš„ï¼Œè®¿é—®å®ƒä»¬å»ºè®®é€šè¿‡ç‰¹å®šçš„æ–¹æ³•ï¼Œä½†å®é™…ä¸Šå®ƒä»¬è¿˜æ˜¯å¯ä»¥ç›´æ¥è¢«å¤–éƒ¨è®¿é—®\n- å¯ä»¥é€šè¿‡åœ¨ç±»ä¸­å®šä¹‰æ–¹æ³•ä»¥è®¿é—®å¯¹è±¡å—ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨å®šä¹‰è¿™äº›æ–¹æ³•ï¼ˆå‡½æ•°ï¼‰æ—¶ï¼Œè¦åœ¨ä¸Šä¸€è¡Œä½¿ç”¨`@property`åŒ…è£…è¿™äº›æ–¹æ³•\n- å¯¹äºè¢«ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨è®¿é—®å®ƒä»¬æ—¶é‡‡ç”¨`getter`æ–¹æ³•ï¼Œéœ€æ·»åŠ `@property`ï¼Œåœ¨ä¿®æ”¹å®ƒä»¬æ—¶é‡‡ç”¨`setter`æ–¹æ³•ï¼Œéœ€æ·»åŠ `@å‡½æ•°ï¼ˆå³æ–¹æ³•ï¼‰å.setter`\n- Pythonå¯ä»¥å¯¹å¯¹è±¡åŠ¨æ€ç»‘å®šæ–°çš„å±æ€§æˆ–æ–¹æ³•\n- å¯ä»¥ä½¿ç”¨`__slots__`é™å®šå¯¹è±¡åªèƒ½ç»‘å®šæŸäº›å±æ€§ï¼Œä½†æ˜¯å®ƒåªå¯¹å½“å‰ç±»çš„å¯¹è±¡ç”Ÿæ•ˆï¼Œå¯¹å­ç±»ä¸èµ·ä½œç”¨\n- å¯ä»¥é€šè¿‡ç»™ç±»å‘é€æ¶ˆæ¯ï¼Œåœ¨ç±»çš„å¯¹è±¡è¢«åˆ›å»ºå‡ºæ¥ä¹‹å‰ç›´æ¥ä½¿ç”¨å…¶ä¸­çš„æ–¹æ³•ï¼Œæ­¤ç§æ–¹æ³•è¢«ç§°ä¸ºé™æ€æ–¹æ³•ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ `@staticmethod`ï¼Œæ­¤ç±»æ–¹æ³•çš„å‚æ•°ä¸å«æœ‰`self`\n- é€šè¿‡ç±»æ–¹æ³•å¯ä»¥è·å–ç±»ç›¸å…³çš„ä¿¡æ¯å¹¶ä¸”å¯ä»¥__åˆ›å»ºå‡ºç±»çš„å¯¹è±¡__ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ `@classmethod`ï¼Œç±»æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯`cls`ï¼Œè¿™ä¸ª`cls`ç›¸å½“äºå°±æ˜¯åœ¨å¤–éƒ¨å®ä¾‹åŒ–ç±»æ—¶å®šä¹‰çš„å¯¹è±¡åï¼Œåªä¸è¿‡å®ƒæ˜¯æ”¾åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨äº†ï¼Œå…¶åŠŸèƒ½å°±æ˜¯å¯ä»¥åƒåœ¨å¤–éƒ¨è°ƒç”¨å¯¹è±¡çš„å±æ€§å’Œæ–¹æ³•ä¸€æ ·åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨å¯¹è±¡ï¼ˆç±»ï¼‰çš„å±æ€§å’Œæ–¹æ³•\n\n### 19/9/20\n\n- ç±»ä¹‹é—´çš„å…³ç³»ï¼š\n  - is-aï¼šç»§æ‰¿æˆ–è€…æ³›åŒ–ï¼Œå¦‚ï¼š__student__ is a __human being__ï¼Œ__cell phone__ is a __electronic device__\n  - has-aï¼šå…³è”ï¼Œå¦‚ __department__ has an __employee__\n  - use-aï¼šä¾èµ–ï¼Œå¦‚ __driver__ use a __car__ \n- ç±»ä¸ç±»ä¹‹é—´å¯ä»¥ç»§æ‰¿ï¼Œæä¾›ç»§æ‰¿ä¿¡æ¯çš„æˆä¸ºçˆ¶ç±»ï¼ˆè¶…ç±»æˆ–è€…åŸºç±»ï¼‰ï¼Œå¾—åˆ°ç»§æ‰¿çš„ç§°ä¸ºå­ç±»ï¼ˆæ´¾ç”Ÿç±»æˆ–è€…è¡ç”Ÿç±»ï¼‰\n- Pythonä¸­ç»§æ‰¿çš„å†™æ³•ï¼š`class å­ç±»å(åŸºç±»å)`\n- åœ¨ç¼–ç¨‹ä¸­ä¸€èˆ¬ä½¿ç”¨å­ç±»å»æ›¿ä»£åŸºç±»\n- åœ¨å­ç±»ä¸­ï¼Œé€šè¿‡é‡æ–°å®šä¹‰çˆ¶ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥è®©åŒä¸€ç§æ–¹æ³•åœ¨ä¸åŒçš„å­ç±»ä¸­æœ‰ä¸åŒçš„è¡Œä¸ºï¼Œè¿™ç§°ä¸ºé‡å†™\n\n### 20/1/11\n\n- Pythonä¸­æä¾›ä¸¤ä¸ªé‡è¦çš„åŠŸèƒ½ï¼šå¼‚å¸¸å¤„ç†å’Œæ–­è¨€ï¼ˆAssertionsï¼‰æ¥å¤„ç†è¿è¡Œä¸­å‡ºç°çš„å¼‚å¸¸å’Œé”™è¯¯ï¼Œä»–ä»¬çš„åŠŸèƒ½æ˜¯ç”¨äºè°ƒè¯•Pythonç¨‹åº\n- å¼‚å¸¸ï¼šæ— æ³•æ­£å¸¸å¤„ç†ç¨‹åºæ—¶ä¼šå‘ç”Ÿå¼‚å¸¸ï¼Œæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚æœä¸æ•è·å¼‚å¸¸ï¼Œç¨‹åºä¼šç»ˆæ­¢æ‰§è¡Œ\n- Pythonä¸­å¼‚å¸¸å¤„ç†çš„å†™æ³•ï¼š\n\n```python\ntry: \n\t#operation1\nexcept exception_type, argument:\n\t#if error occurs in operation1, execute operation2\n  #operation2\nelse: \n\t#if no error occurs in operation1, execute operation3\n  #operation3\n```\n\n- ä½¿ç”¨`except`å¯ä»¥ä¸å¸¦å¼‚å¸¸ç±»å‹ï¼Œä½†æ˜¯ä¼šè®©`try-except`è¯­å¥æ•è·æ‰€æœ‰çš„å¼‚å¸¸ï¼Œä¸å»ºè®®è¿™æ ·å†™\n- å¯ä»¥ä½¿ç”¨`expect(exception1[, expection2[, expection3]])`æ¥æ·»åŠ å¤šä¸ªå¼‚å¸¸ç±»å‹\n- `argument`ä¸ºå¼‚å¸¸çš„å‚æ•°ï¼Œå¯ä»¥ç”¨äºè¾“å‡ºå¼‚å¸¸ä¿¡æ¯çš„å¼‚å¸¸å€¼\n- ä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ï¼Œä½†æ˜¯ä¸`try-except`æœ‰æ‰€ä¸åŒï¼š\n\n```python\ntry:\n\t#operation1\nfinally:\n\t#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished\n```\n\n- `finally`å’Œ`except`ä¸å¯ä»¥åŒæ—¶ä½¿ç”¨\n\n- å¯ä»¥ä½¿ç”¨`raise`è§¦å‘å¼‚å¸¸\n- `append()`æ–¹æ³•ç”¨äºåœ¨åˆ—è¡¨æœ«å°¾æ·»åŠ æ–°çš„å¯¹è±¡ï¼Œå¯¹äºä¸€ä¸ªæ•°ç»„`list`ï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š`list.append()`\n- å¤šçº¿ç¨‹ç”¨äºåŒæ—¶æ‰§è¡Œå¤šä¸ªä¸åŒçš„ç¨‹åºï¼Œå¯ä»¥æŠŠå æ®é•¿æ—¶é—´çš„ç¨‹åºä¸­çš„ä»»åŠ¡æ”¾åˆ°åå°å¤„ç†\n- çº¿ç¨‹ä¸è¿›ç¨‹ï¼šç‹¬ç«‹çš„çº¿ç¨‹æœ‰è‡ªå·±çš„ç¨‹åºå…¥å£ã€æ‰§è¡Œåºåˆ—ã€ç¨‹åºå‡ºå£ï¼Œä½†æ˜¯çº¿ç¨‹ä¸å¯ä»¥ç‹¬ç«‹æ‰§è¡Œï¼Œå¿…é¡»ä¾å­˜åœ¨åº”ç”¨ç¨‹åºä¸­ï¼Œç”±åº”ç”¨ç¨‹åºæä¾›å¤šä¸ªçº¿ç¨‹æ‰§è¡Œæ§åˆ¶\n- åœ¨Pythonä¸­ä½¿ç”¨çº¿ç¨‹ï¼š`thread.start_new_thread(function, args[, kwargs])`ï¼Œå…¶ä¸­`function`ä¸ºçº¿ç¨‹å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°éœ€è¦æå‰å®šä¹‰å¥½ï¼Œ`args`ä¸ºä¼ é€’ç»™çº¿ç¨‹å‡½æ•°çš„å‚æ•°ï¼Œæ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œ`kwargs`ä¸ºå¯é€‰å‚æ•°ï¼Œæ­¤ç§æ–¹å¼ç§°ä¸ºå‡½æ•°å¼ï¼Œçº¿ç¨‹çš„ç»“æŸä¸€èˆ¬é å‡½æ•°çš„è‡ªç„¶ç»“æŸ\n- æ­¤å¤–è¿˜å¯ä»¥ä½¿ç”¨Pythonæ‰€æä¾›çš„`threading`æ¨¡å—ï¼Œç›´æ¥ä»`threading.Thread`ç»§æ‰¿ï¼š`class myThread(threading.Thread)`ï¼Œç„¶åé‡å†™`__init__`å’Œ`run`æ–¹æ³•ï¼ŒæŠŠéœ€è¦æ‰§è¡Œçš„ä»£ç å†™åˆ°`run`æ–¹æ³•é‡Œé¢ï¼Œ`__init__`çš„é‡å†™æ–¹æ³•å¦‚ä¸‹ï¼š\n\n```python\ndef __init__(self, threadID, name, counter):\n\tthreading.Thread.__init__(self)\n\tself.threadID = threadID\n\tself.name = name\n\tself.counter = counter\n```\n\n- ä¸Šè¿°`thread`ç±»æä¾›äº†ä»¥ä¸‹æ–¹æ³•ï¼š\n  - `run()`ï¼šè¡¨ç¤ºçº¿ç¨‹æ´»åŠ¨çš„æ–¹æ³•\n  - `start`ï¼šå¯åŠ¨çº¿ç¨‹\n  - `join()`ï¼šç­‰å¾…ç›´åˆ°çº¿ç¨‹ç»ˆæ­¢\n  - `isAlive()`ï¼šæŸ¥è¯¢çº¿ç¨‹æ˜¯å¦æ´»åŠ¨\n  - `getName()`ï¼šè¿”å›çº¿ç¨‹å\n  - `setName()`ï¼šè®¾ç½®çº¿ç¨‹å\n- ä¸ºäº†é¿å…ä¸¤ä¸ªæˆ–å¤šä¸ªçº¿ç¨‹åŒæ—¶è¿è¡Œï¼Œäº§ç”Ÿå†²çªï¼Œå¯ä»¥ä½¿ç”¨çº¿ç¨‹é”æ¥æ§åˆ¶çº¿ç¨‹æ‰§è¡Œçš„ä¼˜å…ˆé¡ºåºï¼Œè¢«é”å®šçš„çº¿ç¨‹ä¼˜å…ˆæ‰§è¡Œï¼Œå…¶ä»–è¿›ç¨‹å¿…é¡»åœæ­¢\n- å¯ä»¥ä½¿ç”¨`threading.Lock().acquire()`å’Œ`threading.Lock().release()`æ¥é”å®šå’Œé‡Šæ”¾çº¿ç¨‹\n- å¯ä»¥å»ºç«‹ä¸€ä¸ªç©ºæ•°ç»„ç”¨äºå­˜æ”¾çº¿ç¨‹ï¼Œå†é€šè¿‡`append`æ–¹æ³•å°†çº¿ç¨‹æ·»åŠ è‡³è¯¥æ•°ç»„ä¸­ï¼Œé€šè¿‡éå†æ•°ç»„å¯ä»¥å¯¹å…¶ä¸­çš„çº¿ç¨‹åšä¸€æ ·çš„æ“ä½œ\n\n### 20/1/13\n\n- åœ¨ç»§æ‰¿çš„æ—¶å€™ï¼Œ","slug":"Pythonå­¦ä¹ ç¬”è®°","published":1,"updated":"2020-01-13T04:18:25.089Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck6ax1lgj0009j1p22bs352jd","content":"<blockquote>\n<p>è¿™ç¯‡æ–‡ç« ä¸»è¦è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™ä¸ªè¯­è¨€çš„ä¸€äº›ç‰¹æ€§ï¼Œå†…å®¹ä»¥æ—¶é—´é¡ºåºæ•´ç†ï¼Œæ¯”è¾ƒé›¶æ•£æ‚ä¹±ã€‚å¯¹äºä»é›¶å¼€å§‹çš„åŒå­¦ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£<a href=\"https://docs.python.org/zh-cn/3/\">Python 3.8.1 ä¸­æ–‡æ–‡æ¡£</a>æˆ–å…¶ä»–ç½‘ç»œä¸Šçš„æ•™ç¨‹ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚</p>\n</blockquote>\n<h3 id=\"19-9-14\"><a href=\"#19-9-14\" class=\"headerlink\" title=\"19/9/14\"></a>19/9/14</h3><ul>\n<li>æ³¨é‡Šæ–¹æ³•ï¼š<code>#ï¼ˆä¸€è¡Œæ³¨é‡Šï¼‰</code>ï¼Œ<code>â€œâ€â€œ â€â€œâ€ï¼ˆå¤šè¡Œæ³¨é‡Šï¼‰</code></li>\n<li>forå¾ªç¯ï¼š<code>for ï¼ˆå˜é‡ï¼‰ in ï¼ˆèŒƒå›´ï¼‰</code>ï¼ŒèŒƒå›´å¯ä»¥ç”¨<code>range</code>å‡½æ•°</li>\n<li><code>Input</code>å‡½æ•°çš„è¾“å…¥æ˜¯<code>char</code>ç±»å‹çš„</li>\n<li><code>//</code>æ˜¯æ•´é™¤è¿ç®—</li>\n<li>é€—å·ä¸å¯ä»¥ç”¨æ¥åˆ†éš”è¯­å¥</li>\n<li>ä½¿ç”¨ç¼©è¿›ï¼ˆ4ä¸ªç©ºæ ¼ï¼‰æ¥ä»£æ›¿C/C++ä¸­çš„å¤§æ‹¬å·</li>\n</ul>\n<h3 id=\"19-9-15\"><a href=\"#19-9-15\" class=\"headerlink\" title=\"19/9/15\"></a>19/9/15</h3><ul>\n<li><code>for...in</code>å¾ªç¯ä¸­ï¼Œ<code>_</code>å¯ä»¥ä½œä¸ºå¾ªç¯å˜é‡ï¼Œè¿™æ—¶å€™ä»…å¾ªç¯æŒ‡å®šæ¬¡æ•°ï¼Œè€Œä¸éœ€è¦å…³å¿ƒå¾ªç¯å˜é‡çš„å€¼ï¼›äº‹å®ä¸Šï¼Œ<code>_</code>æ˜¯ä¸€ä¸ªåˆæ³•çš„æ ‡è¯†ç¬¦ï¼Œå¦‚æœä¸å…³å¿ƒè¿™ä¸ªå˜é‡ï¼Œå°±å¯ä»¥å°†å…¶å®šä¹‰æˆè¿™ä¸ªå€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ªåƒåœ¾æ¡¶</li>\n<li>å®šä¹‰å‡½æ•°æ—¶ï¼Œä½¿ç”¨<code>å‡½æ•°å(*å‚æ•°å)</code>çš„å®šä¹‰æ–¹å¼ï¼Œ <code>*</code> ä»£è¡¨å‡½æ•°çš„å‚æ•°æ˜¯å¯å˜å‚æ•°ï¼Œå¯ä»¥æœ‰0åˆ°å¤šä¸ªå‚æ•°</li>\n<li>ä¸€ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ¨¡å—(module)ï¼Œè‹¥åœ¨ä¸åŒçš„æ¨¡å—ä¸­å«æœ‰åŒåå‡½æ•°ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡<code>import</code>å¯¼å…¥æŒ‡å®šçš„æ¨¡å—ä¸­çš„å‡½æ•°ï¼Œå¦‚<code>from æ¨¡å— import å‡½æ•°</code>ï¼Œæˆ–è€…<code>import æ¨¡å— as è‡ªå®šä¹‰æ¨¡å—åç§°</code>ï¼Œå†é€šè¿‡<code>è‡ªå®šä¹‰æ¨¡å—åç§°.å‡½æ•°</code>çš„æ–¹å¼è°ƒç”¨</li>\n<li><code>__name__</code>æ˜¯Pythonä¸­ä¸€ä¸ªéšå«çš„å˜é‡ï¼Œä»£è¡¨äº†æ¨¡å—çš„åå­—ï¼Œåªç”¨ç›´æ¥æ‰§è¡Œçš„æ¨¡å—çš„åå­—æ‰æ˜¯<code>__main__</code></li>\n<li>å¯ä»¥ä½¿ç”¨<code>global</code>æŒ‡å®šä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œå¦‚æœå…¨å±€å˜é‡ä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ï¼Œé‚£ä¹ˆä¼šå®šä¹‰ä¸€ä¸ªæ–°çš„å…¨å±€å˜é‡</li>\n<li>åµŒå¥—ä½œç”¨åŸŸï¼šå¯¹äºå‡½æ•°aå†…éƒ¨çš„å‡½æ•°bè€Œè¨€ï¼Œaä¸­å®šä¹‰çš„å˜é‡å¯¹bæ¥è¯´æ˜¯åœ¨åµŒå¥—ä½œç”¨åŸŸä¸­çš„ï¼Œè‹¥è¦æŒ‡å®šä¿®æ”¹åµŒå¥—ä½œç”¨åŸŸä¸­çš„å˜é‡ï¼Œå¯ä»¥ä½¿ç”¨<code>nonlocal</code>æŒ‡ç¤ºå˜é‡æ¥è‡ªåµŒå¥—ä½œç”¨åŸŸ</li>\n<li><code>pass</code>æ˜¯ä¸€ä¸ªç©ºè¯­å¥ï¼Œåªèµ·åˆ°å ä½ä½œç”¨</li>\n<li>å¯ä»¥å®šä¹‰ä¸€ä¸ª<code>main</code>å‡½æ•°ï¼ˆæˆ–è€…ä¸æ¨¡å—åå­—ç›¸åŒçš„å‡½æ•°ï¼‰ï¼Œå†æŒ‰ç…§<code>if __name__ = &#39;__main__&#39;</code>çš„æ ¼å¼ä½¿è„šæœ¬æ‰§è¡Œ</li>\n</ul>\n<h3 id=\"19-9-17\"><a href=\"#19-9-17\" class=\"headerlink\" title=\"19/9/17\"></a>19/9/17</h3><ul>\n<li>ä¸å­—ç¬¦ä¸²æœ‰å…³çš„å‡½æ•°çš„è°ƒç”¨æ–¹å¼ä¸ºï¼š<code>å­—ç¬¦ä¸²åç§°.å­—ç¬¦ä¸²æ“ä½œå‡½æ•°()</code>ï¼Œåœ¨æ­¤æ—¶å­—ç¬¦ä¸²æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå­—ç¬¦ä¸²æ“ä½œå‡½æ•°çš„ä½œç”¨æ˜¯å‘å­—ç¬¦ä¸²å¯¹è±¡å‘é€ä¸€ä¸ªæ¶ˆæ¯</li>\n<li>å­—ç¬¦ä¸²å®è´¨ä¸Šæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯ä»¥è¿›è¡Œä¸‹æ ‡è¿ç®—</li>\n<li>å­—ç¬¦ä¸²åˆ‡ç‰‡å¯ä»¥åœ¨ä¸‹æ ‡è¿ç®—ä¸­ä½¿ç”¨å†’å·è¿›è¡Œè¿ç®—ï¼Œ<code>[èµ·å§‹å­—ç¬¦:ç»“æŸå­—ç¬¦:é—´éš”]</code>ï¼Œè‹¥ä¸å®šä¹‰èµ·å§‹ä¸ç»ˆæ­¢å­—ç¬¦ï¼Œåˆ™é»˜è®¤ä¸ºæ•´ä¸ªå­—ç¬¦ä¸²ï¼Œå½“é—´éš”ä¸ºè´Ÿå€¼æ—¶ï¼Œä»¥ä¸ºç€åˆ‡ç‰‡æ“ä½œåå‘</li>\n<li>å­—ç¬¦ä¸²çš„ç´¢å¼•ä¸ºè´Ÿå€¼æ—¶ï¼Œæ„å‘³ç€ç´¢å¼•ä»å³åˆ°å·¦æ•°</li>\n<li>åˆ—è¡¨å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶æ“ä½œä¸å­—ç¬¦ä¸²ç±»ä¼¼</li>\n<li>å¯ä½¿ç”¨<code>sorted</code>å‡½æ•°å¯¹åˆ—è¡¨è¿›è¡Œæ’åº</li>\n<li>å¯ä»¥ä½¿ç”¨ç”Ÿæˆå¼è¯­æ³•åˆ›å»ºåˆ—è¡¨ï¼š<code>f = [x for x in range(1, 10)]</code>ï¼ˆæ­¤æ–¹æ³•åœ¨åˆ›å»ºåˆ—è¡¨åå…ƒç´ å·²ç»å‡†å¤‡å°±ç»ªï¼Œè€—è´¹è¾ƒå¤šå†…å­˜ï¼‰ï¼Œæˆ–<code>f = (x for x in range(1, 10))</code>ï¼ˆæ­¤æ–¹æ³•åˆ›å»ºçš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼Œéœ€è¦æ•°æ®æ—¶åˆ—è¡¨é€šè¿‡ç”Ÿæˆå™¨äº§ç”Ÿï¼ŒèŠ‚çœå†…å­˜ä½†æ˜¯è€—è´¹è¾ƒå¤šæ—¶é—´ï¼‰</li>\n<li>å¯ä»¥ä½¿ç”¨<code>yield</code>å…³é”®å­—æ¥å®ç°è¿­ä»£ï¼Œä½¿ç”¨<code>yield</code>å°±æ˜¯äº§ç”Ÿäº†ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡é‡åˆ°<code>yield</code>æ—¶å‡½æ•°ä¼šæš‚åœå¹¶ä¿å­˜å½“å‰æ‰€æœ‰çš„è¿è¡Œä¿¡æ¯ï¼Œè¿”å›<code>yield</code>çš„å€¼ï¼Œå¹¶åœ¨ä¸‹ä¸€æ¬¡æ‰§è¡Œæ­¤æ–¹æ³•æ˜¯ä»å½“å‰ä½ç½®å¼€å§‹è¿è¡Œ</li>\n<li>å¯ä»¥å®šä¹‰å…ƒç»„ï¼Œå…¶ç›¸å½“äºä¸èƒ½ä¿®æ”¹çš„æ•°ç»„ï¼Œä¸€ä¸ªå…ƒç»„ä¸­çš„å…ƒç´ æ•°æ®ç±»å‹å¯ä»¥ä¸åŒï¼Œå®šä¹‰å…ƒç»„ä½¿ç”¨<code>t = ()</code></li>\n<li>åˆ—è¡¨å’Œå…ƒç»„å¯ä»¥äº’ç›¸è½¬æ¢</li>\n<li>å¯ä»¥å®šä¹‰é›†åˆï¼Œå®šä¹‰é›†åˆå¯ä»¥ä½¿ç”¨<code>set = {}</code>ï¼Œå…ƒç»„å¯ä»¥è½¬æ¢ä¸ºé›†åˆ</li>\n<li>å­—å…¸ç±»ä¼¼äºæ•°ç»„ï¼Œä½†æ˜¯å®ƒæ˜¯ç”±å¤šç»„é”®å€¼å¯¹ç»„æˆçš„</li>\n</ul>\n<h3 id=\"19-9-19\"><a href=\"#19-9-19\" class=\"headerlink\" title=\"19/9/19\"></a>19/9/19</h3><ul>\n<li>ä½¿ç”¨classå…³é”®å­—å®šä¹‰ç±»ï¼Œå†åœ¨ç±»ä¸­å®šä¹‰å‡½æ•°ï¼Œå¦‚ï¼š<code>class ç±»å(object)</code></li>\n<li><code>__init__</code>å‡½æ•°æ˜¯ç”¨äºåœ¨åˆ›å»ºå¯¹è±¡æ—¶è¿›è¡Œçš„åˆå§‹åŒ–æ“ä½œ</li>\n<li>selfæ˜¯ç±»çš„æœ¬èº«ï¼Œæ˜¯å®ƒçš„å®ä¾‹å˜é‡ï¼Œåœ¨ç±»ä¸­æ‰€æœ‰å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°å°±æ˜¯selfï¼Œåœ¨ç±»ä¸­ä¿®æ”¹å±æ€§å€¼éœ€ä½¿ç”¨<code>self.å±æ€§å€¼ = x</code>çš„è¯­æ³•</li>\n<li>å®ä¾‹åŒ–ç±»çš„æ–¹æ³•ï¼š<code>å¯¹è±¡å = ç±»å(åˆå§‹åŒ–å‡½æ•°å‚æ•°)</code></li>\n<li>å¯¹è±¡ä¸­æ–¹æ³•çš„å¼•ç”¨å¯ä»¥é‡‡ç”¨<code>å¯¹è±¡.æ–¹æ³•ï¼ˆä¹Ÿå³å‡½æ•°ï¼‰</code>çš„è¯­å¥ï¼Œé€šè¿‡æ­¤æ–¹å¼å‘å¯¹è±¡å‘é€æ¶ˆæ¯</li>\n<li>Pythonä¸­ï¼Œå±æ€§å’Œæ–¹æ³•çš„è®¿é—®æƒé™åªæœ‰<code>public</code>å’Œ<code>private</code>ï¼Œè‹¥å¸Œæœ›å±æ€§æˆ–æ–¹æ³•æ˜¯ç§æœ‰çš„ï¼Œåœ¨ç»™å®ƒä»¬å‘½åçš„æ—¶å€™è¦ä½¿ç”¨<code>__</code>å¼€å¤´ï¼Œä½†æ˜¯ä¸å»ºè®®å°†å±æ€§è®¾ç½®ä¸ºç§æœ‰çš„</li>\n<li>ä½¿ç”¨<code>_</code>å¼€å¤´æš—ç¤ºå±æ€§æˆ–æ–¹æ³•æ˜¯å—ä¿æŠ¤(protected)çš„ï¼Œè®¿é—®å®ƒä»¬å»ºè®®é€šè¿‡ç‰¹å®šçš„æ–¹æ³•ï¼Œä½†å®é™…ä¸Šå®ƒä»¬è¿˜æ˜¯å¯ä»¥ç›´æ¥è¢«å¤–éƒ¨è®¿é—®</li>\n<li>å¯ä»¥é€šè¿‡åœ¨ç±»ä¸­å®šä¹‰æ–¹æ³•ä»¥è®¿é—®å¯¹è±¡å—ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨å®šä¹‰è¿™äº›æ–¹æ³•ï¼ˆå‡½æ•°ï¼‰æ—¶ï¼Œè¦åœ¨ä¸Šä¸€è¡Œä½¿ç”¨<code>@property</code>åŒ…è£…è¿™äº›æ–¹æ³•</li>\n<li>å¯¹äºè¢«ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨è®¿é—®å®ƒä»¬æ—¶é‡‡ç”¨<code>getter</code>æ–¹æ³•ï¼Œéœ€æ·»åŠ <code>@property</code>ï¼Œåœ¨ä¿®æ”¹å®ƒä»¬æ—¶é‡‡ç”¨<code>setter</code>æ–¹æ³•ï¼Œéœ€æ·»åŠ <code>@å‡½æ•°ï¼ˆå³æ–¹æ³•ï¼‰å.setter</code></li>\n<li>Pythonå¯ä»¥å¯¹å¯¹è±¡åŠ¨æ€ç»‘å®šæ–°çš„å±æ€§æˆ–æ–¹æ³•</li>\n<li>å¯ä»¥ä½¿ç”¨<code>__slots__</code>é™å®šå¯¹è±¡åªèƒ½ç»‘å®šæŸäº›å±æ€§ï¼Œä½†æ˜¯å®ƒåªå¯¹å½“å‰ç±»çš„å¯¹è±¡ç”Ÿæ•ˆï¼Œå¯¹å­ç±»ä¸èµ·ä½œç”¨</li>\n<li>å¯ä»¥é€šè¿‡ç»™ç±»å‘é€æ¶ˆæ¯ï¼Œåœ¨ç±»çš„å¯¹è±¡è¢«åˆ›å»ºå‡ºæ¥ä¹‹å‰ç›´æ¥ä½¿ç”¨å…¶ä¸­çš„æ–¹æ³•ï¼Œæ­¤ç§æ–¹æ³•è¢«ç§°ä¸ºé™æ€æ–¹æ³•ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ <code>@staticmethod</code>ï¼Œæ­¤ç±»æ–¹æ³•çš„å‚æ•°ä¸å«æœ‰<code>self</code></li>\n<li>é€šè¿‡ç±»æ–¹æ³•å¯ä»¥è·å–ç±»ç›¸å…³çš„ä¿¡æ¯å¹¶ä¸”å¯ä»¥<strong>åˆ›å»ºå‡ºç±»çš„å¯¹è±¡</strong>ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ <code>@classmethod</code>ï¼Œç±»æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯<code>cls</code>ï¼Œè¿™ä¸ª<code>cls</code>ç›¸å½“äºå°±æ˜¯åœ¨å¤–éƒ¨å®ä¾‹åŒ–ç±»æ—¶å®šä¹‰çš„å¯¹è±¡åï¼Œåªä¸è¿‡å®ƒæ˜¯æ”¾åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨äº†ï¼Œå…¶åŠŸèƒ½å°±æ˜¯å¯ä»¥åƒåœ¨å¤–éƒ¨è°ƒç”¨å¯¹è±¡çš„å±æ€§å’Œæ–¹æ³•ä¸€æ ·åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨å¯¹è±¡ï¼ˆç±»ï¼‰çš„å±æ€§å’Œæ–¹æ³•</li>\n</ul>\n<h3 id=\"19-9-20\"><a href=\"#19-9-20\" class=\"headerlink\" title=\"19/9/20\"></a>19/9/20</h3><ul>\n<li>ç±»ä¹‹é—´çš„å…³ç³»ï¼š<ul>\n<li>is-aï¼šç»§æ‰¿æˆ–è€…æ³›åŒ–ï¼Œå¦‚ï¼š<strong>student</strong> is a <strong>human being</strong>ï¼Œ<strong>cell phone</strong> is a <strong>electronic device</strong></li>\n<li>has-aï¼šå…³è”ï¼Œå¦‚ <strong>department</strong> has an <strong>employee</strong></li>\n<li>use-aï¼šä¾èµ–ï¼Œå¦‚ <strong>driver</strong> use a <strong>car</strong> </li>\n</ul>\n</li>\n<li>ç±»ä¸ç±»ä¹‹é—´å¯ä»¥ç»§æ‰¿ï¼Œæä¾›ç»§æ‰¿ä¿¡æ¯çš„æˆä¸ºçˆ¶ç±»ï¼ˆè¶…ç±»æˆ–è€…åŸºç±»ï¼‰ï¼Œå¾—åˆ°ç»§æ‰¿çš„ç§°ä¸ºå­ç±»ï¼ˆæ´¾ç”Ÿç±»æˆ–è€…è¡ç”Ÿç±»ï¼‰</li>\n<li>Pythonä¸­ç»§æ‰¿çš„å†™æ³•ï¼š<code>class å­ç±»å(åŸºç±»å)</code></li>\n<li>åœ¨ç¼–ç¨‹ä¸­ä¸€èˆ¬ä½¿ç”¨å­ç±»å»æ›¿ä»£åŸºç±»</li>\n<li>åœ¨å­ç±»ä¸­ï¼Œé€šè¿‡é‡æ–°å®šä¹‰çˆ¶ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥è®©åŒä¸€ç§æ–¹æ³•åœ¨ä¸åŒçš„å­ç±»ä¸­æœ‰ä¸åŒçš„è¡Œä¸ºï¼Œè¿™ç§°ä¸ºé‡å†™</li>\n</ul>\n<h3 id=\"20-1-11\"><a href=\"#20-1-11\" class=\"headerlink\" title=\"20/1/11\"></a>20/1/11</h3><ul>\n<li>Pythonä¸­æä¾›ä¸¤ä¸ªé‡è¦çš„åŠŸèƒ½ï¼šå¼‚å¸¸å¤„ç†å’Œæ–­è¨€ï¼ˆAssertionsï¼‰æ¥å¤„ç†è¿è¡Œä¸­å‡ºç°çš„å¼‚å¸¸å’Œé”™è¯¯ï¼Œä»–ä»¬çš„åŠŸèƒ½æ˜¯ç”¨äºè°ƒè¯•Pythonç¨‹åº</li>\n<li>å¼‚å¸¸ï¼šæ— æ³•æ­£å¸¸å¤„ç†ç¨‹åºæ—¶ä¼šå‘ç”Ÿå¼‚å¸¸ï¼Œæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚æœä¸æ•è·å¼‚å¸¸ï¼Œç¨‹åºä¼šç»ˆæ­¢æ‰§è¡Œ</li>\n<li>Pythonä¸­å¼‚å¸¸å¤„ç†çš„å†™æ³•ï¼š</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">except</span> exception_type, argument:</span><br><span class=\"line\">\t<span class=\"comment\">#if error occurs in operation1, execute operation2</span></span><br><span class=\"line\">  <span class=\"comment\">#operation2</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#if no error occurs in operation1, execute operation3</span></span><br><span class=\"line\">  <span class=\"comment\">#operation3</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>ä½¿ç”¨<code>except</code>å¯ä»¥ä¸å¸¦å¼‚å¸¸ç±»å‹ï¼Œä½†æ˜¯ä¼šè®©<code>try-except</code>è¯­å¥æ•è·æ‰€æœ‰çš„å¼‚å¸¸ï¼Œä¸å»ºè®®è¿™æ ·å†™</li>\n<li>å¯ä»¥ä½¿ç”¨<code>expect(exception1[, expection2[, expection3]])</code>æ¥æ·»åŠ å¤šä¸ªå¼‚å¸¸ç±»å‹</li>\n<li><code>argument</code>ä¸ºå¼‚å¸¸çš„å‚æ•°ï¼Œå¯ä»¥ç”¨äºè¾“å‡ºå¼‚å¸¸ä¿¡æ¯çš„å¼‚å¸¸å€¼</li>\n<li>ä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ï¼Œä½†æ˜¯ä¸<code>try-except</code>æœ‰æ‰€ä¸åŒï¼š</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">finally</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p><code>finally</code>å’Œ<code>except</code>ä¸å¯ä»¥åŒæ—¶ä½¿ç”¨</p>\n</li>\n<li><p>å¯ä»¥ä½¿ç”¨<code>raise</code>è§¦å‘å¼‚å¸¸</p>\n</li>\n<li><p><code>append()</code>æ–¹æ³•ç”¨äºåœ¨åˆ—è¡¨æœ«å°¾æ·»åŠ æ–°çš„å¯¹è±¡ï¼Œå¯¹äºä¸€ä¸ªæ•°ç»„<code>list</code>ï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š<code>list.append()</code></p>\n</li>\n<li><p>å¤šçº¿ç¨‹ç”¨äºåŒæ—¶æ‰§è¡Œå¤šä¸ªä¸åŒçš„ç¨‹åºï¼Œå¯ä»¥æŠŠå æ®é•¿æ—¶é—´çš„ç¨‹åºä¸­çš„ä»»åŠ¡æ”¾åˆ°åå°å¤„ç†</p>\n</li>\n<li><p>çº¿ç¨‹ä¸è¿›ç¨‹ï¼šç‹¬ç«‹çš„çº¿ç¨‹æœ‰è‡ªå·±çš„ç¨‹åºå…¥å£ã€æ‰§è¡Œåºåˆ—ã€ç¨‹åºå‡ºå£ï¼Œä½†æ˜¯çº¿ç¨‹ä¸å¯ä»¥ç‹¬ç«‹æ‰§è¡Œï¼Œå¿…é¡»ä¾å­˜åœ¨åº”ç”¨ç¨‹åºä¸­ï¼Œç”±åº”ç”¨ç¨‹åºæä¾›å¤šä¸ªçº¿ç¨‹æ‰§è¡Œæ§åˆ¶</p>\n</li>\n<li><p>åœ¨Pythonä¸­ä½¿ç”¨çº¿ç¨‹ï¼š<code>thread.start_new_thread(function, args[, kwargs])</code>ï¼Œå…¶ä¸­<code>function</code>ä¸ºçº¿ç¨‹å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°éœ€è¦æå‰å®šä¹‰å¥½ï¼Œ<code>args</code>ä¸ºä¼ é€’ç»™çº¿ç¨‹å‡½æ•°çš„å‚æ•°ï¼Œæ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œ<code>kwargs</code>ä¸ºå¯é€‰å‚æ•°ï¼Œæ­¤ç§æ–¹å¼ç§°ä¸ºå‡½æ•°å¼ï¼Œçº¿ç¨‹çš„ç»“æŸä¸€èˆ¬é å‡½æ•°çš„è‡ªç„¶ç»“æŸ</p>\n</li>\n<li><p>æ­¤å¤–è¿˜å¯ä»¥ä½¿ç”¨Pythonæ‰€æä¾›çš„<code>threading</code>æ¨¡å—ï¼Œç›´æ¥ä»<code>threading.Thread</code>ç»§æ‰¿ï¼š<code>class myThread(threading.Thread)</code>ï¼Œç„¶åé‡å†™<code>__init__</code>å’Œ<code>run</code>æ–¹æ³•ï¼ŒæŠŠéœ€è¦æ‰§è¡Œçš„ä»£ç å†™åˆ°<code>run</code>æ–¹æ³•é‡Œé¢ï¼Œ<code>__init__</code>çš„é‡å†™æ–¹æ³•å¦‚ä¸‹ï¼š</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, threadID, name, counter)</span>:</span></span><br><span class=\"line\">\tthreading.Thread.__init__(self)</span><br><span class=\"line\">\tself.threadID = threadID</span><br><span class=\"line\">\tself.name = name</span><br><span class=\"line\">\tself.counter = counter</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>ä¸Šè¿°<code>thread</code>ç±»æä¾›äº†ä»¥ä¸‹æ–¹æ³•ï¼š<ul>\n<li><code>run()</code>ï¼šè¡¨ç¤ºçº¿ç¨‹æ´»åŠ¨çš„æ–¹æ³•</li>\n<li><code>start</code>ï¼šå¯åŠ¨çº¿ç¨‹</li>\n<li><code>join()</code>ï¼šç­‰å¾…ç›´åˆ°çº¿ç¨‹ç»ˆæ­¢</li>\n<li><code>isAlive()</code>ï¼šæŸ¥è¯¢çº¿ç¨‹æ˜¯å¦æ´»åŠ¨</li>\n<li><code>getName()</code>ï¼šè¿”å›çº¿ç¨‹å</li>\n<li><code>setName()</code>ï¼šè®¾ç½®çº¿ç¨‹å</li>\n</ul>\n</li>\n<li>ä¸ºäº†é¿å…ä¸¤ä¸ªæˆ–å¤šä¸ªçº¿ç¨‹åŒæ—¶è¿è¡Œï¼Œäº§ç”Ÿå†²çªï¼Œå¯ä»¥ä½¿ç”¨çº¿ç¨‹é”æ¥æ§åˆ¶çº¿ç¨‹æ‰§è¡Œçš„ä¼˜å…ˆé¡ºåºï¼Œè¢«é”å®šçš„çº¿ç¨‹ä¼˜å…ˆæ‰§è¡Œï¼Œå…¶ä»–è¿›ç¨‹å¿…é¡»åœæ­¢</li>\n<li>å¯ä»¥ä½¿ç”¨<code>threading.Lock().acquire()</code>å’Œ<code>threading.Lock().release()</code>æ¥é”å®šå’Œé‡Šæ”¾çº¿ç¨‹</li>\n<li>å¯ä»¥å»ºç«‹ä¸€ä¸ªç©ºæ•°ç»„ç”¨äºå­˜æ”¾çº¿ç¨‹ï¼Œå†é€šè¿‡<code>append</code>æ–¹æ³•å°†çº¿ç¨‹æ·»åŠ è‡³è¯¥æ•°ç»„ä¸­ï¼Œé€šè¿‡éå†æ•°ç»„å¯ä»¥å¯¹å…¶ä¸­çš„çº¿ç¨‹åšä¸€æ ·çš„æ“ä½œ</li>\n</ul>\n<h3 id=\"20-1-13\"><a href=\"#20-1-13\" class=\"headerlink\" title=\"20/1/13\"></a>20/1/13</h3><ul>\n<li>åœ¨ç»§æ‰¿çš„æ—¶å€™ï¼Œ</li>\n</ul>\n","site":{"data":{}},"more":"<blockquote>\n<p>è¿™ç¯‡æ–‡ç« ä¸»è¦è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™ä¸ªè¯­è¨€çš„ä¸€äº›ç‰¹æ€§ï¼Œå†…å®¹ä»¥æ—¶é—´é¡ºåºæ•´ç†ï¼Œæ¯”è¾ƒé›¶æ•£æ‚ä¹±ã€‚å¯¹äºä»é›¶å¼€å§‹çš„åŒå­¦ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£<a href=\"https://docs.python.org/zh-cn/3/\">Python 3.8.1 ä¸­æ–‡æ–‡æ¡£</a>æˆ–å…¶ä»–ç½‘ç»œä¸Šçš„æ•™ç¨‹ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚</p>\n</blockquote>\n<h3 id=\"19-9-14\"><a href=\"#19-9-14\" class=\"headerlink\" title=\"19/9/14\"></a>19/9/14</h3><ul>\n<li>æ³¨é‡Šæ–¹æ³•ï¼š<code>#ï¼ˆä¸€è¡Œæ³¨é‡Šï¼‰</code>ï¼Œ<code>â€œâ€â€œ â€â€œâ€ï¼ˆå¤šè¡Œæ³¨é‡Šï¼‰</code></li>\n<li>forå¾ªç¯ï¼š<code>for ï¼ˆå˜é‡ï¼‰ in ï¼ˆèŒƒå›´ï¼‰</code>ï¼ŒèŒƒå›´å¯ä»¥ç”¨<code>range</code>å‡½æ•°</li>\n<li><code>Input</code>å‡½æ•°çš„è¾“å…¥æ˜¯<code>char</code>ç±»å‹çš„</li>\n<li><code>//</code>æ˜¯æ•´é™¤è¿ç®—</li>\n<li>é€—å·ä¸å¯ä»¥ç”¨æ¥åˆ†éš”è¯­å¥</li>\n<li>ä½¿ç”¨ç¼©è¿›ï¼ˆ4ä¸ªç©ºæ ¼ï¼‰æ¥ä»£æ›¿C/C++ä¸­çš„å¤§æ‹¬å·</li>\n</ul>\n<h3 id=\"19-9-15\"><a href=\"#19-9-15\" class=\"headerlink\" title=\"19/9/15\"></a>19/9/15</h3><ul>\n<li><code>for...in</code>å¾ªç¯ä¸­ï¼Œ<code>_</code>å¯ä»¥ä½œä¸ºå¾ªç¯å˜é‡ï¼Œè¿™æ—¶å€™ä»…å¾ªç¯æŒ‡å®šæ¬¡æ•°ï¼Œè€Œä¸éœ€è¦å…³å¿ƒå¾ªç¯å˜é‡çš„å€¼ï¼›äº‹å®ä¸Šï¼Œ<code>_</code>æ˜¯ä¸€ä¸ªåˆæ³•çš„æ ‡è¯†ç¬¦ï¼Œå¦‚æœä¸å…³å¿ƒè¿™ä¸ªå˜é‡ï¼Œå°±å¯ä»¥å°†å…¶å®šä¹‰æˆè¿™ä¸ªå€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ªåƒåœ¾æ¡¶</li>\n<li>å®šä¹‰å‡½æ•°æ—¶ï¼Œä½¿ç”¨<code>å‡½æ•°å(*å‚æ•°å)</code>çš„å®šä¹‰æ–¹å¼ï¼Œ <code>*</code> ä»£è¡¨å‡½æ•°çš„å‚æ•°æ˜¯å¯å˜å‚æ•°ï¼Œå¯ä»¥æœ‰0åˆ°å¤šä¸ªå‚æ•°</li>\n<li>ä¸€ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ¨¡å—(module)ï¼Œè‹¥åœ¨ä¸åŒçš„æ¨¡å—ä¸­å«æœ‰åŒåå‡½æ•°ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡<code>import</code>å¯¼å…¥æŒ‡å®šçš„æ¨¡å—ä¸­çš„å‡½æ•°ï¼Œå¦‚<code>from æ¨¡å— import å‡½æ•°</code>ï¼Œæˆ–è€…<code>import æ¨¡å— as è‡ªå®šä¹‰æ¨¡å—åç§°</code>ï¼Œå†é€šè¿‡<code>è‡ªå®šä¹‰æ¨¡å—åç§°.å‡½æ•°</code>çš„æ–¹å¼è°ƒç”¨</li>\n<li><code>__name__</code>æ˜¯Pythonä¸­ä¸€ä¸ªéšå«çš„å˜é‡ï¼Œä»£è¡¨äº†æ¨¡å—çš„åå­—ï¼Œåªç”¨ç›´æ¥æ‰§è¡Œçš„æ¨¡å—çš„åå­—æ‰æ˜¯<code>__main__</code></li>\n<li>å¯ä»¥ä½¿ç”¨<code>global</code>æŒ‡å®šä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œå¦‚æœå…¨å±€å˜é‡ä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ï¼Œé‚£ä¹ˆä¼šå®šä¹‰ä¸€ä¸ªæ–°çš„å…¨å±€å˜é‡</li>\n<li>åµŒå¥—ä½œç”¨åŸŸï¼šå¯¹äºå‡½æ•°aå†…éƒ¨çš„å‡½æ•°bè€Œè¨€ï¼Œaä¸­å®šä¹‰çš„å˜é‡å¯¹bæ¥è¯´æ˜¯åœ¨åµŒå¥—ä½œç”¨åŸŸä¸­çš„ï¼Œè‹¥è¦æŒ‡å®šä¿®æ”¹åµŒå¥—ä½œç”¨åŸŸä¸­çš„å˜é‡ï¼Œå¯ä»¥ä½¿ç”¨<code>nonlocal</code>æŒ‡ç¤ºå˜é‡æ¥è‡ªåµŒå¥—ä½œç”¨åŸŸ</li>\n<li><code>pass</code>æ˜¯ä¸€ä¸ªç©ºè¯­å¥ï¼Œåªèµ·åˆ°å ä½ä½œç”¨</li>\n<li>å¯ä»¥å®šä¹‰ä¸€ä¸ª<code>main</code>å‡½æ•°ï¼ˆæˆ–è€…ä¸æ¨¡å—åå­—ç›¸åŒçš„å‡½æ•°ï¼‰ï¼Œå†æŒ‰ç…§<code>if __name__ = &#39;__main__&#39;</code>çš„æ ¼å¼ä½¿è„šæœ¬æ‰§è¡Œ</li>\n</ul>\n<h3 id=\"19-9-17\"><a href=\"#19-9-17\" class=\"headerlink\" title=\"19/9/17\"></a>19/9/17</h3><ul>\n<li>ä¸å­—ç¬¦ä¸²æœ‰å…³çš„å‡½æ•°çš„è°ƒç”¨æ–¹å¼ä¸ºï¼š<code>å­—ç¬¦ä¸²åç§°.å­—ç¬¦ä¸²æ“ä½œå‡½æ•°()</code>ï¼Œåœ¨æ­¤æ—¶å­—ç¬¦ä¸²æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå­—ç¬¦ä¸²æ“ä½œå‡½æ•°çš„ä½œç”¨æ˜¯å‘å­—ç¬¦ä¸²å¯¹è±¡å‘é€ä¸€ä¸ªæ¶ˆæ¯</li>\n<li>å­—ç¬¦ä¸²å®è´¨ä¸Šæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯ä»¥è¿›è¡Œä¸‹æ ‡è¿ç®—</li>\n<li>å­—ç¬¦ä¸²åˆ‡ç‰‡å¯ä»¥åœ¨ä¸‹æ ‡è¿ç®—ä¸­ä½¿ç”¨å†’å·è¿›è¡Œè¿ç®—ï¼Œ<code>[èµ·å§‹å­—ç¬¦:ç»“æŸå­—ç¬¦:é—´éš”]</code>ï¼Œè‹¥ä¸å®šä¹‰èµ·å§‹ä¸ç»ˆæ­¢å­—ç¬¦ï¼Œåˆ™é»˜è®¤ä¸ºæ•´ä¸ªå­—ç¬¦ä¸²ï¼Œå½“é—´éš”ä¸ºè´Ÿå€¼æ—¶ï¼Œä»¥ä¸ºç€åˆ‡ç‰‡æ“ä½œåå‘</li>\n<li>å­—ç¬¦ä¸²çš„ç´¢å¼•ä¸ºè´Ÿå€¼æ—¶ï¼Œæ„å‘³ç€ç´¢å¼•ä»å³åˆ°å·¦æ•°</li>\n<li>åˆ—è¡¨å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶æ“ä½œä¸å­—ç¬¦ä¸²ç±»ä¼¼</li>\n<li>å¯ä½¿ç”¨<code>sorted</code>å‡½æ•°å¯¹åˆ—è¡¨è¿›è¡Œæ’åº</li>\n<li>å¯ä»¥ä½¿ç”¨ç”Ÿæˆå¼è¯­æ³•åˆ›å»ºåˆ—è¡¨ï¼š<code>f = [x for x in range(1, 10)]</code>ï¼ˆæ­¤æ–¹æ³•åœ¨åˆ›å»ºåˆ—è¡¨åå…ƒç´ å·²ç»å‡†å¤‡å°±ç»ªï¼Œè€—è´¹è¾ƒå¤šå†…å­˜ï¼‰ï¼Œæˆ–<code>f = (x for x in range(1, 10))</code>ï¼ˆæ­¤æ–¹æ³•åˆ›å»ºçš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼Œéœ€è¦æ•°æ®æ—¶åˆ—è¡¨é€šè¿‡ç”Ÿæˆå™¨äº§ç”Ÿï¼ŒèŠ‚çœå†…å­˜ä½†æ˜¯è€—è´¹è¾ƒå¤šæ—¶é—´ï¼‰</li>\n<li>å¯ä»¥ä½¿ç”¨<code>yield</code>å…³é”®å­—æ¥å®ç°è¿­ä»£ï¼Œä½¿ç”¨<code>yield</code>å°±æ˜¯äº§ç”Ÿäº†ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡é‡åˆ°<code>yield</code>æ—¶å‡½æ•°ä¼šæš‚åœå¹¶ä¿å­˜å½“å‰æ‰€æœ‰çš„è¿è¡Œä¿¡æ¯ï¼Œè¿”å›<code>yield</code>çš„å€¼ï¼Œå¹¶åœ¨ä¸‹ä¸€æ¬¡æ‰§è¡Œæ­¤æ–¹æ³•æ˜¯ä»å½“å‰ä½ç½®å¼€å§‹è¿è¡Œ</li>\n<li>å¯ä»¥å®šä¹‰å…ƒç»„ï¼Œå…¶ç›¸å½“äºä¸èƒ½ä¿®æ”¹çš„æ•°ç»„ï¼Œä¸€ä¸ªå…ƒç»„ä¸­çš„å…ƒç´ æ•°æ®ç±»å‹å¯ä»¥ä¸åŒï¼Œå®šä¹‰å…ƒç»„ä½¿ç”¨<code>t = ()</code></li>\n<li>åˆ—è¡¨å’Œå…ƒç»„å¯ä»¥äº’ç›¸è½¬æ¢</li>\n<li>å¯ä»¥å®šä¹‰é›†åˆï¼Œå®šä¹‰é›†åˆå¯ä»¥ä½¿ç”¨<code>set = {}</code>ï¼Œå…ƒç»„å¯ä»¥è½¬æ¢ä¸ºé›†åˆ</li>\n<li>å­—å…¸ç±»ä¼¼äºæ•°ç»„ï¼Œä½†æ˜¯å®ƒæ˜¯ç”±å¤šç»„é”®å€¼å¯¹ç»„æˆçš„</li>\n</ul>\n<h3 id=\"19-9-19\"><a href=\"#19-9-19\" class=\"headerlink\" title=\"19/9/19\"></a>19/9/19</h3><ul>\n<li>ä½¿ç”¨classå…³é”®å­—å®šä¹‰ç±»ï¼Œå†åœ¨ç±»ä¸­å®šä¹‰å‡½æ•°ï¼Œå¦‚ï¼š<code>class ç±»å(object)</code></li>\n<li><code>__init__</code>å‡½æ•°æ˜¯ç”¨äºåœ¨åˆ›å»ºå¯¹è±¡æ—¶è¿›è¡Œçš„åˆå§‹åŒ–æ“ä½œ</li>\n<li>selfæ˜¯ç±»çš„æœ¬èº«ï¼Œæ˜¯å®ƒçš„å®ä¾‹å˜é‡ï¼Œåœ¨ç±»ä¸­æ‰€æœ‰å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°å°±æ˜¯selfï¼Œåœ¨ç±»ä¸­ä¿®æ”¹å±æ€§å€¼éœ€ä½¿ç”¨<code>self.å±æ€§å€¼ = x</code>çš„è¯­æ³•</li>\n<li>å®ä¾‹åŒ–ç±»çš„æ–¹æ³•ï¼š<code>å¯¹è±¡å = ç±»å(åˆå§‹åŒ–å‡½æ•°å‚æ•°)</code></li>\n<li>å¯¹è±¡ä¸­æ–¹æ³•çš„å¼•ç”¨å¯ä»¥é‡‡ç”¨<code>å¯¹è±¡.æ–¹æ³•ï¼ˆä¹Ÿå³å‡½æ•°ï¼‰</code>çš„è¯­å¥ï¼Œé€šè¿‡æ­¤æ–¹å¼å‘å¯¹è±¡å‘é€æ¶ˆæ¯</li>\n<li>Pythonä¸­ï¼Œå±æ€§å’Œæ–¹æ³•çš„è®¿é—®æƒé™åªæœ‰<code>public</code>å’Œ<code>private</code>ï¼Œè‹¥å¸Œæœ›å±æ€§æˆ–æ–¹æ³•æ˜¯ç§æœ‰çš„ï¼Œåœ¨ç»™å®ƒä»¬å‘½åçš„æ—¶å€™è¦ä½¿ç”¨<code>__</code>å¼€å¤´ï¼Œä½†æ˜¯ä¸å»ºè®®å°†å±æ€§è®¾ç½®ä¸ºç§æœ‰çš„</li>\n<li>ä½¿ç”¨<code>_</code>å¼€å¤´æš—ç¤ºå±æ€§æˆ–æ–¹æ³•æ˜¯å—ä¿æŠ¤(protected)çš„ï¼Œè®¿é—®å®ƒä»¬å»ºè®®é€šè¿‡ç‰¹å®šçš„æ–¹æ³•ï¼Œä½†å®é™…ä¸Šå®ƒä»¬è¿˜æ˜¯å¯ä»¥ç›´æ¥è¢«å¤–éƒ¨è®¿é—®</li>\n<li>å¯ä»¥é€šè¿‡åœ¨ç±»ä¸­å®šä¹‰æ–¹æ³•ä»¥è®¿é—®å¯¹è±¡å—ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨å®šä¹‰è¿™äº›æ–¹æ³•ï¼ˆå‡½æ•°ï¼‰æ—¶ï¼Œè¦åœ¨ä¸Šä¸€è¡Œä½¿ç”¨<code>@property</code>åŒ…è£…è¿™äº›æ–¹æ³•</li>\n<li>å¯¹äºè¢«ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨è®¿é—®å®ƒä»¬æ—¶é‡‡ç”¨<code>getter</code>æ–¹æ³•ï¼Œéœ€æ·»åŠ <code>@property</code>ï¼Œåœ¨ä¿®æ”¹å®ƒä»¬æ—¶é‡‡ç”¨<code>setter</code>æ–¹æ³•ï¼Œéœ€æ·»åŠ <code>@å‡½æ•°ï¼ˆå³æ–¹æ³•ï¼‰å.setter</code></li>\n<li>Pythonå¯ä»¥å¯¹å¯¹è±¡åŠ¨æ€ç»‘å®šæ–°çš„å±æ€§æˆ–æ–¹æ³•</li>\n<li>å¯ä»¥ä½¿ç”¨<code>__slots__</code>é™å®šå¯¹è±¡åªèƒ½ç»‘å®šæŸäº›å±æ€§ï¼Œä½†æ˜¯å®ƒåªå¯¹å½“å‰ç±»çš„å¯¹è±¡ç”Ÿæ•ˆï¼Œå¯¹å­ç±»ä¸èµ·ä½œç”¨</li>\n<li>å¯ä»¥é€šè¿‡ç»™ç±»å‘é€æ¶ˆæ¯ï¼Œåœ¨ç±»çš„å¯¹è±¡è¢«åˆ›å»ºå‡ºæ¥ä¹‹å‰ç›´æ¥ä½¿ç”¨å…¶ä¸­çš„æ–¹æ³•ï¼Œæ­¤ç§æ–¹æ³•è¢«ç§°ä¸ºé™æ€æ–¹æ³•ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ <code>@staticmethod</code>ï¼Œæ­¤ç±»æ–¹æ³•çš„å‚æ•°ä¸å«æœ‰<code>self</code></li>\n<li>é€šè¿‡ç±»æ–¹æ³•å¯ä»¥è·å–ç±»ç›¸å…³çš„ä¿¡æ¯å¹¶ä¸”å¯ä»¥<strong>åˆ›å»ºå‡ºç±»çš„å¯¹è±¡</strong>ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ <code>@classmethod</code>ï¼Œç±»æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯<code>cls</code>ï¼Œè¿™ä¸ª<code>cls</code>ç›¸å½“äºå°±æ˜¯åœ¨å¤–éƒ¨å®ä¾‹åŒ–ç±»æ—¶å®šä¹‰çš„å¯¹è±¡åï¼Œåªä¸è¿‡å®ƒæ˜¯æ”¾åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨äº†ï¼Œå…¶åŠŸèƒ½å°±æ˜¯å¯ä»¥åƒåœ¨å¤–éƒ¨è°ƒç”¨å¯¹è±¡çš„å±æ€§å’Œæ–¹æ³•ä¸€æ ·åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨å¯¹è±¡ï¼ˆç±»ï¼‰çš„å±æ€§å’Œæ–¹æ³•</li>\n</ul>\n<h3 id=\"19-9-20\"><a href=\"#19-9-20\" class=\"headerlink\" title=\"19/9/20\"></a>19/9/20</h3><ul>\n<li>ç±»ä¹‹é—´çš„å…³ç³»ï¼š<ul>\n<li>is-aï¼šç»§æ‰¿æˆ–è€…æ³›åŒ–ï¼Œå¦‚ï¼š<strong>student</strong> is a <strong>human being</strong>ï¼Œ<strong>cell phone</strong> is a <strong>electronic device</strong></li>\n<li>has-aï¼šå…³è”ï¼Œå¦‚ <strong>department</strong> has an <strong>employee</strong></li>\n<li>use-aï¼šä¾èµ–ï¼Œå¦‚ <strong>driver</strong> use a <strong>car</strong> </li>\n</ul>\n</li>\n<li>ç±»ä¸ç±»ä¹‹é—´å¯ä»¥ç»§æ‰¿ï¼Œæä¾›ç»§æ‰¿ä¿¡æ¯çš„æˆä¸ºçˆ¶ç±»ï¼ˆè¶…ç±»æˆ–è€…åŸºç±»ï¼‰ï¼Œå¾—åˆ°ç»§æ‰¿çš„ç§°ä¸ºå­ç±»ï¼ˆæ´¾ç”Ÿç±»æˆ–è€…è¡ç”Ÿç±»ï¼‰</li>\n<li>Pythonä¸­ç»§æ‰¿çš„å†™æ³•ï¼š<code>class å­ç±»å(åŸºç±»å)</code></li>\n<li>åœ¨ç¼–ç¨‹ä¸­ä¸€èˆ¬ä½¿ç”¨å­ç±»å»æ›¿ä»£åŸºç±»</li>\n<li>åœ¨å­ç±»ä¸­ï¼Œé€šè¿‡é‡æ–°å®šä¹‰çˆ¶ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥è®©åŒä¸€ç§æ–¹æ³•åœ¨ä¸åŒçš„å­ç±»ä¸­æœ‰ä¸åŒçš„è¡Œä¸ºï¼Œè¿™ç§°ä¸ºé‡å†™</li>\n</ul>\n<h3 id=\"20-1-11\"><a href=\"#20-1-11\" class=\"headerlink\" title=\"20/1/11\"></a>20/1/11</h3><ul>\n<li>Pythonä¸­æä¾›ä¸¤ä¸ªé‡è¦çš„åŠŸèƒ½ï¼šå¼‚å¸¸å¤„ç†å’Œæ–­è¨€ï¼ˆAssertionsï¼‰æ¥å¤„ç†è¿è¡Œä¸­å‡ºç°çš„å¼‚å¸¸å’Œé”™è¯¯ï¼Œä»–ä»¬çš„åŠŸèƒ½æ˜¯ç”¨äºè°ƒè¯•Pythonç¨‹åº</li>\n<li>å¼‚å¸¸ï¼šæ— æ³•æ­£å¸¸å¤„ç†ç¨‹åºæ—¶ä¼šå‘ç”Ÿå¼‚å¸¸ï¼Œæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚æœä¸æ•è·å¼‚å¸¸ï¼Œç¨‹åºä¼šç»ˆæ­¢æ‰§è¡Œ</li>\n<li>Pythonä¸­å¼‚å¸¸å¤„ç†çš„å†™æ³•ï¼š</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">except</span> exception_type, argument:</span><br><span class=\"line\">\t<span class=\"comment\">#if error occurs in operation1, execute operation2</span></span><br><span class=\"line\">  <span class=\"comment\">#operation2</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#if no error occurs in operation1, execute operation3</span></span><br><span class=\"line\">  <span class=\"comment\">#operation3</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>ä½¿ç”¨<code>except</code>å¯ä»¥ä¸å¸¦å¼‚å¸¸ç±»å‹ï¼Œä½†æ˜¯ä¼šè®©<code>try-except</code>è¯­å¥æ•è·æ‰€æœ‰çš„å¼‚å¸¸ï¼Œä¸å»ºè®®è¿™æ ·å†™</li>\n<li>å¯ä»¥ä½¿ç”¨<code>expect(exception1[, expection2[, expection3]])</code>æ¥æ·»åŠ å¤šä¸ªå¼‚å¸¸ç±»å‹</li>\n<li><code>argument</code>ä¸ºå¼‚å¸¸çš„å‚æ•°ï¼Œå¯ä»¥ç”¨äºè¾“å‡ºå¼‚å¸¸ä¿¡æ¯çš„å¼‚å¸¸å€¼</li>\n<li>ä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ï¼Œä½†æ˜¯ä¸<code>try-except</code>æœ‰æ‰€ä¸åŒï¼š</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">finally</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p><code>finally</code>å’Œ<code>except</code>ä¸å¯ä»¥åŒæ—¶ä½¿ç”¨</p>\n</li>\n<li><p>å¯ä»¥ä½¿ç”¨<code>raise</code>è§¦å‘å¼‚å¸¸</p>\n</li>\n<li><p><code>append()</code>æ–¹æ³•ç”¨äºåœ¨åˆ—è¡¨æœ«å°¾æ·»åŠ æ–°çš„å¯¹è±¡ï¼Œå¯¹äºä¸€ä¸ªæ•°ç»„<code>list</code>ï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š<code>list.append()</code></p>\n</li>\n<li><p>å¤šçº¿ç¨‹ç”¨äºåŒæ—¶æ‰§è¡Œå¤šä¸ªä¸åŒçš„ç¨‹åºï¼Œå¯ä»¥æŠŠå æ®é•¿æ—¶é—´çš„ç¨‹åºä¸­çš„ä»»åŠ¡æ”¾åˆ°åå°å¤„ç†</p>\n</li>\n<li><p>çº¿ç¨‹ä¸è¿›ç¨‹ï¼šç‹¬ç«‹çš„çº¿ç¨‹æœ‰è‡ªå·±çš„ç¨‹åºå…¥å£ã€æ‰§è¡Œåºåˆ—ã€ç¨‹åºå‡ºå£ï¼Œä½†æ˜¯çº¿ç¨‹ä¸å¯ä»¥ç‹¬ç«‹æ‰§è¡Œï¼Œå¿…é¡»ä¾å­˜åœ¨åº”ç”¨ç¨‹åºä¸­ï¼Œç”±åº”ç”¨ç¨‹åºæä¾›å¤šä¸ªçº¿ç¨‹æ‰§è¡Œæ§åˆ¶</p>\n</li>\n<li><p>åœ¨Pythonä¸­ä½¿ç”¨çº¿ç¨‹ï¼š<code>thread.start_new_thread(function, args[, kwargs])</code>ï¼Œå…¶ä¸­<code>function</code>ä¸ºçº¿ç¨‹å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°éœ€è¦æå‰å®šä¹‰å¥½ï¼Œ<code>args</code>ä¸ºä¼ é€’ç»™çº¿ç¨‹å‡½æ•°çš„å‚æ•°ï¼Œæ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œ<code>kwargs</code>ä¸ºå¯é€‰å‚æ•°ï¼Œæ­¤ç§æ–¹å¼ç§°ä¸ºå‡½æ•°å¼ï¼Œçº¿ç¨‹çš„ç»“æŸä¸€èˆ¬é å‡½æ•°çš„è‡ªç„¶ç»“æŸ</p>\n</li>\n<li><p>æ­¤å¤–è¿˜å¯ä»¥ä½¿ç”¨Pythonæ‰€æä¾›çš„<code>threading</code>æ¨¡å—ï¼Œç›´æ¥ä»<code>threading.Thread</code>ç»§æ‰¿ï¼š<code>class myThread(threading.Thread)</code>ï¼Œç„¶åé‡å†™<code>__init__</code>å’Œ<code>run</code>æ–¹æ³•ï¼ŒæŠŠéœ€è¦æ‰§è¡Œçš„ä»£ç å†™åˆ°<code>run</code>æ–¹æ³•é‡Œé¢ï¼Œ<code>__init__</code>çš„é‡å†™æ–¹æ³•å¦‚ä¸‹ï¼š</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, threadID, name, counter)</span>:</span></span><br><span class=\"line\">\tthreading.Thread.__init__(self)</span><br><span class=\"line\">\tself.threadID = threadID</span><br><span class=\"line\">\tself.name = name</span><br><span class=\"line\">\tself.counter = counter</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>ä¸Šè¿°<code>thread</code>ç±»æä¾›äº†ä»¥ä¸‹æ–¹æ³•ï¼š<ul>\n<li><code>run()</code>ï¼šè¡¨ç¤ºçº¿ç¨‹æ´»åŠ¨çš„æ–¹æ³•</li>\n<li><code>start</code>ï¼šå¯åŠ¨çº¿ç¨‹</li>\n<li><code>join()</code>ï¼šç­‰å¾…ç›´åˆ°çº¿ç¨‹ç»ˆæ­¢</li>\n<li><code>isAlive()</code>ï¼šæŸ¥è¯¢çº¿ç¨‹æ˜¯å¦æ´»åŠ¨</li>\n<li><code>getName()</code>ï¼šè¿”å›çº¿ç¨‹å</li>\n<li><code>setName()</code>ï¼šè®¾ç½®çº¿ç¨‹å</li>\n</ul>\n</li>\n<li>ä¸ºäº†é¿å…ä¸¤ä¸ªæˆ–å¤šä¸ªçº¿ç¨‹åŒæ—¶è¿è¡Œï¼Œäº§ç”Ÿå†²çªï¼Œå¯ä»¥ä½¿ç”¨çº¿ç¨‹é”æ¥æ§åˆ¶çº¿ç¨‹æ‰§è¡Œçš„ä¼˜å…ˆé¡ºåºï¼Œè¢«é”å®šçš„çº¿ç¨‹ä¼˜å…ˆæ‰§è¡Œï¼Œå…¶ä»–è¿›ç¨‹å¿…é¡»åœæ­¢</li>\n<li>å¯ä»¥ä½¿ç”¨<code>threading.Lock().acquire()</code>å’Œ<code>threading.Lock().release()</code>æ¥é”å®šå’Œé‡Šæ”¾çº¿ç¨‹</li>\n<li>å¯ä»¥å»ºç«‹ä¸€ä¸ªç©ºæ•°ç»„ç”¨äºå­˜æ”¾çº¿ç¨‹ï¼Œå†é€šè¿‡<code>append</code>æ–¹æ³•å°†çº¿ç¨‹æ·»åŠ è‡³è¯¥æ•°ç»„ä¸­ï¼Œé€šè¿‡éå†æ•°ç»„å¯ä»¥å¯¹å…¶ä¸­çš„çº¿ç¨‹åšä¸€æ ·çš„æ“ä½œ</li>\n</ul>\n<h3 id=\"20-1-13\"><a href=\"#20-1-13\" class=\"headerlink\" title=\"20/1/13\"></a>20/1/13</h3><ul>\n<li>åœ¨ç»§æ‰¿çš„æ—¶å€™ï¼Œ</li>\n</ul>\n"},{"title":"Summarize of Reinforcement Learning 1","date":"2020-01-17T13:14:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t3.jpeg","excerpt":"A brief introduction to reinforcement learning.","_content":"\n### Preface\n\nThis blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. \n\nThese series of blogs of mine are mostly based on the following works and I'm really grateful to the contributors: \n\n- Online courses of [Stanford University CS234: Reinforcement Learning, Emma Brunskill](https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) and the [notes](https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn).\n- [Blogs of ä»æµåŸŸåˆ°æµ·åŸŸ](https://blog.csdn.net/solo95/category_9298323.html).\n- [Blogs of å¶å¼º](https://zhuanlan.zhihu.com/reinforce).\n\nIf you find any mistake in my articles, please feel free to tell me in comments.\n\n### What is reinforcement learning (RL)?\n\nRL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.\n\nA RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent's desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more \"smarter\" and has a better performance.\n\n### Some basic notions of RL\n\nBecause in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce \"time\" to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript \"t\" means time it is in a time sequence. \n\n- **Agent**: The subject of RL, it is agent that interact with the world.\n- **Model**: The world, the environment, the *agent* stays in the *model*.\n- **Reward**: $ \\{r_t\\} $ , the feedback signal from the *model*, *agent* recieves the *reward*. The *reward* can have different values according to the different **states** of the *agent*.\n- **State**: $\\{s_t\\}$ , the *state* of the *agent*. The *state* can be either finite or infinite, and it is set by people.\n- **Action**: $\\{a_t\\}$ , the movement of the *agent* in the *model*, *actions* are different under different *states*.\n- **Observation**: $\\{o_t\\}$ , the *agent* need to observe its *state* and determine the *reward*.\n- **History**: a sequence of *action*, *reward*, *observation*, which is: $h_t=(a_1,o_1,r_1,...,a_t,o_t,r_t)$.\n- **Sequential Decision Making**: make decision base on the *history*, that is: $a_{t+1}=f(h_t)$.\n\nFigure 1.1 shows how an agent interact with its world.\n\n![Figure 1.1](https://astrobear.top/resource/astroblog/content/rl1.1.jpeg)\n\n### How to model the world?\n\n#### Markov Property\n\n$P(s_{t+1}|s_t,a_t,...,s_1,a_1)=P(s_{t+1}|s_t,a_t)$\n\nLeft-hand side is called the *transition dynamics* of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. \n\nA model consists of the two elements below. \n\n#### Transition dynamics $P(s_{t+1}|s_t,a_t)$\n\nThe probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. \n\n#### Reward function $R(s,a)$\n\nUsually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n### How to make a RL agent?\n\nLet the agent state be a function of the history, $s_t^a=g(h_t)$.\n\nAn agent often consists the three elements below.\n\n#### Policy $\\pi(a_t|s_a^t)$\n\nPolicy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability\n\n$P(a_t=a)=\\pi(a|s_t^a)$.\n\n#### Value function $V^\\pi$\n\nIf we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards\n\n$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+...|s_t=s]$.\n\n#### Model\n\nThe agent in RL may have a model. I have introduced how to make a model in section 3.\n\n### Three questions we are facing\n\n#### Do we need exploration or exploitation?\n\nIn RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.\n\n#### Can the agent generalize its experience?\n\nIn actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?\n\n#### Delayed consequences\n\nThe action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?\n\n### What's next?\n\nNow we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what's the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.","source":"_posts/RLSummarize1.md","raw":"---\ntitle: Summarize of Reinforcement Learning 1\ndate: 2020-1-17 21:14:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t3.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: A brief introduction to reinforcement learning.\n\n#You can begin to input your article below now.\n\n---\n\n### Preface\n\nThis blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. \n\nThese series of blogs of mine are mostly based on the following works and I'm really grateful to the contributors: \n\n- Online courses of [Stanford University CS234: Reinforcement Learning, Emma Brunskill](https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) and the [notes](https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn).\n- [Blogs of ä»æµåŸŸåˆ°æµ·åŸŸ](https://blog.csdn.net/solo95/category_9298323.html).\n- [Blogs of å¶å¼º](https://zhuanlan.zhihu.com/reinforce).\n\nIf you find any mistake in my articles, please feel free to tell me in comments.\n\n### What is reinforcement learning (RL)?\n\nRL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.\n\nA RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent's desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more \"smarter\" and has a better performance.\n\n### Some basic notions of RL\n\nBecause in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce \"time\" to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript \"t\" means time it is in a time sequence. \n\n- **Agent**: The subject of RL, it is agent that interact with the world.\n- **Model**: The world, the environment, the *agent* stays in the *model*.\n- **Reward**: $ \\{r_t\\} $ , the feedback signal from the *model*, *agent* recieves the *reward*. The *reward* can have different values according to the different **states** of the *agent*.\n- **State**: $\\{s_t\\}$ , the *state* of the *agent*. The *state* can be either finite or infinite, and it is set by people.\n- **Action**: $\\{a_t\\}$ , the movement of the *agent* in the *model*, *actions* are different under different *states*.\n- **Observation**: $\\{o_t\\}$ , the *agent* need to observe its *state* and determine the *reward*.\n- **History**: a sequence of *action*, *reward*, *observation*, which is: $h_t=(a_1,o_1,r_1,...,a_t,o_t,r_t)$.\n- **Sequential Decision Making**: make decision base on the *history*, that is: $a_{t+1}=f(h_t)$.\n\nFigure 1.1 shows how an agent interact with its world.\n\n![Figure 1.1](https://astrobear.top/resource/astroblog/content/rl1.1.jpeg)\n\n### How to model the world?\n\n#### Markov Property\n\n$P(s_{t+1}|s_t,a_t,...,s_1,a_1)=P(s_{t+1}|s_t,a_t)$\n\nLeft-hand side is called the *transition dynamics* of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. \n\nA model consists of the two elements below. \n\n#### Transition dynamics $P(s_{t+1}|s_t,a_t)$\n\nThe probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. \n\n#### Reward function $R(s,a)$\n\nUsually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n### How to make a RL agent?\n\nLet the agent state be a function of the history, $s_t^a=g(h_t)$.\n\nAn agent often consists the three elements below.\n\n#### Policy $\\pi(a_t|s_a^t)$\n\nPolicy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability\n\n$P(a_t=a)=\\pi(a|s_t^a)$.\n\n#### Value function $V^\\pi$\n\nIf we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards\n\n$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+...|s_t=s]$.\n\n#### Model\n\nThe agent in RL may have a model. I have introduced how to make a model in section 3.\n\n### Three questions we are facing\n\n#### Do we need exploration or exploitation?\n\nIn RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.\n\n#### Can the agent generalize its experience?\n\nIn actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?\n\n#### Delayed consequences\n\nThe action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?\n\n### What's next?\n\nNow we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what's the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.","slug":"RLSummarize1","published":1,"updated":"2020-02-07T09:39:00.831Z","_id":"ck6ax1lgl000bj1p271y3b93u","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h3><p>This blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. </p>\n<p>These series of blogs of mine are mostly based on the following works and Iâ€™m really grateful to the contributors: </p>\n<ul>\n<li>Online courses of <a href=\"https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u\">Stanford University CS234: Reinforcement Learning, Emma Brunskill</a> and the <a href=\"https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn\">notes</a>.</li>\n<li><a href=\"https://blog.csdn.net/solo95/category_9298323.html\">Blogs of ä»æµåŸŸåˆ°æµ·åŸŸ</a>.</li>\n<li><a href=\"https://zhuanlan.zhihu.com/reinforce\">Blogs of å¶å¼º</a>.</li>\n</ul>\n<p>If you find any mistake in my articles, please feel free to tell me in comments.</p>\n<h3 id=\"What-is-reinforcement-learning-RL\"><a href=\"#What-is-reinforcement-learning-RL\" class=\"headerlink\" title=\"What is reinforcement learning (RL)?\"></a>What is reinforcement learning (RL)?</h3><p>RL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.</p>\n<p>A RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agentâ€™s desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more â€œsmarterâ€ and has a better performance.</p>\n<h3 id=\"Some-basic-notions-of-RL\"><a href=\"#Some-basic-notions-of-RL\" class=\"headerlink\" title=\"Some basic notions of RL\"></a>Some basic notions of RL</h3><p>Because in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce â€œtimeâ€ to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript â€œtâ€ means time it is in a time sequence. </p>\n<ul>\n<li><strong>Agent</strong>: The subject of RL, it is agent that interact with the world.</li>\n<li><strong>Model</strong>: The world, the environment, the <em>agent</em> stays in the <em>model</em>.</li>\n<li><strong>Reward</strong>: $ {r_t} $ , the feedback signal from the <em>model</em>, <em>agent</em> recieves the <em>reward</em>. The <em>reward</em> can have different values according to the different <strong>states</strong> of the <em>agent</em>.</li>\n<li><strong>State</strong>: ${s_t}$ , the <em>state</em> of the <em>agent</em>. The <em>state</em> can be either finite or infinite, and it is set by people.</li>\n<li><strong>Action</strong>: ${a_t}$ , the movement of the <em>agent</em> in the <em>model</em>, <em>actions</em> are different under different <em>states</em>.</li>\n<li><strong>Observation</strong>: ${o_t}$ , the <em>agent</em> need to observe its <em>state</em> and determine the <em>reward</em>.</li>\n<li><strong>History</strong>: a sequence of <em>action</em>, <em>reward</em>, <em>observation</em>, which is: $h_t=(a_1,o_1,r_1,â€¦,a_t,o_t,r_t)$.</li>\n<li><strong>Sequential Decision Making</strong>: make decision base on the <em>history</em>, that is: $a_{t+1}=f(h_t)$.</li>\n</ul>\n<p>Figure 1.1 shows how an agent interact with its world.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/rl1.1.jpeg\" alt=\"Figure 1.1\"></p>\n<h3 id=\"How-to-model-the-world\"><a href=\"#How-to-model-the-world\" class=\"headerlink\" title=\"How to model the world?\"></a>How to model the world?</h3><h4 id=\"Markov-Property\"><a href=\"#Markov-Property\" class=\"headerlink\" title=\"Markov Property\"></a>Markov Property</h4><p>$P(s_{t+1}|s_t,a_t,â€¦,s_1,a_1)=P(s_{t+1}|s_t,a_t)$</p>\n<p>Left-hand side is called the <em>transition dynamics</em> of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. </p>\n<p>A model consists of the two elements below. </p>\n<h4 id=\"Transition-dynamics-P-s-t-1-s-t-a-t\"><a href=\"#Transition-dynamics-P-s-t-1-s-t-a-t\" class=\"headerlink\" title=\"Transition dynamics $P(s_{t+1}|s_t,a_t)$\"></a>Transition dynamics $P(s_{t+1}|s_t,a_t)$</h4><p>The probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. </p>\n<h4 id=\"Reward-function-R-s-a\"><a href=\"#Reward-function-R-s-a\" class=\"headerlink\" title=\"Reward function $R(s,a)$\"></a>Reward function $R(s,a)$</h4><p>Usually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n<h3 id=\"How-to-make-a-RL-agent\"><a href=\"#How-to-make-a-RL-agent\" class=\"headerlink\" title=\"How to make a RL agent?\"></a>How to make a RL agent?</h3><p>Let the agent state be a function of the history, $s_t^a=g(h_t)$.</p>\n<p>An agent often consists the three elements below.</p>\n<h4 id=\"Policy-pi-a-t-s-a-t\"><a href=\"#Policy-pi-a-t-s-a-t\" class=\"headerlink\" title=\"Policy $\\pi(a_t|s_a^t)$\"></a>Policy $\\pi(a_t|s_a^t)$</h4><p>Policy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability</p>\n<p>$P(a_t=a)=\\pi(a|s_t^a)$.</p>\n<h4 id=\"Value-function-V-pi\"><a href=\"#Value-function-V-pi\" class=\"headerlink\" title=\"Value function $V^\\pi$\"></a>Value function $V^\\pi$</h4><p>If we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards</p>\n<p>$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+â€¦|s_t=s]$.</p>\n<h4 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h4><p>The agent in RL may have a model. I have introduced how to make a model in section 3.</p>\n<h3 id=\"Three-questions-we-are-facing\"><a href=\"#Three-questions-we-are-facing\" class=\"headerlink\" title=\"Three questions we are facing\"></a>Three questions we are facing</h3><h4 id=\"Do-we-need-exploration-or-exploitation\"><a href=\"#Do-we-need-exploration-or-exploitation\" class=\"headerlink\" title=\"Do we need exploration or exploitation?\"></a>Do we need exploration or exploitation?</h4><p>In RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.</p>\n<h4 id=\"Can-the-agent-generalize-its-experience\"><a href=\"#Can-the-agent-generalize-its-experience\" class=\"headerlink\" title=\"Can the agent generalize its experience?\"></a>Can the agent generalize its experience?</h4><p>In actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?</p>\n<h4 id=\"Delayed-consequences\"><a href=\"#Delayed-consequences\" class=\"headerlink\" title=\"Delayed consequences\"></a>Delayed consequences</h4><p>The action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?</p>\n<h3 id=\"Whatâ€™s-next\"><a href=\"#Whatâ€™s-next\" class=\"headerlink\" title=\"Whatâ€™s next?\"></a>Whatâ€™s next?</h3><p>Now we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And whatâ€™s the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.</p>\n","site":{"data":{}},"more":"<h3 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h3><p>This blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. </p>\n<p>These series of blogs of mine are mostly based on the following works and Iâ€™m really grateful to the contributors: </p>\n<ul>\n<li>Online courses of <a href=\"https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u\">Stanford University CS234: Reinforcement Learning, Emma Brunskill</a> and the <a href=\"https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn\">notes</a>.</li>\n<li><a href=\"https://blog.csdn.net/solo95/category_9298323.html\">Blogs of ä»æµåŸŸåˆ°æµ·åŸŸ</a>.</li>\n<li><a href=\"https://zhuanlan.zhihu.com/reinforce\">Blogs of å¶å¼º</a>.</li>\n</ul>\n<p>If you find any mistake in my articles, please feel free to tell me in comments.</p>\n<h3 id=\"What-is-reinforcement-learning-RL\"><a href=\"#What-is-reinforcement-learning-RL\" class=\"headerlink\" title=\"What is reinforcement learning (RL)?\"></a>What is reinforcement learning (RL)?</h3><p>RL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.</p>\n<p>A RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agentâ€™s desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more â€œsmarterâ€ and has a better performance.</p>\n<h3 id=\"Some-basic-notions-of-RL\"><a href=\"#Some-basic-notions-of-RL\" class=\"headerlink\" title=\"Some basic notions of RL\"></a>Some basic notions of RL</h3><p>Because in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce â€œtimeâ€ to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript â€œtâ€ means time it is in a time sequence. </p>\n<ul>\n<li><strong>Agent</strong>: The subject of RL, it is agent that interact with the world.</li>\n<li><strong>Model</strong>: The world, the environment, the <em>agent</em> stays in the <em>model</em>.</li>\n<li><strong>Reward</strong>: $ {r_t} $ , the feedback signal from the <em>model</em>, <em>agent</em> recieves the <em>reward</em>. The <em>reward</em> can have different values according to the different <strong>states</strong> of the <em>agent</em>.</li>\n<li><strong>State</strong>: ${s_t}$ , the <em>state</em> of the <em>agent</em>. The <em>state</em> can be either finite or infinite, and it is set by people.</li>\n<li><strong>Action</strong>: ${a_t}$ , the movement of the <em>agent</em> in the <em>model</em>, <em>actions</em> are different under different <em>states</em>.</li>\n<li><strong>Observation</strong>: ${o_t}$ , the <em>agent</em> need to observe its <em>state</em> and determine the <em>reward</em>.</li>\n<li><strong>History</strong>: a sequence of <em>action</em>, <em>reward</em>, <em>observation</em>, which is: $h_t=(a_1,o_1,r_1,â€¦,a_t,o_t,r_t)$.</li>\n<li><strong>Sequential Decision Making</strong>: make decision base on the <em>history</em>, that is: $a_{t+1}=f(h_t)$.</li>\n</ul>\n<p>Figure 1.1 shows how an agent interact with its world.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/rl1.1.jpeg\" alt=\"Figure 1.1\"></p>\n<h3 id=\"How-to-model-the-world\"><a href=\"#How-to-model-the-world\" class=\"headerlink\" title=\"How to model the world?\"></a>How to model the world?</h3><h4 id=\"Markov-Property\"><a href=\"#Markov-Property\" class=\"headerlink\" title=\"Markov Property\"></a>Markov Property</h4><p>$P(s_{t+1}|s_t,a_t,â€¦,s_1,a_1)=P(s_{t+1}|s_t,a_t)$</p>\n<p>Left-hand side is called the <em>transition dynamics</em> of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. </p>\n<p>A model consists of the two elements below. </p>\n<h4 id=\"Transition-dynamics-P-s-t-1-s-t-a-t\"><a href=\"#Transition-dynamics-P-s-t-1-s-t-a-t\" class=\"headerlink\" title=\"Transition dynamics $P(s_{t+1}|s_t,a_t)$\"></a>Transition dynamics $P(s_{t+1}|s_t,a_t)$</h4><p>The probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. </p>\n<h4 id=\"Reward-function-R-s-a\"><a href=\"#Reward-function-R-s-a\" class=\"headerlink\" title=\"Reward function $R(s,a)$\"></a>Reward function $R(s,a)$</h4><p>Usually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n<h3 id=\"How-to-make-a-RL-agent\"><a href=\"#How-to-make-a-RL-agent\" class=\"headerlink\" title=\"How to make a RL agent?\"></a>How to make a RL agent?</h3><p>Let the agent state be a function of the history, $s_t^a=g(h_t)$.</p>\n<p>An agent often consists the three elements below.</p>\n<h4 id=\"Policy-pi-a-t-s-a-t\"><a href=\"#Policy-pi-a-t-s-a-t\" class=\"headerlink\" title=\"Policy $\\pi(a_t|s_a^t)$\"></a>Policy $\\pi(a_t|s_a^t)$</h4><p>Policy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability</p>\n<p>$P(a_t=a)=\\pi(a|s_t^a)$.</p>\n<h4 id=\"Value-function-V-pi\"><a href=\"#Value-function-V-pi\" class=\"headerlink\" title=\"Value function $V^\\pi$\"></a>Value function $V^\\pi$</h4><p>If we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards</p>\n<p>$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+â€¦|s_t=s]$.</p>\n<h4 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h4><p>The agent in RL may have a model. I have introduced how to make a model in section 3.</p>\n<h3 id=\"Three-questions-we-are-facing\"><a href=\"#Three-questions-we-are-facing\" class=\"headerlink\" title=\"Three questions we are facing\"></a>Three questions we are facing</h3><h4 id=\"Do-we-need-exploration-or-exploitation\"><a href=\"#Do-we-need-exploration-or-exploitation\" class=\"headerlink\" title=\"Do we need exploration or exploitation?\"></a>Do we need exploration or exploitation?</h4><p>In RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.</p>\n<h4 id=\"Can-the-agent-generalize-its-experience\"><a href=\"#Can-the-agent-generalize-its-experience\" class=\"headerlink\" title=\"Can the agent generalize its experience?\"></a>Can the agent generalize its experience?</h4><p>In actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?</p>\n<h4 id=\"Delayed-consequences\"><a href=\"#Delayed-consequences\" class=\"headerlink\" title=\"Delayed consequences\"></a>Delayed consequences</h4><p>The action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?</p>\n<h3 id=\"Whatâ€™s-next\"><a href=\"#Whatâ€™s-next\" class=\"headerlink\" title=\"Whatâ€™s next?\"></a>Whatâ€™s next?</h3><p>Now we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And whatâ€™s the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.</p>\n"},{"title":"åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“","date":"2020-01-08T02:29:00.000Z","thumbnail":"https://kinsta.com/wp-content/uploads/2018/03/what-is-nginx.png","excerpt":"æ­å»ºè‡ªå·±çš„æœåŠ¡å™¨å¹¶ä¸éš¾ï¼Œåªæ˜¯è¿‡ç¨‹è¾ƒä¸ºå¤æ‚ã€‚","_content":"\n> ç”±äºè‡ªå·±æ˜¯å»å¹´ä¸ƒæœˆé…ç½®å¥½çš„æœåŠ¡å™¨ï¼Œæœ‰ä¸€äº›ç»†èŠ‚æˆ–è€…é‡åˆ°çš„é—®é¢˜å·²ç»è®°ä¸å¤ªæ¸…ï¼Œæ•…æœ¬æ–‡å¯èƒ½ä¼šæœ‰ä¸å®Œæ•´çš„åœ°æ–¹ï¼Œé‡åˆ°é—®é¢˜è¯·å–„ç”¨æœç´¢å¼•æ“ï¼Œè€Œä¸”æœåŠ¡å™¨çš„é…ç½®æ–¹æ³•ä¹Ÿä¸åªæœ‰è¿™ä¸€ç§ã€‚æœ¬æ–‡ä¸»è¦ç”¨ä½œå¯¹è‡ªå·±æ“ä½œæ­¥éª¤å’Œæ–¹æ³•çš„ä¸€ä¸ªæ€»ç»“ï¼Œä»¥ä¾¿äºæ—¥åæŸ¥é˜…ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚\n\n### è´­ä¹°æœåŠ¡å™¨\n\né¦–å…ˆå»[åä¸ºäº‘å®˜ç½‘](https://www.huaweicloud.com/?locale=zh-cn)æ³¨å†Œä¸€ä¸ªè´¦å·ã€‚å¦‚æœæ˜¯å­¦ç”Ÿï¼Œå¯ä»¥æœç´¢â€œå­¦ç”Ÿâ€ï¼Œå¹¶è¿›è¡Œå­¦ç”Ÿè®¤è¯ã€‚å­¦ç”Ÿè®¤è¯çš„æ­¥éª¤å‚è§[å­¦ç”Ÿè®¤è¯æµç¨‹](https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html)ã€‚è¿›è¡Œèº«ä»½éªŒè¯åå¯ä»¥è´­ä¹°å­¦ç”Ÿä¼˜æƒ å¥—é¤ï¼Œäº‘æœåŠ¡å™¨ä»·æ ¼åªè¦99å…ƒ/å¹´ï¼Œæ¯”é˜¿é‡Œäº‘å’Œè…¾è®¯äº‘çš„éƒ½è¦ä¾¿å®œä¸€äº›ã€‚\n\n![åä¸ºäº‘å­¦ç”Ÿä¼˜æƒ ](https://astrobear.top/resource/astroblog/content/hwcloud_discount.png)\n\nè´­ä¹°å®Œæˆåï¼Œä½ å¯ä»¥åœ¨æ§åˆ¶å°çœ‹åˆ°è‡ªå·±ç°æœ‰çš„èµ„æºä»¥åŠè¿è¡Œæƒ…å†µã€‚\n\n![æ§åˆ¶å°](https://astrobear.top/resource/astroblog/content/console.png)\n\n### é…ç½®å®‰å…¨ç»„\n\n> å®‰å…¨ç»„æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„åˆ†ç»„ï¼Œä¸ºå…·æœ‰ç›¸åŒå®‰å…¨ä¿æŠ¤éœ€æ±‚å¹¶ç›¸äº’ä¿¡ä»»çš„äº‘æœåŠ¡å™¨æä¾›è®¿é—®ç­–ç•¥ã€‚å®‰å…¨ç»„åˆ›å»ºåï¼Œç”¨æˆ·å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­å®šä¹‰å„ç§è®¿é—®è§„åˆ™ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚\n>\n> ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªç”¨æˆ·é»˜è®¤åˆ›å»ºä¸€ä¸ªé»˜è®¤å®‰å…¨ç»„ï¼Œé»˜è®¤å®‰å…¨ç»„çš„è§„åˆ™æ˜¯åœ¨å‡ºæ–¹å‘ä¸Šçš„æ•°æ®æŠ¥æ–‡å…¨éƒ¨æ”¾è¡Œï¼Œå…¥æ–¹å‘è®¿é—®å—é™ï¼Œå®‰å…¨ç»„å†…çš„äº‘æœåŠ¡å™¨æ— éœ€æ·»åŠ è§„åˆ™å³å¯äº’ç›¸è®¿é—®ã€‚é»˜è®¤å®‰å…¨ç»„å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚\n>\n> å®‰å…¨ç»„åˆ›å»ºåï¼Œä½ å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­è®¾ç½®å‡ºæ–¹å‘ã€å…¥æ–¹å‘è§„åˆ™ï¼Œè¿™äº›è§„åˆ™ä¼šå¯¹å®‰å…¨ç»„å†…éƒ¨çš„äº‘æœåŠ¡å™¨å‡ºå…¥æ–¹å‘ç½‘ç»œæµé‡è¿›è¡Œè®¿é—®æ§åˆ¶ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚[^1]\n\nåœ¨æ§åˆ¶å°ç‚¹å‡»â€œå¼¹æ€§äº‘æœåŠ¡å™¨ECSâ€ï¼Œåœ¨è¿™é‡Œä½ å¯çœ‹åˆ°ä½ çš„æœåŠ¡å™¨çš„å…¬ç½‘IPï¼Œè¯·è®°ä¸‹è¿™ä¸ªIPåœ°å€ã€‚ç„¶åç‚¹å‡»åœ¨åˆ—è¡¨ä¸­ç‚¹å‡»ä½ çš„æœåŠ¡å™¨çš„åç§°ã€‚\n\n![é€‰æ‹©æœåŠ¡å™¨](https://astrobear.top/resource/astroblog/content/security_groups.png)\n\nè¿›å…¥äº‘æœåŠ¡å™¨ç®¡ç†é¡µé¢åï¼Œç‚¹å‡»â€œå®‰å…¨ç»„â€ã€‚å†ç‚¹å‡»â€œSys-defaultâ€å¯ä»¥çœ‹åˆ°é»˜è®¤å®‰å…¨ç»„ã€‚ç„¶åä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯æˆ‘ç›®å‰çš„å®‰å…¨ç»„è®¾ç½®ï¼Œä»…ä¾›å‚è€ƒã€‚é€‰æ‹©â€œå…¥/å‡ºæ–¹å‘æ–¹å‘è§„åˆ™â€ï¼Œå†ç‚¹å‡»â€œæ·»åŠ è§„åˆ™â€œå³å¯æ‰‹åŠ¨æ·»åŠ è§„åˆ™ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œé…ç½®çš„éƒ½æ˜¯å…¥æ–¹å‘çš„å®‰å…¨ç»„ï¼Œå¹¶ä¸”æºåœ°å€ï¼ˆè®¿é—®æœåŠ¡å™¨çš„è®¾å¤‡çš„IPåœ°å€ï¼‰éƒ½ä¸ºâ€œ0.0.0.0/0â€ï¼ˆæ‰€æœ‰IPåœ°å€ï¼‰ã€‚\n\né€šå¸¸éœ€è¦é…ç½®å¦‚ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š\n\n- SSHè¿œç¨‹è¿æ¥Linuxå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šSSHï¼Œç«¯å£ï¼š22ï¼‰\n- å…¬ç½‘â€œpingâ€ECSå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šICMPï¼Œç«¯å£ï¼šå…¨éƒ¨ï¼‰\n- å¼¹æ€§äº‘æœåŠ¡å™¨ä½œWebæœåŠ¡å™¨\n  - åè®®ï¼šhttpï¼Œç«¯å£ï¼š80\n  - åè®®ï¼šhttpsï¼Œç«¯å£ï¼š433\n\nè¯¦ç»†é…ç½®è¯·å‚è€ƒ[å®‰å…¨ç»„é…ç½®ç¤ºä¾‹](https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html)ã€‚\n\n![å®‰å…¨ç»„è®¾ç½®](https://astrobear.top/resource/astroblog/content/sg_settings.png)\n\n![å®‰å…¨ç»„è®¾ç½®](https://astrobear.top/resource/astroblog/content/sg_settings1.png)\n\né…ç½®å®Œæˆåï¼Œå¯ä»¥æ‰“å¼€ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œç”¨ä¸‹é¢çš„è¯­å¥æµ‹è¯•ä¸€ä¸‹ï¼š\n\n`ping ä½ çš„å…¬ç½‘IP`\n\nå‡ºç°ç±»ä¼¼ä¸‹é¢çš„å†…å®¹å°±ä»£è¡¨æˆåŠŸäº†ï¼š\n\n![pingæµ‹è¯•](https://astrobear.top/resource/astroblog/content/ping_test.png)\n\nä½ å¯ä»¥æŒ‰ä¸‹`Ctrl+C`æ¥ç»“æŸ`ping`è¿™ä¸ªè¿›ç¨‹ã€‚\n\nç„¶ååœ¨ç»ˆç«¯é‡Œè¾“å…¥ï¼š\n\n`ssh ä½ çš„å…¬ç½‘IP`\n\nå¦‚æœä½ çš„å®‰å…¨ç»„é…ç½®æ­£ç¡®çš„è¯ï¼Œä¼šè®©ä½ è¾“å…¥æœåŠ¡å™¨çš„ç™»å½•å¯†ç ã€‚è¾“å…¥å¯†ç ï¼ˆæ³¨æ„ï¼šå¯†ç æ˜¯ä¸ä¼šæ˜¾ç¤ºçš„ï¼‰åå›è½¦ï¼Œåº”è¯¥å¯ä»¥çœ‹åˆ°è¿™æ ·çš„è¾“å‡ºï¼š\n\n![sshç™»å½•](https://astrobear.top/resource/astroblog/content/ssh_login.png)\n\nè¿™ä¸ªæ—¶å€™ï¼Œä½ çš„ç»ˆç«¯å°±å·²ç»è¿æ¥ä¸Šäº†æœåŠ¡å™¨çš„ç³»ç»Ÿäº†ï¼Œä½ åœ¨ç»ˆç«¯é‡Œçš„ä¸€åˆ‡æ“ä½œéƒ½æ˜¯ä½œç”¨åœ¨æœåŠ¡å™¨ä¸Šçš„ã€‚\n\n### åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\n\né¦–å…ˆè¯·åœ¨ç»ˆç«¯ä½¿ç”¨sshç™»å½•ä½ çš„æœåŠ¡å™¨ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢ç»™å‡ºçš„é¡ºåºè¾“å…¥å‘½ä»¤ã€‚\n\n```shell\nyum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #å®‰è£…ç¼–è¯‘å·¥å…·åŠåº“æ–‡ä»¶\ncd /usr/local/ #åˆ‡æ¢åˆ°ç›®æ ‡å®‰è£…æ–‡ä»¶å¤¹\nwget http://nginx.org/download/nginx-1.16.1.tar.gz #ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„Nginx\ntar -zxvf nginx-1.16.1.tar.gz #è§£å‹æ–‡ä»¶\ncd nginx-1.16.1 #è¿›å…¥è§£å‹çš„æ–‡ä»¶å¤¹\n./configure #æ‰§è¡Œç¨‹åº\nmake #ç¼–è¯‘\nmake install #å®‰è£…\ncd /usr/local/nginx/sbin #è¿›å…¥Nginxå®‰è£…ç›®å½•\n./nginx #è¿è¡ŒNginx\n```\n\næ­¤æ—¶ï¼Œå®‰è£…åº”è¯¥å·²ç»å®Œæˆäº†ã€‚æ‰“å¼€æµè§ˆå™¨ï¼Œåœ¨åœ°å€æ ä¸­è¾“å…¥ä½ çš„å…¬ç½‘ipã€‚å¦‚æœçœ‹åˆ°ä¸‹å›¾æ‰€ç¤ºå†…å®¹ï¼Œå°±ä»£è¡¨å®‰è£…æˆåŠŸäº†ã€‚\n\n![nginxå®‰è£…æˆåŠŸ](https://astrobear.top/resource/astroblog/content/nginx_install.png)\n\n### åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\n\nåœ¨æ‹¥æœ‰äº†è‡ªå·±çš„æœåŠ¡å™¨ä»¥åï¼Œå°±å¯ä»¥åšå¾ˆå¤šäº‹æƒ…äº†ã€‚ä½†æ˜¯ç°åœ¨ä½ åªèƒ½é€šè¿‡IPåœ°å€è®¿é—®è‡ªå·±çš„æœåŠ¡å™¨ï¼Œçœ‹èµ·æ¥æ€»æ˜¯æœ‰ç‚¹åˆ«æ‰­ã€‚å¦å¤–ï¼Œå¦‚æœä½ æƒ³è¦ç½‘ç«™æœ‰ä¸€å®šçš„å½±å“åŠ›çš„è¯ï¼Œä»…æœ‰IPåœ°å€ä¼šè®©äººå‡ ä¹æ‰¾ä¸åˆ°ä½ çš„ç½‘ç«™ï¼Œè€Œä¸”ä¹Ÿä¸ç¬¦åˆå›½å®¶æ³•å¾‹è§„å®šã€‚æ‰€ä»¥è¿˜æ˜¯å»ºè®®å¤§å®¶å¼„ä¸€ä¸ªè‡ªå·±çš„åŸŸåã€‚\n\nç°åœ¨å¸‚é¢ä¸Šçš„äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›åŸŸåæ³¨å†Œçš„æœåŠ¡ï¼Œç›´æ¥åœ¨ä½ çš„æœåŠ¡æä¾›å•†çš„å¹³å°ä¸Šé¢æ³¨å†Œå³å¯ã€‚ä¸‹é¢æˆ‘ç»§ç»­ç”¨åä¸ºäº‘çš„å¹³å°æ¼”ç¤ºã€‚\n\né¦–å…ˆåœ¨åä¸ºäº‘ç½‘ç«™é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œåŸŸåâ€ï¼Œæ‰“å¼€ç¬¬ä¸€ä¸ªé“¾æ¥â€œåŸŸåæ³¨å†ŒæœåŠ¡â€ã€‚ä¹Ÿå¯ä»¥ç›´æ¥ç‚¹å‡»è¿™é‡Œï¼š[åŸŸåæ³¨å†ŒæœåŠ¡_åä¸ºäº‘](https://www.huaweicloud.com/product/domain.html)ã€‚\n\nç„¶åä½ å¯ä»¥åœ¨ç½‘é¡µä¸­é€‰æ‹©ä½ çš„åŸŸåï¼Œå¸¸è§çš„å¦‚`.com`ï¼Œ`.cn`ï¼Œ`.net`ç­‰ã€‚è¿™äº›åŸŸåä¼šç›¸å¯¹æ¯”è¾ƒè´µã€‚ä½œä¸ºå­¦ç”Ÿå…šï¼Œæˆ‘é€‰æ‹©ä¸€ä¸ªæœ€ä¾¿å®œçš„åŸŸå`.top`ï¼Œåªéœ€è¦9å…ƒ/å¹´ã€‚\n\nç‚¹å‡»ä½ æƒ³è¦çš„åŸŸååï¼Œä¼šè·³è½¬åˆ°ä¸€ä¸ªæ–°çš„é¡µé¢ã€‚æ¥ä¸‹æ¥å†æ¬¡é€‰æ‹©ä½ è¦çš„åŸŸåï¼Œå¹¶ä¸”åœ¨â€œæŸ¥åŸŸåâ€çš„æœç´¢æ¡†å†…è¾“å…¥ä½ æƒ³è¦çš„åŸŸåï¼Œçœ‹çœ‹æ˜¯å¦å·²ç»è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨äº†å°±æ¢ä¸€ä¸ªã€‚è‹¥æ˜¾ç¤ºâ€œåŸŸåå¯æ³¨å†Œâ€ï¼Œå°±ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚\n\n![åŸŸåè´­ä¹°](https://astrobear.top/resource/astroblog/content/buy_domain.png)\n\nè´­ä¹°å®Œæˆåï¼Œä½ å°±æ‹¥æœ‰äº†è‡ªå·±åŸŸåäº†ï¼\n\n### å¤‡æ¡ˆ\n\n> å¤‡æ¡ˆæ˜¯ä¸­å›½å¤§é™†çš„ä¸€é¡¹æ³•è§„ï¼Œä½¿ç”¨å¤§é™†èŠ‚ç‚¹æœåŠ¡å™¨æä¾›äº’è”ç½‘ä¿¡æ¯æœåŠ¡çš„ç”¨æˆ·ï¼Œéœ€è¦åœ¨æœåŠ¡å™¨æä¾›å•†å¤„æäº¤å¤‡æ¡ˆç”³è¯·ã€‚\n>\n> æ ¹æ®å·¥ä¿¡éƒ¨ã€Šäº’è”ç½‘ä¿¡æ¯æœåŠ¡ç®¡ç†åŠæ³•ã€‹(å›½åŠ¡é™¢292å·ä»¤)å’Œå·¥ä¿¡éƒ¨ä»¤ç¬¬33å·ã€Šéç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å¤‡æ¡ˆç®¡ç†åŠæ³•ã€‹è§„å®šï¼Œå›½å®¶å¯¹ç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œè®¸å¯åˆ¶åº¦ï¼Œå¯¹éç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œå¤‡æ¡ˆåˆ¶åº¦ã€‚æœªå–å¾—è®¸å¯æˆ–è€…æœªå±¥è¡Œå¤‡æ¡ˆæ‰‹ç»­çš„ï¼Œä¸å¾—ä»äº‹äº’è”ç½‘ä¿¡æ¯æœåŠ¡ï¼Œå¦åˆ™å±è¿æ³•è¡Œä¸ºã€‚é€šä¿—æ¥è®²ï¼Œè¦å¼€åŠç½‘ç«™å¿…é¡»å…ˆåŠç†ç½‘ç«™å¤‡æ¡ˆï¼Œå¤‡æ¡ˆæˆåŠŸå¹¶è·å–é€šä¿¡ç®¡ç†å±€ä¸‹å‘çš„ICPå¤‡æ¡ˆå·åæ‰èƒ½å¼€é€šè®¿é—®ã€‚[^2]\n\nè¿™ä¸€æ­¥ä¸å¤šè¯´äº†ï¼Œå…·ä½“æ­¥éª¤æ¯”è¾ƒç¹çï¼ŒèŠ±è´¹çš„æ—¶é—´ä¹Ÿæ¯”è¾ƒé•¿ï¼Œéœ€è¦ä¸€ä¸¤å‘¨ã€‚ç½‘ç«™ä¸Šæœ‰å¾ˆæ¸…æ™°çš„[æ“ä½œæ–¹æ³•](https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html)ï¼Œè¯·è‡ªè¡ŒæŸ¥é˜…ï¼Œæ ¹æ®æ­¥éª¤æ“ä½œå³å¯ã€‚éœ€è¦æ³¨æ„ä¸€ç‚¹çš„æ˜¯ï¼Œåœ¨å®¡æ ¸è¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ¥åˆ°æœåŠ¡æä¾›å•†æ‰“æ¥çš„ç”µè¯ï¼Œä¸è¦æ¼æ¥ã€‚\n\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢çš„å¤‡æ¡ˆæ“ä½œæ˜¯åœ¨å·¥ä¿¡éƒ¨å¤‡æ¡ˆçš„ã€‚å®Œæˆäº†åœ¨å·¥ä¿¡éƒ¨çš„å¤‡æ¡ˆä»¥åè¿˜éœ€è¦å…¬å®‰å¤‡æ¡ˆã€‚å…·ä½“[æ“ä½œæ–¹æ³•](http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c)ä¹Ÿè¯·è‡ªè¡ŒæŸ¥é˜…ã€‚\n\n### åŸŸåè§£æ\n\nåœ¨å®Œæˆä¸€ç³»åˆ—ç¹ççš„å¤‡æ¡ˆæµç¨‹ä»¥åï¼Œä½ çš„ç½‘ç«™è¿˜ä¸å¯ä»¥é€šè¿‡åŸŸåè®¿é—®ã€‚åªæœ‰æŠŠä½ çš„åŸŸåè·ŸæœåŠ¡å™¨çš„IPåœ°å€ç»‘å®šåœ¨ä¸€èµ·ä¹‹åï¼Œå¹¶ä¸”åœ¨æœåŠ¡å™¨ä¸Šä¿®æ”¹äº†é…ç½®æ–‡ä»¶ä¹‹åæ‰å¯ä»¥ã€‚\n\né¦–å…ˆæ‰“å¼€ç®¡ç†æ§åˆ¶å°ï¼Œåœ¨æ§åˆ¶å°ä¸­é€‰æ‹©â€œåŸŸåæ³¨å†Œâ€ã€‚ç„¶ååœ¨ä¸‹é¢çš„é¡µé¢ä¸­ç‚¹å‡»â€œè§£æâ€ã€‚\n\n![åŸŸåæ³¨å†Œ](https://astrobear.top/resource/astroblog/content/domain.png)\n\nç‚¹å‡»ä½ çš„åŸŸåï¼Œæ˜¾ç¤ºå¦‚ä¸‹é¡µé¢ã€‚è¿™é‡Œæ˜¾ç¤ºçš„æ˜¯ä½ åŸŸåçš„è®°å½•é›†ï¼Œå‰ä¸¤ä¸ªè®°å½•é›†åº”è¯¥æ˜¯é¢„ç½®è®¾ç½®ï¼Œä¸å¯æš‚åœæœåŠ¡ã€‚<span id=\"1\">ä½ å¯ä»¥åœ¨è¿™åŸºç¡€ä¸Šæ·»åŠ è‡ªå·±çš„è®°å½•é›†ã€‚</span>\n\n![è®°å½•é›†](https://astrobear.top/resource/astroblog/content/record.png)\n\nç‚¹å‡»é¡µé¢å³ä¸Šè§’çº¢è‰²æŒ‰é’®ä»¥æ·»åŠ è®°å½•é›†ã€‚æ·»åŠ è®°å½•é›†çš„é…ç½®å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä¸‹å›¾ä¸­ç»™å‡ºçš„ä¾‹å­æ˜¯æ·»åŠ çš„â€œAâ€å‹è®°å½•é›†ï¼Œä¹Ÿå³é€šè¿‡`example.com`è®¿é—®ç½‘ç«™ã€‚è‹¥éœ€è¦é€šè¿‡`www.example.com`è®¿é—®ç½‘ç«™ï¼Œåˆ™éœ€è¦ä¸º`example.com`çš„å­åŸŸåæ·»åŠ â€œAâ€å‹è®°å½•é›†ã€‚å…·ä½“é…ç½®å‚è§ï¼š[é…ç½®ç½‘ç«™è§£æ_åä¸ºäº‘](https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1)ã€‚ç‚¹å‡»â€œç¡®å®šâ€ï¼Œå®Œæˆæ·»åŠ ã€‚ä½ å¯ä»¥é€šè¿‡`ping ä½ çš„åŸŸå`æ¥æµ‹è¯•ä½ æ·»åŠ çš„è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆäº†ã€‚\n\n![æ·»åŠ è®°å½•é›†](https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png)\n\n### é…ç½®nginx\n\n<span id=\"2\">æ‰“å¼€</span>ä½ ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œè¾“å…¥å‘½ä»¤ï¼š`ssh ä½ çš„IPåœ°å€`ï¼Œè¾“å…¥ä½ çš„æœåŠ¡å™¨çš„å¯†ç ã€‚\n\nè¿›å…¥ä½ çš„nginxçš„å®‰è£…ç›®å½•ï¼š`cd /usr/local/nginx/`ã€‚\n\nä½¿ç”¨vimæ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ï¼š`vim ./conf/nginx.conf`ã€‚\n\næŒ‰`I`å¼€å§‹è¾“å…¥ã€‚\n\nåœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š\n\n```nginx\nserver {\n\t    listen   80; #ç›‘å¬ç«¯å£è®¾ä¸º 80\n\t    server_name  example.com; #ç»‘å®šæ‚¨çš„åŸŸå\n\t    index index.htm index.html; #æŒ‡å®šé»˜è®¤æ–‡ä»¶\n\t    root html; #æŒ‡å®šç½‘ç«™æ ¹ç›®å½•\n}\n```\n\nç„¶åæŒ‰`esc`é€€å‡ºç¼–è¾‘ï¼Œå†æŒ‰`Shift+zz`ä¿å­˜ã€‚\n\nè¾“å…¥ï¼š`cd ./sbin`ï¼Œåˆ‡æ¢æ–‡ä»¶å¤¹ã€‚\n\næ‰§è¡Œå‘½ä»¤ï¼š`nginx -s relod`ï¼Œé‡å¯nginxæœåŠ¡ã€‚\n\nè¿™æ—¶å€™å†å°è¯•ç”¨æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œåº”è¯¥ä¼šæ˜¾ç¤ºä¹‹å‰å‡ºç°è¿‡çš„â€œWelcome to nginx â€çš„é¡µé¢äº†ï¼\n\n### ç”³è¯·SSLè¯ä¹¦\n\nSSLè¯ä¹¦å¯ä»¥åœ¨æ•°æ®ä¼ è¾“çš„è¿‡ç¨‹ä¸­å¯¹å…¶è¿›è¡ŒåŠ å¯†å’Œéšè—ï¼Œå¯ä»¥æå¤§åœ°æé«˜æ•°æ®ä¼ è¾“çš„å®‰å…¨æ€§ã€‚æ‹¥æœ‰SSLè¯ä¹¦çš„ç½‘ç«™çš„è¯·æ±‚å¤´éƒ½æ˜¯`https`ï¼Œå¹¶ä¸”åœ¨é“¾æ¥æ—è¾¹ä¼šå‡ºç°ä¸€æŠŠå°é”ã€‚ä½†æ˜¯ï¼ŒSSLè¯ä¹¦å¹¶ä¸æ˜¯æ‰€æœ‰ç½‘ç«™éƒ½å¿…é¡»çš„ï¼Œè¿™è§†ä½ çš„éœ€è¦è€Œå®šã€‚æ¯”å¦‚ï¼Œå¾®ä¿¡å°ç¨‹åºçš„æœåŠ¡å™¨å°±å¿…é¡»è¦æœ‰åŸŸåå’ŒSSLè¯ä¹¦ã€‚å¦å¤–ï¼Œå‡ºäºä¿¡æ¯ä¼ è¾“çš„å®‰å…¨æ€§æ–¹é¢çš„è€ƒè™‘ï¼Œæœ‰SSLè¯ä¹¦è¿˜æ˜¯æ˜¾å¾—æ›´ä¸ºå¦¥å½“å’Œä¸“ä¸šä¸€ç‚¹ã€‚\n\nç°åœ¨å¸‚é¢ä¸Šå„å¤§äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›é…å¥—çš„SSLè¯ä¹¦ç”³è¯·æœåŠ¡ï¼Œä¸€èˆ¬éƒ½æ˜¯æä¾›ä¼ä¸šçº§çš„è¯ä¹¦ï¼Œä»·æ ¼æ¯”è¾ƒæ˜‚è´µã€‚ä½†æ˜¯åŒæ—¶ç½‘ç»œä¸Šä¹Ÿæœ‰ä¸€äº›å…è´¹çš„SSLè¯ä¹¦æœåŠ¡å¯ä»¥é€‰æ‹©ã€‚ä¸‹é¢è¿˜æ˜¯ä»¥åä¸ºäº‘çš„å¹³å°ä¸ºä¾‹ï¼Œç®€å•è¯´æ˜ä¸€ä¸‹å¦‚ä½•ç”³è¯·SSLè¯ä¹¦ã€‚\n\né¦–å…ˆåœ¨åä¸ºäº‘é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œå…è´¹è¯ä¹¦â€œï¼Œç„¶åç‚¹å‡»[äºšæ´²è¯šä¿¡åŸŸåå‹DVå•åŸŸåSSLè¯ä¹¦--å…è´¹è¯ä¹¦](https://marketplace.huaweicloud.com/product/00301-315148-0--0)ï¼Œå¯ä»¥çœ‹åˆ°è¯ä¹¦çš„ä»·æ ¼æ˜¯0.00å…ƒã€‚ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚\n\n![è´­ä¹°SSLè¯ä¹¦](https://astrobear.top/resource/astroblog/content/buy_ssl.png)\n\nå®Œæˆè´­ä¹°åè¯·ä¸è¦ç«‹å³å…³é—­é¡µé¢ï¼Œé¡µé¢ä¸­çš„è®¢å•å·åœ¨ä¹‹åè¿˜éœ€è¦ç”¨åˆ°ã€‚å°”åï¼Œç³»ç»Ÿä¼šå‘é€â€HuaweiCloudè´¦æˆ·ç”³è¯·â€é‚®ä»¶è‡³ç”¨æˆ·é‚®ç®±ï¼Œå³ä½ åœ¨åä¸ºäº‘çš„æ³¨å†Œé‚®ç®±ã€‚\n\n![HuaweiCloudè´¦æˆ·ç”³è¯·](https://astrobear.top/resource/astroblog/content/request_account.png)\n\nç‚¹å‡»é‚®ä»¶ä¸­çš„ç™»å½•åœ°å€è¿›å…¥ç³»ç»Ÿï¼Œå¹¶ä½¿ç”¨é‚®ä»¶æä¾›çš„è´¦å·å’Œåˆå§‹å¯†ç è¿›è¡Œç™»å½•ã€‚ç™»å…¥ç³»ç»Ÿåè¯·ä¿®æ”¹ä½ çš„åˆå§‹å¯†ç ï¼Œç„¶åè¯·æ ¹æ®åä¸ºäº‘ä¸­ç»™ä½ æä¾›çš„è®¢å•å·åœ¨è¯¥ç³»ç»Ÿä¸­æŸ¥è¯¢ä½ çš„è®¢å•ã€‚æŸ¥è¯¢åˆ°ä½ çš„è®¢å•ä»¥åï¼Œéœ€è¦ä½ è¡¥å……ä¸€äº›ä¿¡æ¯ï¼Œè¯·å¦‚å®å¡«å†™ã€‚ç³»ç»Ÿä¼šè¦ä½ å¡«å†™å…¬å¸ä¿¡æ¯ï¼Œå¦‚æœåªæ˜¯ä¸ªäººç½‘ç«™ï¼Œé‚£ä¹ˆå…¬å¸åç§°ç›´æ¥å¡«å†™ä½ çš„åå­—å³å¯ï¼Œå…¬å¸åœ°å€å°±å¡«å†™ä½ çš„ä½å€ã€‚\n\nå¡«å†™å®Œæˆåä¼šè¿›å…¥å®¡æ ¸é˜¶æ®µï¼Œç³»ç»Ÿä¼šç»™ä½ å‘é€ä¸€å°é‚®ä»¶ã€‚\n\n![è¯ä¹¦å®¡æ ¸](https://astrobear.top/resource/astroblog/content/check.png)\n\næ ¹æ®é‚®ä»¶çš„æç¤ºï¼Œéœ€è¦åœ¨è®°å½•é›†ä¸­æ·»åŠ æ–°çš„å†…å®¹ã€‚è¯·æ ¹æ®[å‰æ–‡](#1)æ‰€è¿°æ–¹æ³•ï¼Œå°†é‚®ä»¶ä¸­çš„å†…å®¹æ·»åŠ è‡³æ–°çš„è®°å½•é›†ã€‚å¡«å†™æ–¹æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚\n\n![å¡«å†™è®°å½•é›†](https://astrobear.top/resource/astroblog/content/modify_record.png)\n\nå¡«å†™å®Œæˆåï¼Œå¯ä»¥åœ¨æœ¬åœ°ç”µè„‘çš„ç»ˆç«¯é‡Œè¾“å…¥`nslookup -querytype=txt ä½ çš„åŸŸå`æ¥æµ‹è¯•è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆã€‚\n\n![æµ‹è¯•è®°å½•é›†](https://astrobear.top/resource/astroblog/content/test_record.png)\n\nä¸€èˆ¬æ¥è¯´ï¼Œè®°å½•é›†ç”Ÿæ•ˆå10åˆ†é’Ÿä»¥å†…è¯ä¹¦å°±ä¼šé¢å‘äº†ã€‚\n\n![è¯ä¹¦é¢å‘](https://astrobear.top/resource/astroblog/content/issue.png)\n\n### SSLè¯ä¹¦éƒ¨ç½²\n\næ¥ä¸‹æ¥æˆ‘ä»¬è¦æŠŠSSLè¯ä¹¦éƒ¨ç½²åˆ°æˆ‘ä»¬çš„æœåŠ¡å™¨ä¸Šã€‚\n\nåœ¨æ”¶åˆ°çš„â€œè¯ä¹¦é¢å‘â€çš„é‚®ä»¶çš„åº•éƒ¨æœ‰ä¸€æ¡é“¾æ¥ï¼Œç‚¹å‡»è¿™æ¡é“¾æ¥ï¼Œè¿›å…¥è¯ä¹¦ç®¡ç†ç³»ç»Ÿã€‚ç™»å½•ç³»ç»Ÿï¼Œåœ¨å·¦ä¾§å¯¼èˆªæ ä¸­ç‚¹å‡»â€œSSLè¯ä¹¦â€ï¼Œå†ç‚¹å‡»â€œé¢„è§ˆâ€ï¼Œå†åœ¨å³ä¾§çš„â€œä¿¡æ¯é¢„è§ˆâ€ä¸­ç‚¹å‡»â€œä¸‹è½½æœ€æ–°è¯ä¹¦â€œã€‚\n\n![ä¸‹è½½è¯ä¹¦](https://astrobear.top/resource/astroblog/content/download_cert.png)\n\nåœ¨å¼¹å‡ºçš„å¯¹è¯æ¡†å†…ï¼Œé€‰æ‹©è¯ä¹¦æ ¼å¼ä¸ºâ€œPEM(é€‚ç”¨äºNginx,SLB)â€ï¼Œè¾“å…¥ä½ çš„è®¢å•å¯†ç ã€‚è¯ä¹¦å¯†ç å¯ä»¥ç•™ç©ºã€‚\n\n![ä¸‹è½½è¯ä¹¦](https://astrobear.top/resource/astroblog/content/download_cert1.png)\n\nä¸‹è½½å®Œæˆåï¼Œè§£å‹ä¸‹è½½çš„å‹ç¼©åŒ…ï¼Œéœ€è¦è¾“å…¥ä½ çš„è®¢å•å¯†ç ï¼ˆå¦‚æœä½ æ²¡æœ‰è®¾ç½®è¯ä¹¦å¯†ç ï¼‰ã€‚è§£å‹ä»¥åå¯ä»¥å¾—åˆ°ä¸‹å›¾ä¸¤ä¸ªæ–‡ä»¶ã€‚\n\n![è§£å‹ç¼©](https://astrobear.top/resource/astroblog/content/unzip_cert.png)\n\næ¥ä¸‹æ¥ï¼Œæ‰“å¼€ä½ çš„ç»ˆç«¯ï¼ŒæŒ‰é¡ºåºè¾“å…¥ä¸‹åˆ—å‘½ä»¤ï¼š\n\n```shell\nssh ä½ çš„å…¬ç½‘IP #sshç™»å½•ï¼Œè¾“å…¥ä½ çš„å¯†ç \ncd /usr/local/nginx #åˆ‡æ¢åˆ°nginxçš„å®‰è£…ç›®å½•\nmkdir ./cert #åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹certç”¨äºå­˜æ”¾ä½ çš„è¯ä¹¦\nexit #æ–­å¼€ä¸æœåŠ¡å™¨çš„è¿æ¥\nscp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.key ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.keyæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹\nscp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.crt ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.crtæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹\n```\n\næ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä¿®æ”¹nginxçš„é…ç½®æ–‡ä»¶ã€‚å‚è€ƒ[å‰æ–‡](#2)æ‰€è¿°æ–¹æ³•æ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ã€‚å…ˆå°†ä½ ä¹‹å‰æ’å…¥çš„å†…å®¹åˆ é™¤æˆ–è€…ä½¿ç”¨`#`æ³¨é‡Šæ‰ï¼Œç„¶ååœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š\n\n```nginx\nserver {\n         listen       443 ssl;\n         server_name  example.com; #ä½ è¯ä¹¦ç»‘å®šçš„åŸŸå;\n\n        ssl_certificate      /usr/local/nginx/cert/ä½ çš„åŸŸå.crt;\n        ssl_certificate_key  /usr/local/nginx/cert/ä½ çš„åŸŸå.key;\n\n        ssl_session_cache    shared:SSL:1m;\n        ssl_session_timeout  5m;\n\n        ssl_ciphers  HIGH:!aNULL:!MD5;\n        ssl_prefer_server_ciphers  on;\n        \n        location / {\n            index index.htm index.html; #æŒ‡å®šé»˜è®¤æ–‡ä»¶ã€‚\n\t    \t\t\troot html; #æŒ‡å®šç½‘ç«™æ ¹ç›®å½•ã€‚\n        }\n}\nserver { #å°†ä½ çš„80ç«¯å£é‡å®šå‘è‡³433ç«¯å£ï¼Œå³å¼ºåˆ¶ä½¿ç”¨httpsè®¿é—®\n  \t\t\tlisten 80;\n  \t\t\tserver_name; example.com; #ä½ çš„åŸŸå\n\t\t\t\trewrite ^/(.*)$ https://example.com:443/$1 permanent;\n}\n```\n\nå°†æ–‡ä»¶ä¿å­˜ä»¥åé‡å¯nginxæœåŠ¡ã€‚\n\né‡å¯ä»¥åä½ å¯èƒ½ä¼šé‡åˆ°è¿™æ ·çš„é—®é¢˜ï¼š`**unknown directive â€œsslâ€ in /usr/local/nginx/conf/nginx.conf:121**`ï¼Œè¿™æ˜¯å› ä¸ºä½ åœ¨å®‰è£…nginxæ—¶ï¼Œæ²¡æœ‰ç¼–è¯‘SSLæ¨¡å—ã€‚ä½ å¯ä»¥åœ¨ç»ˆç«¯é‡ŒæŒ‰ç…§ä¸‹è¿°æ­¥éª¤è§£å†³[^ 3]ï¼š\n\n```shell\ncd ../nginx-1.16.1 #è¿›å…¥åˆ°nginxçš„æºç åŒ…çš„ç›®å½•ä¸‹\n./configure --with-http_ssl_module #å¸¦å‚æ•°æ‰§è¡Œç¨‹åº\nmake #ç¼–è¯‘\ncp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #å¤‡ä»½æ—§çš„nginx\ncp ./objs/nginx /usr/local/nginx/sbin/ #ç„¶åå°†æ–°çš„nginxçš„ç¨‹åºå¤åˆ¶ä¸€ä»½\ncd /usr/local/nginx/sbin/ #åˆ‡æ¢åˆ°sbinç›®å½•\n./nginx -s reload #é‡å¯nginxæœåŠ¡\n```\n\nå¦‚æœé‡å¯æˆåŠŸçš„è¯ï¼Œæ‰“å¼€æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œè¿™æ—¶å€™åº”è¯¥å¯ä»¥åœ¨é“¾æ¥æ—è¾¹çœ‹åˆ°ä¸€ä¸ªå°é”äº†ï¼\n\n[^1]:https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\n\n[^2]: https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\n[^ 3]: https://blog.csdn.net/qq_26369317/article/details/102863613\n\n","source":"_posts/åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“.md","raw":"---\ntitle: åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“\ndate: 2020-1-8 10:29\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- Nginx\n\t- Internet server\n\t- Network Technology\n\t- Experience\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://kinsta.com/wp-content/uploads/2018/03/what-is-nginx.png\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: æ­å»ºè‡ªå·±çš„æœåŠ¡å™¨å¹¶ä¸éš¾ï¼Œåªæ˜¯è¿‡ç¨‹è¾ƒä¸ºå¤æ‚ã€‚\n\n#You can begin to input your article below now.\n---\n\n> ç”±äºè‡ªå·±æ˜¯å»å¹´ä¸ƒæœˆé…ç½®å¥½çš„æœåŠ¡å™¨ï¼Œæœ‰ä¸€äº›ç»†èŠ‚æˆ–è€…é‡åˆ°çš„é—®é¢˜å·²ç»è®°ä¸å¤ªæ¸…ï¼Œæ•…æœ¬æ–‡å¯èƒ½ä¼šæœ‰ä¸å®Œæ•´çš„åœ°æ–¹ï¼Œé‡åˆ°é—®é¢˜è¯·å–„ç”¨æœç´¢å¼•æ“ï¼Œè€Œä¸”æœåŠ¡å™¨çš„é…ç½®æ–¹æ³•ä¹Ÿä¸åªæœ‰è¿™ä¸€ç§ã€‚æœ¬æ–‡ä¸»è¦ç”¨ä½œå¯¹è‡ªå·±æ“ä½œæ­¥éª¤å’Œæ–¹æ³•çš„ä¸€ä¸ªæ€»ç»“ï¼Œä»¥ä¾¿äºæ—¥åæŸ¥é˜…ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚\n\n### è´­ä¹°æœåŠ¡å™¨\n\né¦–å…ˆå»[åä¸ºäº‘å®˜ç½‘](https://www.huaweicloud.com/?locale=zh-cn)æ³¨å†Œä¸€ä¸ªè´¦å·ã€‚å¦‚æœæ˜¯å­¦ç”Ÿï¼Œå¯ä»¥æœç´¢â€œå­¦ç”Ÿâ€ï¼Œå¹¶è¿›è¡Œå­¦ç”Ÿè®¤è¯ã€‚å­¦ç”Ÿè®¤è¯çš„æ­¥éª¤å‚è§[å­¦ç”Ÿè®¤è¯æµç¨‹](https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html)ã€‚è¿›è¡Œèº«ä»½éªŒè¯åå¯ä»¥è´­ä¹°å­¦ç”Ÿä¼˜æƒ å¥—é¤ï¼Œäº‘æœåŠ¡å™¨ä»·æ ¼åªè¦99å…ƒ/å¹´ï¼Œæ¯”é˜¿é‡Œäº‘å’Œè…¾è®¯äº‘çš„éƒ½è¦ä¾¿å®œä¸€äº›ã€‚\n\n![åä¸ºäº‘å­¦ç”Ÿä¼˜æƒ ](https://astrobear.top/resource/astroblog/content/hwcloud_discount.png)\n\nè´­ä¹°å®Œæˆåï¼Œä½ å¯ä»¥åœ¨æ§åˆ¶å°çœ‹åˆ°è‡ªå·±ç°æœ‰çš„èµ„æºä»¥åŠè¿è¡Œæƒ…å†µã€‚\n\n![æ§åˆ¶å°](https://astrobear.top/resource/astroblog/content/console.png)\n\n### é…ç½®å®‰å…¨ç»„\n\n> å®‰å…¨ç»„æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„åˆ†ç»„ï¼Œä¸ºå…·æœ‰ç›¸åŒå®‰å…¨ä¿æŠ¤éœ€æ±‚å¹¶ç›¸äº’ä¿¡ä»»çš„äº‘æœåŠ¡å™¨æä¾›è®¿é—®ç­–ç•¥ã€‚å®‰å…¨ç»„åˆ›å»ºåï¼Œç”¨æˆ·å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­å®šä¹‰å„ç§è®¿é—®è§„åˆ™ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚\n>\n> ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªç”¨æˆ·é»˜è®¤åˆ›å»ºä¸€ä¸ªé»˜è®¤å®‰å…¨ç»„ï¼Œé»˜è®¤å®‰å…¨ç»„çš„è§„åˆ™æ˜¯åœ¨å‡ºæ–¹å‘ä¸Šçš„æ•°æ®æŠ¥æ–‡å…¨éƒ¨æ”¾è¡Œï¼Œå…¥æ–¹å‘è®¿é—®å—é™ï¼Œå®‰å…¨ç»„å†…çš„äº‘æœåŠ¡å™¨æ— éœ€æ·»åŠ è§„åˆ™å³å¯äº’ç›¸è®¿é—®ã€‚é»˜è®¤å®‰å…¨ç»„å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚\n>\n> å®‰å…¨ç»„åˆ›å»ºåï¼Œä½ å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­è®¾ç½®å‡ºæ–¹å‘ã€å…¥æ–¹å‘è§„åˆ™ï¼Œè¿™äº›è§„åˆ™ä¼šå¯¹å®‰å…¨ç»„å†…éƒ¨çš„äº‘æœåŠ¡å™¨å‡ºå…¥æ–¹å‘ç½‘ç»œæµé‡è¿›è¡Œè®¿é—®æ§åˆ¶ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚[^1]\n\nåœ¨æ§åˆ¶å°ç‚¹å‡»â€œå¼¹æ€§äº‘æœåŠ¡å™¨ECSâ€ï¼Œåœ¨è¿™é‡Œä½ å¯çœ‹åˆ°ä½ çš„æœåŠ¡å™¨çš„å…¬ç½‘IPï¼Œè¯·è®°ä¸‹è¿™ä¸ªIPåœ°å€ã€‚ç„¶åç‚¹å‡»åœ¨åˆ—è¡¨ä¸­ç‚¹å‡»ä½ çš„æœåŠ¡å™¨çš„åç§°ã€‚\n\n![é€‰æ‹©æœåŠ¡å™¨](https://astrobear.top/resource/astroblog/content/security_groups.png)\n\nè¿›å…¥äº‘æœåŠ¡å™¨ç®¡ç†é¡µé¢åï¼Œç‚¹å‡»â€œå®‰å…¨ç»„â€ã€‚å†ç‚¹å‡»â€œSys-defaultâ€å¯ä»¥çœ‹åˆ°é»˜è®¤å®‰å…¨ç»„ã€‚ç„¶åä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯æˆ‘ç›®å‰çš„å®‰å…¨ç»„è®¾ç½®ï¼Œä»…ä¾›å‚è€ƒã€‚é€‰æ‹©â€œå…¥/å‡ºæ–¹å‘æ–¹å‘è§„åˆ™â€ï¼Œå†ç‚¹å‡»â€œæ·»åŠ è§„åˆ™â€œå³å¯æ‰‹åŠ¨æ·»åŠ è§„åˆ™ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œé…ç½®çš„éƒ½æ˜¯å…¥æ–¹å‘çš„å®‰å…¨ç»„ï¼Œå¹¶ä¸”æºåœ°å€ï¼ˆè®¿é—®æœåŠ¡å™¨çš„è®¾å¤‡çš„IPåœ°å€ï¼‰éƒ½ä¸ºâ€œ0.0.0.0/0â€ï¼ˆæ‰€æœ‰IPåœ°å€ï¼‰ã€‚\n\né€šå¸¸éœ€è¦é…ç½®å¦‚ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š\n\n- SSHè¿œç¨‹è¿æ¥Linuxå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šSSHï¼Œç«¯å£ï¼š22ï¼‰\n- å…¬ç½‘â€œpingâ€ECSå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šICMPï¼Œç«¯å£ï¼šå…¨éƒ¨ï¼‰\n- å¼¹æ€§äº‘æœåŠ¡å™¨ä½œWebæœåŠ¡å™¨\n  - åè®®ï¼šhttpï¼Œç«¯å£ï¼š80\n  - åè®®ï¼šhttpsï¼Œç«¯å£ï¼š433\n\nè¯¦ç»†é…ç½®è¯·å‚è€ƒ[å®‰å…¨ç»„é…ç½®ç¤ºä¾‹](https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html)ã€‚\n\n![å®‰å…¨ç»„è®¾ç½®](https://astrobear.top/resource/astroblog/content/sg_settings.png)\n\n![å®‰å…¨ç»„è®¾ç½®](https://astrobear.top/resource/astroblog/content/sg_settings1.png)\n\né…ç½®å®Œæˆåï¼Œå¯ä»¥æ‰“å¼€ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œç”¨ä¸‹é¢çš„è¯­å¥æµ‹è¯•ä¸€ä¸‹ï¼š\n\n`ping ä½ çš„å…¬ç½‘IP`\n\nå‡ºç°ç±»ä¼¼ä¸‹é¢çš„å†…å®¹å°±ä»£è¡¨æˆåŠŸäº†ï¼š\n\n![pingæµ‹è¯•](https://astrobear.top/resource/astroblog/content/ping_test.png)\n\nä½ å¯ä»¥æŒ‰ä¸‹`Ctrl+C`æ¥ç»“æŸ`ping`è¿™ä¸ªè¿›ç¨‹ã€‚\n\nç„¶ååœ¨ç»ˆç«¯é‡Œè¾“å…¥ï¼š\n\n`ssh ä½ çš„å…¬ç½‘IP`\n\nå¦‚æœä½ çš„å®‰å…¨ç»„é…ç½®æ­£ç¡®çš„è¯ï¼Œä¼šè®©ä½ è¾“å…¥æœåŠ¡å™¨çš„ç™»å½•å¯†ç ã€‚è¾“å…¥å¯†ç ï¼ˆæ³¨æ„ï¼šå¯†ç æ˜¯ä¸ä¼šæ˜¾ç¤ºçš„ï¼‰åå›è½¦ï¼Œåº”è¯¥å¯ä»¥çœ‹åˆ°è¿™æ ·çš„è¾“å‡ºï¼š\n\n![sshç™»å½•](https://astrobear.top/resource/astroblog/content/ssh_login.png)\n\nè¿™ä¸ªæ—¶å€™ï¼Œä½ çš„ç»ˆç«¯å°±å·²ç»è¿æ¥ä¸Šäº†æœåŠ¡å™¨çš„ç³»ç»Ÿäº†ï¼Œä½ åœ¨ç»ˆç«¯é‡Œçš„ä¸€åˆ‡æ“ä½œéƒ½æ˜¯ä½œç”¨åœ¨æœåŠ¡å™¨ä¸Šçš„ã€‚\n\n### åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\n\né¦–å…ˆè¯·åœ¨ç»ˆç«¯ä½¿ç”¨sshç™»å½•ä½ çš„æœåŠ¡å™¨ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢ç»™å‡ºçš„é¡ºåºè¾“å…¥å‘½ä»¤ã€‚\n\n```shell\nyum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #å®‰è£…ç¼–è¯‘å·¥å…·åŠåº“æ–‡ä»¶\ncd /usr/local/ #åˆ‡æ¢åˆ°ç›®æ ‡å®‰è£…æ–‡ä»¶å¤¹\nwget http://nginx.org/download/nginx-1.16.1.tar.gz #ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„Nginx\ntar -zxvf nginx-1.16.1.tar.gz #è§£å‹æ–‡ä»¶\ncd nginx-1.16.1 #è¿›å…¥è§£å‹çš„æ–‡ä»¶å¤¹\n./configure #æ‰§è¡Œç¨‹åº\nmake #ç¼–è¯‘\nmake install #å®‰è£…\ncd /usr/local/nginx/sbin #è¿›å…¥Nginxå®‰è£…ç›®å½•\n./nginx #è¿è¡ŒNginx\n```\n\næ­¤æ—¶ï¼Œå®‰è£…åº”è¯¥å·²ç»å®Œæˆäº†ã€‚æ‰“å¼€æµè§ˆå™¨ï¼Œåœ¨åœ°å€æ ä¸­è¾“å…¥ä½ çš„å…¬ç½‘ipã€‚å¦‚æœçœ‹åˆ°ä¸‹å›¾æ‰€ç¤ºå†…å®¹ï¼Œå°±ä»£è¡¨å®‰è£…æˆåŠŸäº†ã€‚\n\n![nginxå®‰è£…æˆåŠŸ](https://astrobear.top/resource/astroblog/content/nginx_install.png)\n\n### åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\n\nåœ¨æ‹¥æœ‰äº†è‡ªå·±çš„æœåŠ¡å™¨ä»¥åï¼Œå°±å¯ä»¥åšå¾ˆå¤šäº‹æƒ…äº†ã€‚ä½†æ˜¯ç°åœ¨ä½ åªèƒ½é€šè¿‡IPåœ°å€è®¿é—®è‡ªå·±çš„æœåŠ¡å™¨ï¼Œçœ‹èµ·æ¥æ€»æ˜¯æœ‰ç‚¹åˆ«æ‰­ã€‚å¦å¤–ï¼Œå¦‚æœä½ æƒ³è¦ç½‘ç«™æœ‰ä¸€å®šçš„å½±å“åŠ›çš„è¯ï¼Œä»…æœ‰IPåœ°å€ä¼šè®©äººå‡ ä¹æ‰¾ä¸åˆ°ä½ çš„ç½‘ç«™ï¼Œè€Œä¸”ä¹Ÿä¸ç¬¦åˆå›½å®¶æ³•å¾‹è§„å®šã€‚æ‰€ä»¥è¿˜æ˜¯å»ºè®®å¤§å®¶å¼„ä¸€ä¸ªè‡ªå·±çš„åŸŸåã€‚\n\nç°åœ¨å¸‚é¢ä¸Šçš„äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›åŸŸåæ³¨å†Œçš„æœåŠ¡ï¼Œç›´æ¥åœ¨ä½ çš„æœåŠ¡æä¾›å•†çš„å¹³å°ä¸Šé¢æ³¨å†Œå³å¯ã€‚ä¸‹é¢æˆ‘ç»§ç»­ç”¨åä¸ºäº‘çš„å¹³å°æ¼”ç¤ºã€‚\n\né¦–å…ˆåœ¨åä¸ºäº‘ç½‘ç«™é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œåŸŸåâ€ï¼Œæ‰“å¼€ç¬¬ä¸€ä¸ªé“¾æ¥â€œåŸŸåæ³¨å†ŒæœåŠ¡â€ã€‚ä¹Ÿå¯ä»¥ç›´æ¥ç‚¹å‡»è¿™é‡Œï¼š[åŸŸåæ³¨å†ŒæœåŠ¡_åä¸ºäº‘](https://www.huaweicloud.com/product/domain.html)ã€‚\n\nç„¶åä½ å¯ä»¥åœ¨ç½‘é¡µä¸­é€‰æ‹©ä½ çš„åŸŸåï¼Œå¸¸è§çš„å¦‚`.com`ï¼Œ`.cn`ï¼Œ`.net`ç­‰ã€‚è¿™äº›åŸŸåä¼šç›¸å¯¹æ¯”è¾ƒè´µã€‚ä½œä¸ºå­¦ç”Ÿå…šï¼Œæˆ‘é€‰æ‹©ä¸€ä¸ªæœ€ä¾¿å®œçš„åŸŸå`.top`ï¼Œåªéœ€è¦9å…ƒ/å¹´ã€‚\n\nç‚¹å‡»ä½ æƒ³è¦çš„åŸŸååï¼Œä¼šè·³è½¬åˆ°ä¸€ä¸ªæ–°çš„é¡µé¢ã€‚æ¥ä¸‹æ¥å†æ¬¡é€‰æ‹©ä½ è¦çš„åŸŸåï¼Œå¹¶ä¸”åœ¨â€œæŸ¥åŸŸåâ€çš„æœç´¢æ¡†å†…è¾“å…¥ä½ æƒ³è¦çš„åŸŸåï¼Œçœ‹çœ‹æ˜¯å¦å·²ç»è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨äº†å°±æ¢ä¸€ä¸ªã€‚è‹¥æ˜¾ç¤ºâ€œåŸŸåå¯æ³¨å†Œâ€ï¼Œå°±ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚\n\n![åŸŸåè´­ä¹°](https://astrobear.top/resource/astroblog/content/buy_domain.png)\n\nè´­ä¹°å®Œæˆåï¼Œä½ å°±æ‹¥æœ‰äº†è‡ªå·±åŸŸåäº†ï¼\n\n### å¤‡æ¡ˆ\n\n> å¤‡æ¡ˆæ˜¯ä¸­å›½å¤§é™†çš„ä¸€é¡¹æ³•è§„ï¼Œä½¿ç”¨å¤§é™†èŠ‚ç‚¹æœåŠ¡å™¨æä¾›äº’è”ç½‘ä¿¡æ¯æœåŠ¡çš„ç”¨æˆ·ï¼Œéœ€è¦åœ¨æœåŠ¡å™¨æä¾›å•†å¤„æäº¤å¤‡æ¡ˆç”³è¯·ã€‚\n>\n> æ ¹æ®å·¥ä¿¡éƒ¨ã€Šäº’è”ç½‘ä¿¡æ¯æœåŠ¡ç®¡ç†åŠæ³•ã€‹(å›½åŠ¡é™¢292å·ä»¤)å’Œå·¥ä¿¡éƒ¨ä»¤ç¬¬33å·ã€Šéç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å¤‡æ¡ˆç®¡ç†åŠæ³•ã€‹è§„å®šï¼Œå›½å®¶å¯¹ç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œè®¸å¯åˆ¶åº¦ï¼Œå¯¹éç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œå¤‡æ¡ˆåˆ¶åº¦ã€‚æœªå–å¾—è®¸å¯æˆ–è€…æœªå±¥è¡Œå¤‡æ¡ˆæ‰‹ç»­çš„ï¼Œä¸å¾—ä»äº‹äº’è”ç½‘ä¿¡æ¯æœåŠ¡ï¼Œå¦åˆ™å±è¿æ³•è¡Œä¸ºã€‚é€šä¿—æ¥è®²ï¼Œè¦å¼€åŠç½‘ç«™å¿…é¡»å…ˆåŠç†ç½‘ç«™å¤‡æ¡ˆï¼Œå¤‡æ¡ˆæˆåŠŸå¹¶è·å–é€šä¿¡ç®¡ç†å±€ä¸‹å‘çš„ICPå¤‡æ¡ˆå·åæ‰èƒ½å¼€é€šè®¿é—®ã€‚[^2]\n\nè¿™ä¸€æ­¥ä¸å¤šè¯´äº†ï¼Œå…·ä½“æ­¥éª¤æ¯”è¾ƒç¹çï¼ŒèŠ±è´¹çš„æ—¶é—´ä¹Ÿæ¯”è¾ƒé•¿ï¼Œéœ€è¦ä¸€ä¸¤å‘¨ã€‚ç½‘ç«™ä¸Šæœ‰å¾ˆæ¸…æ™°çš„[æ“ä½œæ–¹æ³•](https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html)ï¼Œè¯·è‡ªè¡ŒæŸ¥é˜…ï¼Œæ ¹æ®æ­¥éª¤æ“ä½œå³å¯ã€‚éœ€è¦æ³¨æ„ä¸€ç‚¹çš„æ˜¯ï¼Œåœ¨å®¡æ ¸è¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ¥åˆ°æœåŠ¡æä¾›å•†æ‰“æ¥çš„ç”µè¯ï¼Œä¸è¦æ¼æ¥ã€‚\n\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢çš„å¤‡æ¡ˆæ“ä½œæ˜¯åœ¨å·¥ä¿¡éƒ¨å¤‡æ¡ˆçš„ã€‚å®Œæˆäº†åœ¨å·¥ä¿¡éƒ¨çš„å¤‡æ¡ˆä»¥åè¿˜éœ€è¦å…¬å®‰å¤‡æ¡ˆã€‚å…·ä½“[æ“ä½œæ–¹æ³•](http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c)ä¹Ÿè¯·è‡ªè¡ŒæŸ¥é˜…ã€‚\n\n### åŸŸåè§£æ\n\nåœ¨å®Œæˆä¸€ç³»åˆ—ç¹ççš„å¤‡æ¡ˆæµç¨‹ä»¥åï¼Œä½ çš„ç½‘ç«™è¿˜ä¸å¯ä»¥é€šè¿‡åŸŸåè®¿é—®ã€‚åªæœ‰æŠŠä½ çš„åŸŸåè·ŸæœåŠ¡å™¨çš„IPåœ°å€ç»‘å®šåœ¨ä¸€èµ·ä¹‹åï¼Œå¹¶ä¸”åœ¨æœåŠ¡å™¨ä¸Šä¿®æ”¹äº†é…ç½®æ–‡ä»¶ä¹‹åæ‰å¯ä»¥ã€‚\n\né¦–å…ˆæ‰“å¼€ç®¡ç†æ§åˆ¶å°ï¼Œåœ¨æ§åˆ¶å°ä¸­é€‰æ‹©â€œåŸŸåæ³¨å†Œâ€ã€‚ç„¶ååœ¨ä¸‹é¢çš„é¡µé¢ä¸­ç‚¹å‡»â€œè§£æâ€ã€‚\n\n![åŸŸåæ³¨å†Œ](https://astrobear.top/resource/astroblog/content/domain.png)\n\nç‚¹å‡»ä½ çš„åŸŸåï¼Œæ˜¾ç¤ºå¦‚ä¸‹é¡µé¢ã€‚è¿™é‡Œæ˜¾ç¤ºçš„æ˜¯ä½ åŸŸåçš„è®°å½•é›†ï¼Œå‰ä¸¤ä¸ªè®°å½•é›†åº”è¯¥æ˜¯é¢„ç½®è®¾ç½®ï¼Œä¸å¯æš‚åœæœåŠ¡ã€‚<span id=\"1\">ä½ å¯ä»¥åœ¨è¿™åŸºç¡€ä¸Šæ·»åŠ è‡ªå·±çš„è®°å½•é›†ã€‚</span>\n\n![è®°å½•é›†](https://astrobear.top/resource/astroblog/content/record.png)\n\nç‚¹å‡»é¡µé¢å³ä¸Šè§’çº¢è‰²æŒ‰é’®ä»¥æ·»åŠ è®°å½•é›†ã€‚æ·»åŠ è®°å½•é›†çš„é…ç½®å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä¸‹å›¾ä¸­ç»™å‡ºçš„ä¾‹å­æ˜¯æ·»åŠ çš„â€œAâ€å‹è®°å½•é›†ï¼Œä¹Ÿå³é€šè¿‡`example.com`è®¿é—®ç½‘ç«™ã€‚è‹¥éœ€è¦é€šè¿‡`www.example.com`è®¿é—®ç½‘ç«™ï¼Œåˆ™éœ€è¦ä¸º`example.com`çš„å­åŸŸåæ·»åŠ â€œAâ€å‹è®°å½•é›†ã€‚å…·ä½“é…ç½®å‚è§ï¼š[é…ç½®ç½‘ç«™è§£æ_åä¸ºäº‘](https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1)ã€‚ç‚¹å‡»â€œç¡®å®šâ€ï¼Œå®Œæˆæ·»åŠ ã€‚ä½ å¯ä»¥é€šè¿‡`ping ä½ çš„åŸŸå`æ¥æµ‹è¯•ä½ æ·»åŠ çš„è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆäº†ã€‚\n\n![æ·»åŠ è®°å½•é›†](https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png)\n\n### é…ç½®nginx\n\n<span id=\"2\">æ‰“å¼€</span>ä½ ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œè¾“å…¥å‘½ä»¤ï¼š`ssh ä½ çš„IPåœ°å€`ï¼Œè¾“å…¥ä½ çš„æœåŠ¡å™¨çš„å¯†ç ã€‚\n\nè¿›å…¥ä½ çš„nginxçš„å®‰è£…ç›®å½•ï¼š`cd /usr/local/nginx/`ã€‚\n\nä½¿ç”¨vimæ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ï¼š`vim ./conf/nginx.conf`ã€‚\n\næŒ‰`I`å¼€å§‹è¾“å…¥ã€‚\n\nåœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š\n\n```nginx\nserver {\n\t    listen   80; #ç›‘å¬ç«¯å£è®¾ä¸º 80\n\t    server_name  example.com; #ç»‘å®šæ‚¨çš„åŸŸå\n\t    index index.htm index.html; #æŒ‡å®šé»˜è®¤æ–‡ä»¶\n\t    root html; #æŒ‡å®šç½‘ç«™æ ¹ç›®å½•\n}\n```\n\nç„¶åæŒ‰`esc`é€€å‡ºç¼–è¾‘ï¼Œå†æŒ‰`Shift+zz`ä¿å­˜ã€‚\n\nè¾“å…¥ï¼š`cd ./sbin`ï¼Œåˆ‡æ¢æ–‡ä»¶å¤¹ã€‚\n\næ‰§è¡Œå‘½ä»¤ï¼š`nginx -s relod`ï¼Œé‡å¯nginxæœåŠ¡ã€‚\n\nè¿™æ—¶å€™å†å°è¯•ç”¨æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œåº”è¯¥ä¼šæ˜¾ç¤ºä¹‹å‰å‡ºç°è¿‡çš„â€œWelcome to nginx â€çš„é¡µé¢äº†ï¼\n\n### ç”³è¯·SSLè¯ä¹¦\n\nSSLè¯ä¹¦å¯ä»¥åœ¨æ•°æ®ä¼ è¾“çš„è¿‡ç¨‹ä¸­å¯¹å…¶è¿›è¡ŒåŠ å¯†å’Œéšè—ï¼Œå¯ä»¥æå¤§åœ°æé«˜æ•°æ®ä¼ è¾“çš„å®‰å…¨æ€§ã€‚æ‹¥æœ‰SSLè¯ä¹¦çš„ç½‘ç«™çš„è¯·æ±‚å¤´éƒ½æ˜¯`https`ï¼Œå¹¶ä¸”åœ¨é“¾æ¥æ—è¾¹ä¼šå‡ºç°ä¸€æŠŠå°é”ã€‚ä½†æ˜¯ï¼ŒSSLè¯ä¹¦å¹¶ä¸æ˜¯æ‰€æœ‰ç½‘ç«™éƒ½å¿…é¡»çš„ï¼Œè¿™è§†ä½ çš„éœ€è¦è€Œå®šã€‚æ¯”å¦‚ï¼Œå¾®ä¿¡å°ç¨‹åºçš„æœåŠ¡å™¨å°±å¿…é¡»è¦æœ‰åŸŸåå’ŒSSLè¯ä¹¦ã€‚å¦å¤–ï¼Œå‡ºäºä¿¡æ¯ä¼ è¾“çš„å®‰å…¨æ€§æ–¹é¢çš„è€ƒè™‘ï¼Œæœ‰SSLè¯ä¹¦è¿˜æ˜¯æ˜¾å¾—æ›´ä¸ºå¦¥å½“å’Œä¸“ä¸šä¸€ç‚¹ã€‚\n\nç°åœ¨å¸‚é¢ä¸Šå„å¤§äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›é…å¥—çš„SSLè¯ä¹¦ç”³è¯·æœåŠ¡ï¼Œä¸€èˆ¬éƒ½æ˜¯æä¾›ä¼ä¸šçº§çš„è¯ä¹¦ï¼Œä»·æ ¼æ¯”è¾ƒæ˜‚è´µã€‚ä½†æ˜¯åŒæ—¶ç½‘ç»œä¸Šä¹Ÿæœ‰ä¸€äº›å…è´¹çš„SSLè¯ä¹¦æœåŠ¡å¯ä»¥é€‰æ‹©ã€‚ä¸‹é¢è¿˜æ˜¯ä»¥åä¸ºäº‘çš„å¹³å°ä¸ºä¾‹ï¼Œç®€å•è¯´æ˜ä¸€ä¸‹å¦‚ä½•ç”³è¯·SSLè¯ä¹¦ã€‚\n\né¦–å…ˆåœ¨åä¸ºäº‘é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œå…è´¹è¯ä¹¦â€œï¼Œç„¶åç‚¹å‡»[äºšæ´²è¯šä¿¡åŸŸåå‹DVå•åŸŸåSSLè¯ä¹¦--å…è´¹è¯ä¹¦](https://marketplace.huaweicloud.com/product/00301-315148-0--0)ï¼Œå¯ä»¥çœ‹åˆ°è¯ä¹¦çš„ä»·æ ¼æ˜¯0.00å…ƒã€‚ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚\n\n![è´­ä¹°SSLè¯ä¹¦](https://astrobear.top/resource/astroblog/content/buy_ssl.png)\n\nå®Œæˆè´­ä¹°åè¯·ä¸è¦ç«‹å³å…³é—­é¡µé¢ï¼Œé¡µé¢ä¸­çš„è®¢å•å·åœ¨ä¹‹åè¿˜éœ€è¦ç”¨åˆ°ã€‚å°”åï¼Œç³»ç»Ÿä¼šå‘é€â€HuaweiCloudè´¦æˆ·ç”³è¯·â€é‚®ä»¶è‡³ç”¨æˆ·é‚®ç®±ï¼Œå³ä½ åœ¨åä¸ºäº‘çš„æ³¨å†Œé‚®ç®±ã€‚\n\n![HuaweiCloudè´¦æˆ·ç”³è¯·](https://astrobear.top/resource/astroblog/content/request_account.png)\n\nç‚¹å‡»é‚®ä»¶ä¸­çš„ç™»å½•åœ°å€è¿›å…¥ç³»ç»Ÿï¼Œå¹¶ä½¿ç”¨é‚®ä»¶æä¾›çš„è´¦å·å’Œåˆå§‹å¯†ç è¿›è¡Œç™»å½•ã€‚ç™»å…¥ç³»ç»Ÿåè¯·ä¿®æ”¹ä½ çš„åˆå§‹å¯†ç ï¼Œç„¶åè¯·æ ¹æ®åä¸ºäº‘ä¸­ç»™ä½ æä¾›çš„è®¢å•å·åœ¨è¯¥ç³»ç»Ÿä¸­æŸ¥è¯¢ä½ çš„è®¢å•ã€‚æŸ¥è¯¢åˆ°ä½ çš„è®¢å•ä»¥åï¼Œéœ€è¦ä½ è¡¥å……ä¸€äº›ä¿¡æ¯ï¼Œè¯·å¦‚å®å¡«å†™ã€‚ç³»ç»Ÿä¼šè¦ä½ å¡«å†™å…¬å¸ä¿¡æ¯ï¼Œå¦‚æœåªæ˜¯ä¸ªäººç½‘ç«™ï¼Œé‚£ä¹ˆå…¬å¸åç§°ç›´æ¥å¡«å†™ä½ çš„åå­—å³å¯ï¼Œå…¬å¸åœ°å€å°±å¡«å†™ä½ çš„ä½å€ã€‚\n\nå¡«å†™å®Œæˆåä¼šè¿›å…¥å®¡æ ¸é˜¶æ®µï¼Œç³»ç»Ÿä¼šç»™ä½ å‘é€ä¸€å°é‚®ä»¶ã€‚\n\n![è¯ä¹¦å®¡æ ¸](https://astrobear.top/resource/astroblog/content/check.png)\n\næ ¹æ®é‚®ä»¶çš„æç¤ºï¼Œéœ€è¦åœ¨è®°å½•é›†ä¸­æ·»åŠ æ–°çš„å†…å®¹ã€‚è¯·æ ¹æ®[å‰æ–‡](#1)æ‰€è¿°æ–¹æ³•ï¼Œå°†é‚®ä»¶ä¸­çš„å†…å®¹æ·»åŠ è‡³æ–°çš„è®°å½•é›†ã€‚å¡«å†™æ–¹æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚\n\n![å¡«å†™è®°å½•é›†](https://astrobear.top/resource/astroblog/content/modify_record.png)\n\nå¡«å†™å®Œæˆåï¼Œå¯ä»¥åœ¨æœ¬åœ°ç”µè„‘çš„ç»ˆç«¯é‡Œè¾“å…¥`nslookup -querytype=txt ä½ çš„åŸŸå`æ¥æµ‹è¯•è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆã€‚\n\n![æµ‹è¯•è®°å½•é›†](https://astrobear.top/resource/astroblog/content/test_record.png)\n\nä¸€èˆ¬æ¥è¯´ï¼Œè®°å½•é›†ç”Ÿæ•ˆå10åˆ†é’Ÿä»¥å†…è¯ä¹¦å°±ä¼šé¢å‘äº†ã€‚\n\n![è¯ä¹¦é¢å‘](https://astrobear.top/resource/astroblog/content/issue.png)\n\n### SSLè¯ä¹¦éƒ¨ç½²\n\næ¥ä¸‹æ¥æˆ‘ä»¬è¦æŠŠSSLè¯ä¹¦éƒ¨ç½²åˆ°æˆ‘ä»¬çš„æœåŠ¡å™¨ä¸Šã€‚\n\nåœ¨æ”¶åˆ°çš„â€œè¯ä¹¦é¢å‘â€çš„é‚®ä»¶çš„åº•éƒ¨æœ‰ä¸€æ¡é“¾æ¥ï¼Œç‚¹å‡»è¿™æ¡é“¾æ¥ï¼Œè¿›å…¥è¯ä¹¦ç®¡ç†ç³»ç»Ÿã€‚ç™»å½•ç³»ç»Ÿï¼Œåœ¨å·¦ä¾§å¯¼èˆªæ ä¸­ç‚¹å‡»â€œSSLè¯ä¹¦â€ï¼Œå†ç‚¹å‡»â€œé¢„è§ˆâ€ï¼Œå†åœ¨å³ä¾§çš„â€œä¿¡æ¯é¢„è§ˆâ€ä¸­ç‚¹å‡»â€œä¸‹è½½æœ€æ–°è¯ä¹¦â€œã€‚\n\n![ä¸‹è½½è¯ä¹¦](https://astrobear.top/resource/astroblog/content/download_cert.png)\n\nåœ¨å¼¹å‡ºçš„å¯¹è¯æ¡†å†…ï¼Œé€‰æ‹©è¯ä¹¦æ ¼å¼ä¸ºâ€œPEM(é€‚ç”¨äºNginx,SLB)â€ï¼Œè¾“å…¥ä½ çš„è®¢å•å¯†ç ã€‚è¯ä¹¦å¯†ç å¯ä»¥ç•™ç©ºã€‚\n\n![ä¸‹è½½è¯ä¹¦](https://astrobear.top/resource/astroblog/content/download_cert1.png)\n\nä¸‹è½½å®Œæˆåï¼Œè§£å‹ä¸‹è½½çš„å‹ç¼©åŒ…ï¼Œéœ€è¦è¾“å…¥ä½ çš„è®¢å•å¯†ç ï¼ˆå¦‚æœä½ æ²¡æœ‰è®¾ç½®è¯ä¹¦å¯†ç ï¼‰ã€‚è§£å‹ä»¥åå¯ä»¥å¾—åˆ°ä¸‹å›¾ä¸¤ä¸ªæ–‡ä»¶ã€‚\n\n![è§£å‹ç¼©](https://astrobear.top/resource/astroblog/content/unzip_cert.png)\n\næ¥ä¸‹æ¥ï¼Œæ‰“å¼€ä½ çš„ç»ˆç«¯ï¼ŒæŒ‰é¡ºåºè¾“å…¥ä¸‹åˆ—å‘½ä»¤ï¼š\n\n```shell\nssh ä½ çš„å…¬ç½‘IP #sshç™»å½•ï¼Œè¾“å…¥ä½ çš„å¯†ç \ncd /usr/local/nginx #åˆ‡æ¢åˆ°nginxçš„å®‰è£…ç›®å½•\nmkdir ./cert #åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹certç”¨äºå­˜æ”¾ä½ çš„è¯ä¹¦\nexit #æ–­å¼€ä¸æœåŠ¡å™¨çš„è¿æ¥\nscp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.key ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.keyæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹\nscp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.crt ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.crtæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹\n```\n\næ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä¿®æ”¹nginxçš„é…ç½®æ–‡ä»¶ã€‚å‚è€ƒ[å‰æ–‡](#2)æ‰€è¿°æ–¹æ³•æ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ã€‚å…ˆå°†ä½ ä¹‹å‰æ’å…¥çš„å†…å®¹åˆ é™¤æˆ–è€…ä½¿ç”¨`#`æ³¨é‡Šæ‰ï¼Œç„¶ååœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š\n\n```nginx\nserver {\n         listen       443 ssl;\n         server_name  example.com; #ä½ è¯ä¹¦ç»‘å®šçš„åŸŸå;\n\n        ssl_certificate      /usr/local/nginx/cert/ä½ çš„åŸŸå.crt;\n        ssl_certificate_key  /usr/local/nginx/cert/ä½ çš„åŸŸå.key;\n\n        ssl_session_cache    shared:SSL:1m;\n        ssl_session_timeout  5m;\n\n        ssl_ciphers  HIGH:!aNULL:!MD5;\n        ssl_prefer_server_ciphers  on;\n        \n        location / {\n            index index.htm index.html; #æŒ‡å®šé»˜è®¤æ–‡ä»¶ã€‚\n\t    \t\t\troot html; #æŒ‡å®šç½‘ç«™æ ¹ç›®å½•ã€‚\n        }\n}\nserver { #å°†ä½ çš„80ç«¯å£é‡å®šå‘è‡³433ç«¯å£ï¼Œå³å¼ºåˆ¶ä½¿ç”¨httpsè®¿é—®\n  \t\t\tlisten 80;\n  \t\t\tserver_name; example.com; #ä½ çš„åŸŸå\n\t\t\t\trewrite ^/(.*)$ https://example.com:443/$1 permanent;\n}\n```\n\nå°†æ–‡ä»¶ä¿å­˜ä»¥åé‡å¯nginxæœåŠ¡ã€‚\n\né‡å¯ä»¥åä½ å¯èƒ½ä¼šé‡åˆ°è¿™æ ·çš„é—®é¢˜ï¼š`**unknown directive â€œsslâ€ in /usr/local/nginx/conf/nginx.conf:121**`ï¼Œè¿™æ˜¯å› ä¸ºä½ åœ¨å®‰è£…nginxæ—¶ï¼Œæ²¡æœ‰ç¼–è¯‘SSLæ¨¡å—ã€‚ä½ å¯ä»¥åœ¨ç»ˆç«¯é‡ŒæŒ‰ç…§ä¸‹è¿°æ­¥éª¤è§£å†³[^ 3]ï¼š\n\n```shell\ncd ../nginx-1.16.1 #è¿›å…¥åˆ°nginxçš„æºç åŒ…çš„ç›®å½•ä¸‹\n./configure --with-http_ssl_module #å¸¦å‚æ•°æ‰§è¡Œç¨‹åº\nmake #ç¼–è¯‘\ncp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #å¤‡ä»½æ—§çš„nginx\ncp ./objs/nginx /usr/local/nginx/sbin/ #ç„¶åå°†æ–°çš„nginxçš„ç¨‹åºå¤åˆ¶ä¸€ä»½\ncd /usr/local/nginx/sbin/ #åˆ‡æ¢åˆ°sbinç›®å½•\n./nginx -s reload #é‡å¯nginxæœåŠ¡\n```\n\nå¦‚æœé‡å¯æˆåŠŸçš„è¯ï¼Œæ‰“å¼€æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œè¿™æ—¶å€™åº”è¯¥å¯ä»¥åœ¨é“¾æ¥æ—è¾¹çœ‹åˆ°ä¸€ä¸ªå°é”äº†ï¼\n\n[^1]:https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\n\n[^2]: https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\n[^ 3]: https://blog.csdn.net/qq_26369317/article/details/102863613\n\n","slug":"åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“","published":1,"updated":"2020-01-10T08:45:23.632Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck6ax1lgp000ej1p28m9ng16k","content":"<blockquote>\n<p>ç”±äºè‡ªå·±æ˜¯å»å¹´ä¸ƒæœˆé…ç½®å¥½çš„æœåŠ¡å™¨ï¼Œæœ‰ä¸€äº›ç»†èŠ‚æˆ–è€…é‡åˆ°çš„é—®é¢˜å·²ç»è®°ä¸å¤ªæ¸…ï¼Œæ•…æœ¬æ–‡å¯èƒ½ä¼šæœ‰ä¸å®Œæ•´çš„åœ°æ–¹ï¼Œé‡åˆ°é—®é¢˜è¯·å–„ç”¨æœç´¢å¼•æ“ï¼Œè€Œä¸”æœåŠ¡å™¨çš„é…ç½®æ–¹æ³•ä¹Ÿä¸åªæœ‰è¿™ä¸€ç§ã€‚æœ¬æ–‡ä¸»è¦ç”¨ä½œå¯¹è‡ªå·±æ“ä½œæ­¥éª¤å’Œæ–¹æ³•çš„ä¸€ä¸ªæ€»ç»“ï¼Œä»¥ä¾¿äºæ—¥åæŸ¥é˜…ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚</p>\n</blockquote>\n<h3 id=\"è´­ä¹°æœåŠ¡å™¨\"><a href=\"#è´­ä¹°æœåŠ¡å™¨\" class=\"headerlink\" title=\"è´­ä¹°æœåŠ¡å™¨\"></a>è´­ä¹°æœåŠ¡å™¨</h3><p>é¦–å…ˆå»<a href=\"https://www.huaweicloud.com/?locale=zh-cn\">åä¸ºäº‘å®˜ç½‘</a>æ³¨å†Œä¸€ä¸ªè´¦å·ã€‚å¦‚æœæ˜¯å­¦ç”Ÿï¼Œå¯ä»¥æœç´¢â€œå­¦ç”Ÿâ€ï¼Œå¹¶è¿›è¡Œå­¦ç”Ÿè®¤è¯ã€‚å­¦ç”Ÿè®¤è¯çš„æ­¥éª¤å‚è§<a href=\"https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html\">å­¦ç”Ÿè®¤è¯æµç¨‹</a>ã€‚è¿›è¡Œèº«ä»½éªŒè¯åå¯ä»¥è´­ä¹°å­¦ç”Ÿä¼˜æƒ å¥—é¤ï¼Œäº‘æœåŠ¡å™¨ä»·æ ¼åªè¦99å…ƒ/å¹´ï¼Œæ¯”é˜¿é‡Œäº‘å’Œè…¾è®¯äº‘çš„éƒ½è¦ä¾¿å®œä¸€äº›ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hwcloud_discount.png\" alt=\"åä¸ºäº‘å­¦ç”Ÿä¼˜æƒ \"></p>\n<p>è´­ä¹°å®Œæˆåï¼Œä½ å¯ä»¥åœ¨æ§åˆ¶å°çœ‹åˆ°è‡ªå·±ç°æœ‰çš„èµ„æºä»¥åŠè¿è¡Œæƒ…å†µã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/console.png\" alt=\"æ§åˆ¶å°\"></p>\n<h3 id=\"é…ç½®å®‰å…¨ç»„\"><a href=\"#é…ç½®å®‰å…¨ç»„\" class=\"headerlink\" title=\"é…ç½®å®‰å…¨ç»„\"></a>é…ç½®å®‰å…¨ç»„</h3><blockquote>\n<p>å®‰å…¨ç»„æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„åˆ†ç»„ï¼Œä¸ºå…·æœ‰ç›¸åŒå®‰å…¨ä¿æŠ¤éœ€æ±‚å¹¶ç›¸äº’ä¿¡ä»»çš„äº‘æœåŠ¡å™¨æä¾›è®¿é—®ç­–ç•¥ã€‚å®‰å…¨ç»„åˆ›å»ºåï¼Œç”¨æˆ·å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­å®šä¹‰å„ç§è®¿é—®è§„åˆ™ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚</p>\n<p>ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªç”¨æˆ·é»˜è®¤åˆ›å»ºä¸€ä¸ªé»˜è®¤å®‰å…¨ç»„ï¼Œé»˜è®¤å®‰å…¨ç»„çš„è§„åˆ™æ˜¯åœ¨å‡ºæ–¹å‘ä¸Šçš„æ•°æ®æŠ¥æ–‡å…¨éƒ¨æ”¾è¡Œï¼Œå…¥æ–¹å‘è®¿é—®å—é™ï¼Œå®‰å…¨ç»„å†…çš„äº‘æœåŠ¡å™¨æ— éœ€æ·»åŠ è§„åˆ™å³å¯äº’ç›¸è®¿é—®ã€‚é»˜è®¤å®‰å…¨ç»„å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚</p>\n<p>å®‰å…¨ç»„åˆ›å»ºåï¼Œä½ å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­è®¾ç½®å‡ºæ–¹å‘ã€å…¥æ–¹å‘è§„åˆ™ï¼Œè¿™äº›è§„åˆ™ä¼šå¯¹å®‰å…¨ç»„å†…éƒ¨çš„äº‘æœåŠ¡å™¨å‡ºå…¥æ–¹å‘ç½‘ç»œæµé‡è¿›è¡Œè®¿é—®æ§åˆ¶ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚<a href=\"https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\">^1</a></p>\n</blockquote>\n<p>åœ¨æ§åˆ¶å°ç‚¹å‡»â€œå¼¹æ€§äº‘æœåŠ¡å™¨ECSâ€ï¼Œåœ¨è¿™é‡Œä½ å¯çœ‹åˆ°ä½ çš„æœåŠ¡å™¨çš„å…¬ç½‘IPï¼Œè¯·è®°ä¸‹è¿™ä¸ªIPåœ°å€ã€‚ç„¶åç‚¹å‡»åœ¨åˆ—è¡¨ä¸­ç‚¹å‡»ä½ çš„æœåŠ¡å™¨çš„åç§°ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/security_groups.png\" alt=\"é€‰æ‹©æœåŠ¡å™¨\"></p>\n<p>è¿›å…¥äº‘æœåŠ¡å™¨ç®¡ç†é¡µé¢åï¼Œç‚¹å‡»â€œå®‰å…¨ç»„â€ã€‚å†ç‚¹å‡»â€œSys-defaultâ€å¯ä»¥çœ‹åˆ°é»˜è®¤å®‰å…¨ç»„ã€‚ç„¶åä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯æˆ‘ç›®å‰çš„å®‰å…¨ç»„è®¾ç½®ï¼Œä»…ä¾›å‚è€ƒã€‚é€‰æ‹©â€œå…¥/å‡ºæ–¹å‘æ–¹å‘è§„åˆ™â€ï¼Œå†ç‚¹å‡»â€œæ·»åŠ è§„åˆ™â€œå³å¯æ‰‹åŠ¨æ·»åŠ è§„åˆ™ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œé…ç½®çš„éƒ½æ˜¯å…¥æ–¹å‘çš„å®‰å…¨ç»„ï¼Œå¹¶ä¸”æºåœ°å€ï¼ˆè®¿é—®æœåŠ¡å™¨çš„è®¾å¤‡çš„IPåœ°å€ï¼‰éƒ½ä¸ºâ€œ0.0.0.0/0â€ï¼ˆæ‰€æœ‰IPåœ°å€ï¼‰ã€‚</p>\n<p>é€šå¸¸éœ€è¦é…ç½®å¦‚ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š</p>\n<ul>\n<li>SSHè¿œç¨‹è¿æ¥Linuxå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šSSHï¼Œç«¯å£ï¼š22ï¼‰</li>\n<li>å…¬ç½‘â€œpingâ€ECSå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šICMPï¼Œç«¯å£ï¼šå…¨éƒ¨ï¼‰</li>\n<li>å¼¹æ€§äº‘æœåŠ¡å™¨ä½œWebæœåŠ¡å™¨<ul>\n<li>åè®®ï¼šhttpï¼Œç«¯å£ï¼š80</li>\n<li>åè®®ï¼šhttpsï¼Œç«¯å£ï¼š433</li>\n</ul>\n</li>\n</ul>\n<p>è¯¦ç»†é…ç½®è¯·å‚è€ƒ<a href=\"https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html\">å®‰å…¨ç»„é…ç½®ç¤ºä¾‹</a>ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings.png\" alt=\"å®‰å…¨ç»„è®¾ç½®\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings1.png\" alt=\"å®‰å…¨ç»„è®¾ç½®\"></p>\n<p>é…ç½®å®Œæˆåï¼Œå¯ä»¥æ‰“å¼€ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œç”¨ä¸‹é¢çš„è¯­å¥æµ‹è¯•ä¸€ä¸‹ï¼š</p>\n<p><code>ping ä½ çš„å…¬ç½‘IP</code></p>\n<p>å‡ºç°ç±»ä¼¼ä¸‹é¢çš„å†…å®¹å°±ä»£è¡¨æˆåŠŸäº†ï¼š</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ping_test.png\" alt=\"pingæµ‹è¯•\"></p>\n<p>ä½ å¯ä»¥æŒ‰ä¸‹<code>Ctrl+C</code>æ¥ç»“æŸ<code>ping</code>è¿™ä¸ªè¿›ç¨‹ã€‚</p>\n<p>ç„¶ååœ¨ç»ˆç«¯é‡Œè¾“å…¥ï¼š</p>\n<p><code>ssh ä½ çš„å…¬ç½‘IP</code></p>\n<p>å¦‚æœä½ çš„å®‰å…¨ç»„é…ç½®æ­£ç¡®çš„è¯ï¼Œä¼šè®©ä½ è¾“å…¥æœåŠ¡å™¨çš„ç™»å½•å¯†ç ã€‚è¾“å…¥å¯†ç ï¼ˆæ³¨æ„ï¼šå¯†ç æ˜¯ä¸ä¼šæ˜¾ç¤ºçš„ï¼‰åå›è½¦ï¼Œåº”è¯¥å¯ä»¥çœ‹åˆ°è¿™æ ·çš„è¾“å‡ºï¼š</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ssh_login.png\" alt=\"sshç™»å½•\"></p>\n<p>è¿™ä¸ªæ—¶å€™ï¼Œä½ çš„ç»ˆç«¯å°±å·²ç»è¿æ¥ä¸Šäº†æœåŠ¡å™¨çš„ç³»ç»Ÿäº†ï¼Œä½ åœ¨ç»ˆç«¯é‡Œçš„ä¸€åˆ‡æ“ä½œéƒ½æ˜¯ä½œç”¨åœ¨æœåŠ¡å™¨ä¸Šçš„ã€‚</p>\n<h3 id=\"åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\"><a href=\"#åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\" class=\"headerlink\" title=\"åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\"></a>åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx</h3><p>é¦–å…ˆè¯·åœ¨ç»ˆç«¯ä½¿ç”¨sshç™»å½•ä½ çš„æœåŠ¡å™¨ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢ç»™å‡ºçš„é¡ºåºè¾“å…¥å‘½ä»¤ã€‚</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #å®‰è£…ç¼–è¯‘å·¥å…·åŠåº“æ–‡ä»¶</span><br><span class=\"line\">cd /usr/local/ #åˆ‡æ¢åˆ°ç›®æ ‡å®‰è£…æ–‡ä»¶å¤¹</span><br><span class=\"line\">wget http://nginx.org/download/nginx-1.16.1.tar.gz #ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„Nginx</span><br><span class=\"line\">tar -zxvf nginx-1.16.1.tar.gz #è§£å‹æ–‡ä»¶</span><br><span class=\"line\">cd nginx-1.16.1 #è¿›å…¥è§£å‹çš„æ–‡ä»¶å¤¹</span><br><span class=\"line\">./configure #æ‰§è¡Œç¨‹åº</span><br><span class=\"line\">make #ç¼–è¯‘</span><br><span class=\"line\">make install #å®‰è£…</span><br><span class=\"line\">cd /usr/local/nginx/sbin #è¿›å…¥Nginxå®‰è£…ç›®å½•</span><br><span class=\"line\">./nginx #è¿è¡ŒNginx</span><br></pre></td></tr></table></figure>\n\n<p>æ­¤æ—¶ï¼Œå®‰è£…åº”è¯¥å·²ç»å®Œæˆäº†ã€‚æ‰“å¼€æµè§ˆå™¨ï¼Œåœ¨åœ°å€æ ä¸­è¾“å…¥ä½ çš„å…¬ç½‘ipã€‚å¦‚æœçœ‹åˆ°ä¸‹å›¾æ‰€ç¤ºå†…å®¹ï¼Œå°±ä»£è¡¨å®‰è£…æˆåŠŸäº†ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/nginx_install.png\" alt=\"nginxå®‰è£…æˆåŠŸ\"></p>\n<h3 id=\"åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\"><a href=\"#åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\" class=\"headerlink\" title=\"åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\"></a>åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå</h3><p>åœ¨æ‹¥æœ‰äº†è‡ªå·±çš„æœåŠ¡å™¨ä»¥åï¼Œå°±å¯ä»¥åšå¾ˆå¤šäº‹æƒ…äº†ã€‚ä½†æ˜¯ç°åœ¨ä½ åªèƒ½é€šè¿‡IPåœ°å€è®¿é—®è‡ªå·±çš„æœåŠ¡å™¨ï¼Œçœ‹èµ·æ¥æ€»æ˜¯æœ‰ç‚¹åˆ«æ‰­ã€‚å¦å¤–ï¼Œå¦‚æœä½ æƒ³è¦ç½‘ç«™æœ‰ä¸€å®šçš„å½±å“åŠ›çš„è¯ï¼Œä»…æœ‰IPåœ°å€ä¼šè®©äººå‡ ä¹æ‰¾ä¸åˆ°ä½ çš„ç½‘ç«™ï¼Œè€Œä¸”ä¹Ÿä¸ç¬¦åˆå›½å®¶æ³•å¾‹è§„å®šã€‚æ‰€ä»¥è¿˜æ˜¯å»ºè®®å¤§å®¶å¼„ä¸€ä¸ªè‡ªå·±çš„åŸŸåã€‚</p>\n<p>ç°åœ¨å¸‚é¢ä¸Šçš„äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›åŸŸåæ³¨å†Œçš„æœåŠ¡ï¼Œç›´æ¥åœ¨ä½ çš„æœåŠ¡æä¾›å•†çš„å¹³å°ä¸Šé¢æ³¨å†Œå³å¯ã€‚ä¸‹é¢æˆ‘ç»§ç»­ç”¨åä¸ºäº‘çš„å¹³å°æ¼”ç¤ºã€‚</p>\n<p>é¦–å…ˆåœ¨åä¸ºäº‘ç½‘ç«™é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œåŸŸåâ€ï¼Œæ‰“å¼€ç¬¬ä¸€ä¸ªé“¾æ¥â€œåŸŸåæ³¨å†ŒæœåŠ¡â€ã€‚ä¹Ÿå¯ä»¥ç›´æ¥ç‚¹å‡»è¿™é‡Œï¼š<a href=\"https://www.huaweicloud.com/product/domain.html\">åŸŸåæ³¨å†ŒæœåŠ¡_åä¸ºäº‘</a>ã€‚</p>\n<p>ç„¶åä½ å¯ä»¥åœ¨ç½‘é¡µä¸­é€‰æ‹©ä½ çš„åŸŸåï¼Œå¸¸è§çš„å¦‚<code>.com</code>ï¼Œ<code>.cn</code>ï¼Œ<code>.net</code>ç­‰ã€‚è¿™äº›åŸŸåä¼šç›¸å¯¹æ¯”è¾ƒè´µã€‚ä½œä¸ºå­¦ç”Ÿå…šï¼Œæˆ‘é€‰æ‹©ä¸€ä¸ªæœ€ä¾¿å®œçš„åŸŸå<code>.top</code>ï¼Œåªéœ€è¦9å…ƒ/å¹´ã€‚</p>\n<p>ç‚¹å‡»ä½ æƒ³è¦çš„åŸŸååï¼Œä¼šè·³è½¬åˆ°ä¸€ä¸ªæ–°çš„é¡µé¢ã€‚æ¥ä¸‹æ¥å†æ¬¡é€‰æ‹©ä½ è¦çš„åŸŸåï¼Œå¹¶ä¸”åœ¨â€œæŸ¥åŸŸåâ€çš„æœç´¢æ¡†å†…è¾“å…¥ä½ æƒ³è¦çš„åŸŸåï¼Œçœ‹çœ‹æ˜¯å¦å·²ç»è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨äº†å°±æ¢ä¸€ä¸ªã€‚è‹¥æ˜¾ç¤ºâ€œåŸŸåå¯æ³¨å†Œâ€ï¼Œå°±ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_domain.png\" alt=\"åŸŸåè´­ä¹°\"></p>\n<p>è´­ä¹°å®Œæˆåï¼Œä½ å°±æ‹¥æœ‰äº†è‡ªå·±åŸŸåäº†ï¼</p>\n<h3 id=\"å¤‡æ¡ˆ\"><a href=\"#å¤‡æ¡ˆ\" class=\"headerlink\" title=\"å¤‡æ¡ˆ\"></a>å¤‡æ¡ˆ</h3><blockquote>\n<p>å¤‡æ¡ˆæ˜¯ä¸­å›½å¤§é™†çš„ä¸€é¡¹æ³•è§„ï¼Œä½¿ç”¨å¤§é™†èŠ‚ç‚¹æœåŠ¡å™¨æä¾›äº’è”ç½‘ä¿¡æ¯æœåŠ¡çš„ç”¨æˆ·ï¼Œéœ€è¦åœ¨æœåŠ¡å™¨æä¾›å•†å¤„æäº¤å¤‡æ¡ˆç”³è¯·ã€‚</p>\n<p>æ ¹æ®å·¥ä¿¡éƒ¨ã€Šäº’è”ç½‘ä¿¡æ¯æœåŠ¡ç®¡ç†åŠæ³•ã€‹(å›½åŠ¡é™¢292å·ä»¤)å’Œå·¥ä¿¡éƒ¨ä»¤ç¬¬33å·ã€Šéç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å¤‡æ¡ˆç®¡ç†åŠæ³•ã€‹è§„å®šï¼Œå›½å®¶å¯¹ç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œè®¸å¯åˆ¶åº¦ï¼Œå¯¹éç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œå¤‡æ¡ˆåˆ¶åº¦ã€‚æœªå–å¾—è®¸å¯æˆ–è€…æœªå±¥è¡Œå¤‡æ¡ˆæ‰‹ç»­çš„ï¼Œä¸å¾—ä»äº‹äº’è”ç½‘ä¿¡æ¯æœåŠ¡ï¼Œå¦åˆ™å±è¿æ³•è¡Œä¸ºã€‚é€šä¿—æ¥è®²ï¼Œè¦å¼€åŠç½‘ç«™å¿…é¡»å…ˆåŠç†ç½‘ç«™å¤‡æ¡ˆï¼Œå¤‡æ¡ˆæˆåŠŸå¹¶è·å–é€šä¿¡ç®¡ç†å±€ä¸‹å‘çš„ICPå¤‡æ¡ˆå·åæ‰èƒ½å¼€é€šè®¿é—®ã€‚<a href=\"https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\">^2</a></p>\n</blockquote>\n<p>è¿™ä¸€æ­¥ä¸å¤šè¯´äº†ï¼Œå…·ä½“æ­¥éª¤æ¯”è¾ƒç¹çï¼ŒèŠ±è´¹çš„æ—¶é—´ä¹Ÿæ¯”è¾ƒé•¿ï¼Œéœ€è¦ä¸€ä¸¤å‘¨ã€‚ç½‘ç«™ä¸Šæœ‰å¾ˆæ¸…æ™°çš„<a href=\"https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html\">æ“ä½œæ–¹æ³•</a>ï¼Œè¯·è‡ªè¡ŒæŸ¥é˜…ï¼Œæ ¹æ®æ­¥éª¤æ“ä½œå³å¯ã€‚éœ€è¦æ³¨æ„ä¸€ç‚¹çš„æ˜¯ï¼Œåœ¨å®¡æ ¸è¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ¥åˆ°æœåŠ¡æä¾›å•†æ‰“æ¥çš„ç”µè¯ï¼Œä¸è¦æ¼æ¥ã€‚</p>\n<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢çš„å¤‡æ¡ˆæ“ä½œæ˜¯åœ¨å·¥ä¿¡éƒ¨å¤‡æ¡ˆçš„ã€‚å®Œæˆäº†åœ¨å·¥ä¿¡éƒ¨çš„å¤‡æ¡ˆä»¥åè¿˜éœ€è¦å…¬å®‰å¤‡æ¡ˆã€‚å…·ä½“<a href=\"http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c\">æ“ä½œæ–¹æ³•</a>ä¹Ÿè¯·è‡ªè¡ŒæŸ¥é˜…ã€‚</p>\n<h3 id=\"åŸŸåè§£æ\"><a href=\"#åŸŸåè§£æ\" class=\"headerlink\" title=\"åŸŸåè§£æ\"></a>åŸŸåè§£æ</h3><p>åœ¨å®Œæˆä¸€ç³»åˆ—ç¹ççš„å¤‡æ¡ˆæµç¨‹ä»¥åï¼Œä½ çš„ç½‘ç«™è¿˜ä¸å¯ä»¥é€šè¿‡åŸŸåè®¿é—®ã€‚åªæœ‰æŠŠä½ çš„åŸŸåè·ŸæœåŠ¡å™¨çš„IPåœ°å€ç»‘å®šåœ¨ä¸€èµ·ä¹‹åï¼Œå¹¶ä¸”åœ¨æœåŠ¡å™¨ä¸Šä¿®æ”¹äº†é…ç½®æ–‡ä»¶ä¹‹åæ‰å¯ä»¥ã€‚</p>\n<p>é¦–å…ˆæ‰“å¼€ç®¡ç†æ§åˆ¶å°ï¼Œåœ¨æ§åˆ¶å°ä¸­é€‰æ‹©â€œåŸŸåæ³¨å†Œâ€ã€‚ç„¶ååœ¨ä¸‹é¢çš„é¡µé¢ä¸­ç‚¹å‡»â€œè§£æâ€ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/domain.png\" alt=\"åŸŸåæ³¨å†Œ\"></p>\n<p>ç‚¹å‡»ä½ çš„åŸŸåï¼Œæ˜¾ç¤ºå¦‚ä¸‹é¡µé¢ã€‚è¿™é‡Œæ˜¾ç¤ºçš„æ˜¯ä½ åŸŸåçš„è®°å½•é›†ï¼Œå‰ä¸¤ä¸ªè®°å½•é›†åº”è¯¥æ˜¯é¢„ç½®è®¾ç½®ï¼Œä¸å¯æš‚åœæœåŠ¡ã€‚<span id=\"1\">ä½ å¯ä»¥åœ¨è¿™åŸºç¡€ä¸Šæ·»åŠ è‡ªå·±çš„è®°å½•é›†ã€‚</span></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/record.png\" alt=\"è®°å½•é›†\"></p>\n<p>ç‚¹å‡»é¡µé¢å³ä¸Šè§’çº¢è‰²æŒ‰é’®ä»¥æ·»åŠ è®°å½•é›†ã€‚æ·»åŠ è®°å½•é›†çš„é…ç½®å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä¸‹å›¾ä¸­ç»™å‡ºçš„ä¾‹å­æ˜¯æ·»åŠ çš„â€œAâ€å‹è®°å½•é›†ï¼Œä¹Ÿå³é€šè¿‡<code>example.com</code>è®¿é—®ç½‘ç«™ã€‚è‹¥éœ€è¦é€šè¿‡<code>www.example.com</code>è®¿é—®ç½‘ç«™ï¼Œåˆ™éœ€è¦ä¸º<code>example.com</code>çš„å­åŸŸåæ·»åŠ â€œAâ€å‹è®°å½•é›†ã€‚å…·ä½“é…ç½®å‚è§ï¼š<a href=\"https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1\">é…ç½®ç½‘ç«™è§£æ_åä¸ºäº‘</a>ã€‚ç‚¹å‡»â€œç¡®å®šâ€ï¼Œå®Œæˆæ·»åŠ ã€‚ä½ å¯ä»¥é€šè¿‡<code>ping ä½ çš„åŸŸå</code>æ¥æµ‹è¯•ä½ æ·»åŠ çš„è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆäº†ã€‚</p>\n<p><img src=\"https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png\" alt=\"æ·»åŠ è®°å½•é›†\"></p>\n<h3 id=\"é…ç½®nginx\"><a href=\"#é…ç½®nginx\" class=\"headerlink\" title=\"é…ç½®nginx\"></a>é…ç½®nginx</h3><p><span id=\"2\">æ‰“å¼€</span>ä½ ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œè¾“å…¥å‘½ä»¤ï¼š<code>ssh ä½ çš„IPåœ°å€</code>ï¼Œè¾“å…¥ä½ çš„æœåŠ¡å™¨çš„å¯†ç ã€‚</p>\n<p>è¿›å…¥ä½ çš„nginxçš„å®‰è£…ç›®å½•ï¼š<code>cd /usr/local/nginx/</code>ã€‚</p>\n<p>ä½¿ç”¨vimæ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ï¼š<code>vim ./conf/nginx.conf</code>ã€‚</p>\n<p>æŒ‰<code>I</code>å¼€å§‹è¾“å…¥ã€‚</p>\n<p>åœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">\t    <span class=\"attribute\">listen</span>   <span class=\"number\">80</span>; <span class=\"comment\">#ç›‘å¬ç«¯å£è®¾ä¸º 80</span></span><br><span class=\"line\">\t    <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#ç»‘å®šæ‚¨çš„åŸŸå</span></span><br><span class=\"line\">\t    <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#æŒ‡å®šé»˜è®¤æ–‡ä»¶</span></span><br><span class=\"line\">\t    <span class=\"attribute\">root</span> html; <span class=\"comment\">#æŒ‡å®šç½‘ç«™æ ¹ç›®å½•</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>ç„¶åæŒ‰<code>esc</code>é€€å‡ºç¼–è¾‘ï¼Œå†æŒ‰<code>Shift+zz</code>ä¿å­˜ã€‚</p>\n<p>è¾“å…¥ï¼š<code>cd ./sbin</code>ï¼Œåˆ‡æ¢æ–‡ä»¶å¤¹ã€‚</p>\n<p>æ‰§è¡Œå‘½ä»¤ï¼š<code>nginx -s relod</code>ï¼Œé‡å¯nginxæœåŠ¡ã€‚</p>\n<p>è¿™æ—¶å€™å†å°è¯•ç”¨æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œåº”è¯¥ä¼šæ˜¾ç¤ºä¹‹å‰å‡ºç°è¿‡çš„â€œWelcome to nginx â€çš„é¡µé¢äº†ï¼</p>\n<h3 id=\"ç”³è¯·SSLè¯ä¹¦\"><a href=\"#ç”³è¯·SSLè¯ä¹¦\" class=\"headerlink\" title=\"ç”³è¯·SSLè¯ä¹¦\"></a>ç”³è¯·SSLè¯ä¹¦</h3><p>SSLè¯ä¹¦å¯ä»¥åœ¨æ•°æ®ä¼ è¾“çš„è¿‡ç¨‹ä¸­å¯¹å…¶è¿›è¡ŒåŠ å¯†å’Œéšè—ï¼Œå¯ä»¥æå¤§åœ°æé«˜æ•°æ®ä¼ è¾“çš„å®‰å…¨æ€§ã€‚æ‹¥æœ‰SSLè¯ä¹¦çš„ç½‘ç«™çš„è¯·æ±‚å¤´éƒ½æ˜¯<code>https</code>ï¼Œå¹¶ä¸”åœ¨é“¾æ¥æ—è¾¹ä¼šå‡ºç°ä¸€æŠŠå°é”ã€‚ä½†æ˜¯ï¼ŒSSLè¯ä¹¦å¹¶ä¸æ˜¯æ‰€æœ‰ç½‘ç«™éƒ½å¿…é¡»çš„ï¼Œè¿™è§†ä½ çš„éœ€è¦è€Œå®šã€‚æ¯”å¦‚ï¼Œå¾®ä¿¡å°ç¨‹åºçš„æœåŠ¡å™¨å°±å¿…é¡»è¦æœ‰åŸŸåå’ŒSSLè¯ä¹¦ã€‚å¦å¤–ï¼Œå‡ºäºä¿¡æ¯ä¼ è¾“çš„å®‰å…¨æ€§æ–¹é¢çš„è€ƒè™‘ï¼Œæœ‰SSLè¯ä¹¦è¿˜æ˜¯æ˜¾å¾—æ›´ä¸ºå¦¥å½“å’Œä¸“ä¸šä¸€ç‚¹ã€‚</p>\n<p>ç°åœ¨å¸‚é¢ä¸Šå„å¤§äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›é…å¥—çš„SSLè¯ä¹¦ç”³è¯·æœåŠ¡ï¼Œä¸€èˆ¬éƒ½æ˜¯æä¾›ä¼ä¸šçº§çš„è¯ä¹¦ï¼Œä»·æ ¼æ¯”è¾ƒæ˜‚è´µã€‚ä½†æ˜¯åŒæ—¶ç½‘ç»œä¸Šä¹Ÿæœ‰ä¸€äº›å…è´¹çš„SSLè¯ä¹¦æœåŠ¡å¯ä»¥é€‰æ‹©ã€‚ä¸‹é¢è¿˜æ˜¯ä»¥åä¸ºäº‘çš„å¹³å°ä¸ºä¾‹ï¼Œç®€å•è¯´æ˜ä¸€ä¸‹å¦‚ä½•ç”³è¯·SSLè¯ä¹¦ã€‚</p>\n<p>é¦–å…ˆåœ¨åä¸ºäº‘é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œå…è´¹è¯ä¹¦â€œï¼Œç„¶åç‚¹å‡»<a href=\"https://marketplace.huaweicloud.com/product/00301-315148-0--0\">äºšæ´²è¯šä¿¡åŸŸåå‹DVå•åŸŸåSSLè¯ä¹¦â€“å…è´¹è¯ä¹¦</a>ï¼Œå¯ä»¥çœ‹åˆ°è¯ä¹¦çš„ä»·æ ¼æ˜¯0.00å…ƒã€‚ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_ssl.png\" alt=\"è´­ä¹°SSLè¯ä¹¦\"></p>\n<p>å®Œæˆè´­ä¹°åè¯·ä¸è¦ç«‹å³å…³é—­é¡µé¢ï¼Œé¡µé¢ä¸­çš„è®¢å•å·åœ¨ä¹‹åè¿˜éœ€è¦ç”¨åˆ°ã€‚å°”åï¼Œç³»ç»Ÿä¼šå‘é€â€HuaweiCloudè´¦æˆ·ç”³è¯·â€é‚®ä»¶è‡³ç”¨æˆ·é‚®ç®±ï¼Œå³ä½ åœ¨åä¸ºäº‘çš„æ³¨å†Œé‚®ç®±ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/request_account.png\" alt=\"HuaweiCloudè´¦æˆ·ç”³è¯·\"></p>\n<p>ç‚¹å‡»é‚®ä»¶ä¸­çš„ç™»å½•åœ°å€è¿›å…¥ç³»ç»Ÿï¼Œå¹¶ä½¿ç”¨é‚®ä»¶æä¾›çš„è´¦å·å’Œåˆå§‹å¯†ç è¿›è¡Œç™»å½•ã€‚ç™»å…¥ç³»ç»Ÿåè¯·ä¿®æ”¹ä½ çš„åˆå§‹å¯†ç ï¼Œç„¶åè¯·æ ¹æ®åä¸ºäº‘ä¸­ç»™ä½ æä¾›çš„è®¢å•å·åœ¨è¯¥ç³»ç»Ÿä¸­æŸ¥è¯¢ä½ çš„è®¢å•ã€‚æŸ¥è¯¢åˆ°ä½ çš„è®¢å•ä»¥åï¼Œéœ€è¦ä½ è¡¥å……ä¸€äº›ä¿¡æ¯ï¼Œè¯·å¦‚å®å¡«å†™ã€‚ç³»ç»Ÿä¼šè¦ä½ å¡«å†™å…¬å¸ä¿¡æ¯ï¼Œå¦‚æœåªæ˜¯ä¸ªäººç½‘ç«™ï¼Œé‚£ä¹ˆå…¬å¸åç§°ç›´æ¥å¡«å†™ä½ çš„åå­—å³å¯ï¼Œå…¬å¸åœ°å€å°±å¡«å†™ä½ çš„ä½å€ã€‚</p>\n<p>å¡«å†™å®Œæˆåä¼šè¿›å…¥å®¡æ ¸é˜¶æ®µï¼Œç³»ç»Ÿä¼šç»™ä½ å‘é€ä¸€å°é‚®ä»¶ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/check.png\" alt=\"è¯ä¹¦å®¡æ ¸\"></p>\n<p>æ ¹æ®é‚®ä»¶çš„æç¤ºï¼Œéœ€è¦åœ¨è®°å½•é›†ä¸­æ·»åŠ æ–°çš„å†…å®¹ã€‚è¯·æ ¹æ®<a href=\"#1\">å‰æ–‡</a>æ‰€è¿°æ–¹æ³•ï¼Œå°†é‚®ä»¶ä¸­çš„å†…å®¹æ·»åŠ è‡³æ–°çš„è®°å½•é›†ã€‚å¡«å†™æ–¹æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/modify_record.png\" alt=\"å¡«å†™è®°å½•é›†\"></p>\n<p>å¡«å†™å®Œæˆåï¼Œå¯ä»¥åœ¨æœ¬åœ°ç”µè„‘çš„ç»ˆç«¯é‡Œè¾“å…¥<code>nslookup -querytype=txt ä½ çš„åŸŸå</code>æ¥æµ‹è¯•è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/test_record.png\" alt=\"æµ‹è¯•è®°å½•é›†\"></p>\n<p>ä¸€èˆ¬æ¥è¯´ï¼Œè®°å½•é›†ç”Ÿæ•ˆå10åˆ†é’Ÿä»¥å†…è¯ä¹¦å°±ä¼šé¢å‘äº†ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/issue.png\" alt=\"è¯ä¹¦é¢å‘\"></p>\n<h3 id=\"SSLè¯ä¹¦éƒ¨ç½²\"><a href=\"#SSLè¯ä¹¦éƒ¨ç½²\" class=\"headerlink\" title=\"SSLè¯ä¹¦éƒ¨ç½²\"></a>SSLè¯ä¹¦éƒ¨ç½²</h3><p>æ¥ä¸‹æ¥æˆ‘ä»¬è¦æŠŠSSLè¯ä¹¦éƒ¨ç½²åˆ°æˆ‘ä»¬çš„æœåŠ¡å™¨ä¸Šã€‚</p>\n<p>åœ¨æ”¶åˆ°çš„â€œè¯ä¹¦é¢å‘â€çš„é‚®ä»¶çš„åº•éƒ¨æœ‰ä¸€æ¡é“¾æ¥ï¼Œç‚¹å‡»è¿™æ¡é“¾æ¥ï¼Œè¿›å…¥è¯ä¹¦ç®¡ç†ç³»ç»Ÿã€‚ç™»å½•ç³»ç»Ÿï¼Œåœ¨å·¦ä¾§å¯¼èˆªæ ä¸­ç‚¹å‡»â€œSSLè¯ä¹¦â€ï¼Œå†ç‚¹å‡»â€œé¢„è§ˆâ€ï¼Œå†åœ¨å³ä¾§çš„â€œä¿¡æ¯é¢„è§ˆâ€ä¸­ç‚¹å‡»â€œä¸‹è½½æœ€æ–°è¯ä¹¦â€œã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert.png\" alt=\"ä¸‹è½½è¯ä¹¦\"></p>\n<p>åœ¨å¼¹å‡ºçš„å¯¹è¯æ¡†å†…ï¼Œé€‰æ‹©è¯ä¹¦æ ¼å¼ä¸ºâ€œPEM(é€‚ç”¨äºNginx,SLB)â€ï¼Œè¾“å…¥ä½ çš„è®¢å•å¯†ç ã€‚è¯ä¹¦å¯†ç å¯ä»¥ç•™ç©ºã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert1.png\" alt=\"ä¸‹è½½è¯ä¹¦\"></p>\n<p>ä¸‹è½½å®Œæˆåï¼Œè§£å‹ä¸‹è½½çš„å‹ç¼©åŒ…ï¼Œéœ€è¦è¾“å…¥ä½ çš„è®¢å•å¯†ç ï¼ˆå¦‚æœä½ æ²¡æœ‰è®¾ç½®è¯ä¹¦å¯†ç ï¼‰ã€‚è§£å‹ä»¥åå¯ä»¥å¾—åˆ°ä¸‹å›¾ä¸¤ä¸ªæ–‡ä»¶ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/unzip_cert.png\" alt=\"è§£å‹ç¼©\"></p>\n<p>æ¥ä¸‹æ¥ï¼Œæ‰“å¼€ä½ çš„ç»ˆç«¯ï¼ŒæŒ‰é¡ºåºè¾“å…¥ä¸‹åˆ—å‘½ä»¤ï¼š</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh ä½ çš„å…¬ç½‘IP #sshç™»å½•ï¼Œè¾“å…¥ä½ çš„å¯†ç </span><br><span class=\"line\">cd /usr/local/nginx #åˆ‡æ¢åˆ°nginxçš„å®‰è£…ç›®å½•</span><br><span class=\"line\">mkdir ./cert #åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹certç”¨äºå­˜æ”¾ä½ çš„è¯ä¹¦</span><br><span class=\"line\">exit #æ–­å¼€ä¸æœåŠ¡å™¨çš„è¿æ¥</span><br><span class=\"line\">scp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.key ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.keyæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹</span><br><span class=\"line\">scp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.crt ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.crtæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹</span><br></pre></td></tr></table></figure>\n\n<p>æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä¿®æ”¹nginxçš„é…ç½®æ–‡ä»¶ã€‚å‚è€ƒ<a href=\"#2\">å‰æ–‡</a>æ‰€è¿°æ–¹æ³•æ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ã€‚å…ˆå°†ä½ ä¹‹å‰æ’å…¥çš„å†…å®¹åˆ é™¤æˆ–è€…ä½¿ç”¨<code>#</code>æ³¨é‡Šæ‰ï¼Œç„¶ååœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">         <span class=\"attribute\">listen</span>       <span class=\"number\">443</span> ssl;</span><br><span class=\"line\">         <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#ä½ è¯ä¹¦ç»‘å®šçš„åŸŸå;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate</span>      /usr/local/nginx/cert/ä½ çš„åŸŸå.crt;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate_key</span>  /usr/local/nginx/cert/ä½ çš„åŸŸå.key;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_cache</span>    shared:SSL:<span class=\"number\">1m</span>;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_timeout</span>  <span class=\"number\">5m</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_ciphers</span>  HIGH:!aNULL:!MD5;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_prefer_server_ciphers</span>  <span class=\"literal\">on</span>;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"attribute\">location</span> / &#123;</span><br><span class=\"line\">            <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#æŒ‡å®šé»˜è®¤æ–‡ä»¶ã€‚</span></span><br><span class=\"line\">\t    \t\t\t<span class=\"attribute\">root</span> html; <span class=\"comment\">#æŒ‡å®šç½‘ç«™æ ¹ç›®å½•ã€‚</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"section\">server</span> &#123; <span class=\"comment\">#å°†ä½ çš„80ç«¯å£é‡å®šå‘è‡³433ç«¯å£ï¼Œå³å¼ºåˆ¶ä½¿ç”¨httpsè®¿é—®</span></span><br><span class=\"line\">  \t\t\t<span class=\"attribute\">listen</span> <span class=\"number\">80</span>;</span><br><span class=\"line\">  \t\t\tserver_name; example.com; #ä½ çš„åŸŸå</span><br><span class=\"line\">\t\t\t\t<span class=\"attribute\">rewrite</span><span class=\"regexp\"> ^/(.*)$</span> https://example.com:443/<span class=\"variable\">$1</span> <span class=\"literal\">permanent</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>å°†æ–‡ä»¶ä¿å­˜ä»¥åé‡å¯nginxæœåŠ¡ã€‚</p>\n<p>é‡å¯ä»¥åä½ å¯èƒ½ä¼šé‡åˆ°è¿™æ ·çš„é—®é¢˜ï¼š<code>**unknown directive â€œsslâ€ in /usr/local/nginx/conf/nginx.conf:121**</code>ï¼Œè¿™æ˜¯å› ä¸ºä½ åœ¨å®‰è£…nginxæ—¶ï¼Œæ²¡æœ‰ç¼–è¯‘SSLæ¨¡å—ã€‚ä½ å¯ä»¥åœ¨ç»ˆç«¯é‡ŒæŒ‰ç…§ä¸‹è¿°æ­¥éª¤è§£å†³<a href=\"https://blog.csdn.net/qq_26369317/article/details/102863613\">^ 3</a>ï¼š</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ../nginx-1.16.1 #è¿›å…¥åˆ°nginxçš„æºç åŒ…çš„ç›®å½•ä¸‹</span><br><span class=\"line\">./configure --with-http_ssl_module #å¸¦å‚æ•°æ‰§è¡Œç¨‹åº</span><br><span class=\"line\">make #ç¼–è¯‘</span><br><span class=\"line\">cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #å¤‡ä»½æ—§çš„nginx</span><br><span class=\"line\">cp ./objs/nginx /usr/local/nginx/sbin/ #ç„¶åå°†æ–°çš„nginxçš„ç¨‹åºå¤åˆ¶ä¸€ä»½</span><br><span class=\"line\">cd /usr/local/nginx/sbin/ #åˆ‡æ¢åˆ°sbinç›®å½•</span><br><span class=\"line\">./nginx -s reload #é‡å¯nginxæœåŠ¡</span><br></pre></td></tr></table></figure>\n\n<p>å¦‚æœé‡å¯æˆåŠŸçš„è¯ï¼Œæ‰“å¼€æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œè¿™æ—¶å€™åº”è¯¥å¯ä»¥åœ¨é“¾æ¥æ—è¾¹çœ‹åˆ°ä¸€ä¸ªå°é”äº†ï¼</p>\n","site":{"data":{}},"more":"<blockquote>\n<p>ç”±äºè‡ªå·±æ˜¯å»å¹´ä¸ƒæœˆé…ç½®å¥½çš„æœåŠ¡å™¨ï¼Œæœ‰ä¸€äº›ç»†èŠ‚æˆ–è€…é‡åˆ°çš„é—®é¢˜å·²ç»è®°ä¸å¤ªæ¸…ï¼Œæ•…æœ¬æ–‡å¯èƒ½ä¼šæœ‰ä¸å®Œæ•´çš„åœ°æ–¹ï¼Œé‡åˆ°é—®é¢˜è¯·å–„ç”¨æœç´¢å¼•æ“ï¼Œè€Œä¸”æœåŠ¡å™¨çš„é…ç½®æ–¹æ³•ä¹Ÿä¸åªæœ‰è¿™ä¸€ç§ã€‚æœ¬æ–‡ä¸»è¦ç”¨ä½œå¯¹è‡ªå·±æ“ä½œæ­¥éª¤å’Œæ–¹æ³•çš„ä¸€ä¸ªæ€»ç»“ï¼Œä»¥ä¾¿äºæ—¥åæŸ¥é˜…ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚</p>\n</blockquote>\n<h3 id=\"è´­ä¹°æœåŠ¡å™¨\"><a href=\"#è´­ä¹°æœåŠ¡å™¨\" class=\"headerlink\" title=\"è´­ä¹°æœåŠ¡å™¨\"></a>è´­ä¹°æœåŠ¡å™¨</h3><p>é¦–å…ˆå»<a href=\"https://www.huaweicloud.com/?locale=zh-cn\">åä¸ºäº‘å®˜ç½‘</a>æ³¨å†Œä¸€ä¸ªè´¦å·ã€‚å¦‚æœæ˜¯å­¦ç”Ÿï¼Œå¯ä»¥æœç´¢â€œå­¦ç”Ÿâ€ï¼Œå¹¶è¿›è¡Œå­¦ç”Ÿè®¤è¯ã€‚å­¦ç”Ÿè®¤è¯çš„æ­¥éª¤å‚è§<a href=\"https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html\">å­¦ç”Ÿè®¤è¯æµç¨‹</a>ã€‚è¿›è¡Œèº«ä»½éªŒè¯åå¯ä»¥è´­ä¹°å­¦ç”Ÿä¼˜æƒ å¥—é¤ï¼Œäº‘æœåŠ¡å™¨ä»·æ ¼åªè¦99å…ƒ/å¹´ï¼Œæ¯”é˜¿é‡Œäº‘å’Œè…¾è®¯äº‘çš„éƒ½è¦ä¾¿å®œä¸€äº›ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hwcloud_discount.png\" alt=\"åä¸ºäº‘å­¦ç”Ÿä¼˜æƒ \"></p>\n<p>è´­ä¹°å®Œæˆåï¼Œä½ å¯ä»¥åœ¨æ§åˆ¶å°çœ‹åˆ°è‡ªå·±ç°æœ‰çš„èµ„æºä»¥åŠè¿è¡Œæƒ…å†µã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/console.png\" alt=\"æ§åˆ¶å°\"></p>\n<h3 id=\"é…ç½®å®‰å…¨ç»„\"><a href=\"#é…ç½®å®‰å…¨ç»„\" class=\"headerlink\" title=\"é…ç½®å®‰å…¨ç»„\"></a>é…ç½®å®‰å…¨ç»„</h3><blockquote>\n<p>å®‰å…¨ç»„æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„åˆ†ç»„ï¼Œä¸ºå…·æœ‰ç›¸åŒå®‰å…¨ä¿æŠ¤éœ€æ±‚å¹¶ç›¸äº’ä¿¡ä»»çš„äº‘æœåŠ¡å™¨æä¾›è®¿é—®ç­–ç•¥ã€‚å®‰å…¨ç»„åˆ›å»ºåï¼Œç”¨æˆ·å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­å®šä¹‰å„ç§è®¿é—®è§„åˆ™ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚</p>\n<p>ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªç”¨æˆ·é»˜è®¤åˆ›å»ºä¸€ä¸ªé»˜è®¤å®‰å…¨ç»„ï¼Œé»˜è®¤å®‰å…¨ç»„çš„è§„åˆ™æ˜¯åœ¨å‡ºæ–¹å‘ä¸Šçš„æ•°æ®æŠ¥æ–‡å…¨éƒ¨æ”¾è¡Œï¼Œå…¥æ–¹å‘è®¿é—®å—é™ï¼Œå®‰å…¨ç»„å†…çš„äº‘æœåŠ¡å™¨æ— éœ€æ·»åŠ è§„åˆ™å³å¯äº’ç›¸è®¿é—®ã€‚é»˜è®¤å®‰å…¨ç»„å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚</p>\n<p>å®‰å…¨ç»„åˆ›å»ºåï¼Œä½ å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­è®¾ç½®å‡ºæ–¹å‘ã€å…¥æ–¹å‘è§„åˆ™ï¼Œè¿™äº›è§„åˆ™ä¼šå¯¹å®‰å…¨ç»„å†…éƒ¨çš„äº‘æœåŠ¡å™¨å‡ºå…¥æ–¹å‘ç½‘ç»œæµé‡è¿›è¡Œè®¿é—®æ§åˆ¶ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚<a href=\"https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\">^1</a></p>\n</blockquote>\n<p>åœ¨æ§åˆ¶å°ç‚¹å‡»â€œå¼¹æ€§äº‘æœåŠ¡å™¨ECSâ€ï¼Œåœ¨è¿™é‡Œä½ å¯çœ‹åˆ°ä½ çš„æœåŠ¡å™¨çš„å…¬ç½‘IPï¼Œè¯·è®°ä¸‹è¿™ä¸ªIPåœ°å€ã€‚ç„¶åç‚¹å‡»åœ¨åˆ—è¡¨ä¸­ç‚¹å‡»ä½ çš„æœåŠ¡å™¨çš„åç§°ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/security_groups.png\" alt=\"é€‰æ‹©æœåŠ¡å™¨\"></p>\n<p>è¿›å…¥äº‘æœåŠ¡å™¨ç®¡ç†é¡µé¢åï¼Œç‚¹å‡»â€œå®‰å…¨ç»„â€ã€‚å†ç‚¹å‡»â€œSys-defaultâ€å¯ä»¥çœ‹åˆ°é»˜è®¤å®‰å…¨ç»„ã€‚ç„¶åä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯æˆ‘ç›®å‰çš„å®‰å…¨ç»„è®¾ç½®ï¼Œä»…ä¾›å‚è€ƒã€‚é€‰æ‹©â€œå…¥/å‡ºæ–¹å‘æ–¹å‘è§„åˆ™â€ï¼Œå†ç‚¹å‡»â€œæ·»åŠ è§„åˆ™â€œå³å¯æ‰‹åŠ¨æ·»åŠ è§„åˆ™ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œé…ç½®çš„éƒ½æ˜¯å…¥æ–¹å‘çš„å®‰å…¨ç»„ï¼Œå¹¶ä¸”æºåœ°å€ï¼ˆè®¿é—®æœåŠ¡å™¨çš„è®¾å¤‡çš„IPåœ°å€ï¼‰éƒ½ä¸ºâ€œ0.0.0.0/0â€ï¼ˆæ‰€æœ‰IPåœ°å€ï¼‰ã€‚</p>\n<p>é€šå¸¸éœ€è¦é…ç½®å¦‚ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š</p>\n<ul>\n<li>SSHè¿œç¨‹è¿æ¥Linuxå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šSSHï¼Œç«¯å£ï¼š22ï¼‰</li>\n<li>å…¬ç½‘â€œpingâ€ECSå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šICMPï¼Œç«¯å£ï¼šå…¨éƒ¨ï¼‰</li>\n<li>å¼¹æ€§äº‘æœåŠ¡å™¨ä½œWebæœåŠ¡å™¨<ul>\n<li>åè®®ï¼šhttpï¼Œç«¯å£ï¼š80</li>\n<li>åè®®ï¼šhttpsï¼Œç«¯å£ï¼š433</li>\n</ul>\n</li>\n</ul>\n<p>è¯¦ç»†é…ç½®è¯·å‚è€ƒ<a href=\"https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html\">å®‰å…¨ç»„é…ç½®ç¤ºä¾‹</a>ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings.png\" alt=\"å®‰å…¨ç»„è®¾ç½®\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings1.png\" alt=\"å®‰å…¨ç»„è®¾ç½®\"></p>\n<p>é…ç½®å®Œæˆåï¼Œå¯ä»¥æ‰“å¼€ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œç”¨ä¸‹é¢çš„è¯­å¥æµ‹è¯•ä¸€ä¸‹ï¼š</p>\n<p><code>ping ä½ çš„å…¬ç½‘IP</code></p>\n<p>å‡ºç°ç±»ä¼¼ä¸‹é¢çš„å†…å®¹å°±ä»£è¡¨æˆåŠŸäº†ï¼š</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ping_test.png\" alt=\"pingæµ‹è¯•\"></p>\n<p>ä½ å¯ä»¥æŒ‰ä¸‹<code>Ctrl+C</code>æ¥ç»“æŸ<code>ping</code>è¿™ä¸ªè¿›ç¨‹ã€‚</p>\n<p>ç„¶ååœ¨ç»ˆç«¯é‡Œè¾“å…¥ï¼š</p>\n<p><code>ssh ä½ çš„å…¬ç½‘IP</code></p>\n<p>å¦‚æœä½ çš„å®‰å…¨ç»„é…ç½®æ­£ç¡®çš„è¯ï¼Œä¼šè®©ä½ è¾“å…¥æœåŠ¡å™¨çš„ç™»å½•å¯†ç ã€‚è¾“å…¥å¯†ç ï¼ˆæ³¨æ„ï¼šå¯†ç æ˜¯ä¸ä¼šæ˜¾ç¤ºçš„ï¼‰åå›è½¦ï¼Œåº”è¯¥å¯ä»¥çœ‹åˆ°è¿™æ ·çš„è¾“å‡ºï¼š</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ssh_login.png\" alt=\"sshç™»å½•\"></p>\n<p>è¿™ä¸ªæ—¶å€™ï¼Œä½ çš„ç»ˆç«¯å°±å·²ç»è¿æ¥ä¸Šäº†æœåŠ¡å™¨çš„ç³»ç»Ÿäº†ï¼Œä½ åœ¨ç»ˆç«¯é‡Œçš„ä¸€åˆ‡æ“ä½œéƒ½æ˜¯ä½œç”¨åœ¨æœåŠ¡å™¨ä¸Šçš„ã€‚</p>\n<h3 id=\"åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\"><a href=\"#åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\" class=\"headerlink\" title=\"åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\"></a>åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx</h3><p>é¦–å…ˆè¯·åœ¨ç»ˆç«¯ä½¿ç”¨sshç™»å½•ä½ çš„æœåŠ¡å™¨ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢ç»™å‡ºçš„é¡ºåºè¾“å…¥å‘½ä»¤ã€‚</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #å®‰è£…ç¼–è¯‘å·¥å…·åŠåº“æ–‡ä»¶</span><br><span class=\"line\">cd /usr/local/ #åˆ‡æ¢åˆ°ç›®æ ‡å®‰è£…æ–‡ä»¶å¤¹</span><br><span class=\"line\">wget http://nginx.org/download/nginx-1.16.1.tar.gz #ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„Nginx</span><br><span class=\"line\">tar -zxvf nginx-1.16.1.tar.gz #è§£å‹æ–‡ä»¶</span><br><span class=\"line\">cd nginx-1.16.1 #è¿›å…¥è§£å‹çš„æ–‡ä»¶å¤¹</span><br><span class=\"line\">./configure #æ‰§è¡Œç¨‹åº</span><br><span class=\"line\">make #ç¼–è¯‘</span><br><span class=\"line\">make install #å®‰è£…</span><br><span class=\"line\">cd /usr/local/nginx/sbin #è¿›å…¥Nginxå®‰è£…ç›®å½•</span><br><span class=\"line\">./nginx #è¿è¡ŒNginx</span><br></pre></td></tr></table></figure>\n\n<p>æ­¤æ—¶ï¼Œå®‰è£…åº”è¯¥å·²ç»å®Œæˆäº†ã€‚æ‰“å¼€æµè§ˆå™¨ï¼Œåœ¨åœ°å€æ ä¸­è¾“å…¥ä½ çš„å…¬ç½‘ipã€‚å¦‚æœçœ‹åˆ°ä¸‹å›¾æ‰€ç¤ºå†…å®¹ï¼Œå°±ä»£è¡¨å®‰è£…æˆåŠŸäº†ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/nginx_install.png\" alt=\"nginxå®‰è£…æˆåŠŸ\"></p>\n<h3 id=\"åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\"><a href=\"#åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\" class=\"headerlink\" title=\"åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\"></a>åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå</h3><p>åœ¨æ‹¥æœ‰äº†è‡ªå·±çš„æœåŠ¡å™¨ä»¥åï¼Œå°±å¯ä»¥åšå¾ˆå¤šäº‹æƒ…äº†ã€‚ä½†æ˜¯ç°åœ¨ä½ åªèƒ½é€šè¿‡IPåœ°å€è®¿é—®è‡ªå·±çš„æœåŠ¡å™¨ï¼Œçœ‹èµ·æ¥æ€»æ˜¯æœ‰ç‚¹åˆ«æ‰­ã€‚å¦å¤–ï¼Œå¦‚æœä½ æƒ³è¦ç½‘ç«™æœ‰ä¸€å®šçš„å½±å“åŠ›çš„è¯ï¼Œä»…æœ‰IPåœ°å€ä¼šè®©äººå‡ ä¹æ‰¾ä¸åˆ°ä½ çš„ç½‘ç«™ï¼Œè€Œä¸”ä¹Ÿä¸ç¬¦åˆå›½å®¶æ³•å¾‹è§„å®šã€‚æ‰€ä»¥è¿˜æ˜¯å»ºè®®å¤§å®¶å¼„ä¸€ä¸ªè‡ªå·±çš„åŸŸåã€‚</p>\n<p>ç°åœ¨å¸‚é¢ä¸Šçš„äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›åŸŸåæ³¨å†Œçš„æœåŠ¡ï¼Œç›´æ¥åœ¨ä½ çš„æœåŠ¡æä¾›å•†çš„å¹³å°ä¸Šé¢æ³¨å†Œå³å¯ã€‚ä¸‹é¢æˆ‘ç»§ç»­ç”¨åä¸ºäº‘çš„å¹³å°æ¼”ç¤ºã€‚</p>\n<p>é¦–å…ˆåœ¨åä¸ºäº‘ç½‘ç«™é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œåŸŸåâ€ï¼Œæ‰“å¼€ç¬¬ä¸€ä¸ªé“¾æ¥â€œåŸŸåæ³¨å†ŒæœåŠ¡â€ã€‚ä¹Ÿå¯ä»¥ç›´æ¥ç‚¹å‡»è¿™é‡Œï¼š<a href=\"https://www.huaweicloud.com/product/domain.html\">åŸŸåæ³¨å†ŒæœåŠ¡_åä¸ºäº‘</a>ã€‚</p>\n<p>ç„¶åä½ å¯ä»¥åœ¨ç½‘é¡µä¸­é€‰æ‹©ä½ çš„åŸŸåï¼Œå¸¸è§çš„å¦‚<code>.com</code>ï¼Œ<code>.cn</code>ï¼Œ<code>.net</code>ç­‰ã€‚è¿™äº›åŸŸåä¼šç›¸å¯¹æ¯”è¾ƒè´µã€‚ä½œä¸ºå­¦ç”Ÿå…šï¼Œæˆ‘é€‰æ‹©ä¸€ä¸ªæœ€ä¾¿å®œçš„åŸŸå<code>.top</code>ï¼Œåªéœ€è¦9å…ƒ/å¹´ã€‚</p>\n<p>ç‚¹å‡»ä½ æƒ³è¦çš„åŸŸååï¼Œä¼šè·³è½¬åˆ°ä¸€ä¸ªæ–°çš„é¡µé¢ã€‚æ¥ä¸‹æ¥å†æ¬¡é€‰æ‹©ä½ è¦çš„åŸŸåï¼Œå¹¶ä¸”åœ¨â€œæŸ¥åŸŸåâ€çš„æœç´¢æ¡†å†…è¾“å…¥ä½ æƒ³è¦çš„åŸŸåï¼Œçœ‹çœ‹æ˜¯å¦å·²ç»è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨äº†å°±æ¢ä¸€ä¸ªã€‚è‹¥æ˜¾ç¤ºâ€œåŸŸåå¯æ³¨å†Œâ€ï¼Œå°±ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_domain.png\" alt=\"åŸŸåè´­ä¹°\"></p>\n<p>è´­ä¹°å®Œæˆåï¼Œä½ å°±æ‹¥æœ‰äº†è‡ªå·±åŸŸåäº†ï¼</p>\n<h3 id=\"å¤‡æ¡ˆ\"><a href=\"#å¤‡æ¡ˆ\" class=\"headerlink\" title=\"å¤‡æ¡ˆ\"></a>å¤‡æ¡ˆ</h3><blockquote>\n<p>å¤‡æ¡ˆæ˜¯ä¸­å›½å¤§é™†çš„ä¸€é¡¹æ³•è§„ï¼Œä½¿ç”¨å¤§é™†èŠ‚ç‚¹æœåŠ¡å™¨æä¾›äº’è”ç½‘ä¿¡æ¯æœåŠ¡çš„ç”¨æˆ·ï¼Œéœ€è¦åœ¨æœåŠ¡å™¨æä¾›å•†å¤„æäº¤å¤‡æ¡ˆç”³è¯·ã€‚</p>\n<p>æ ¹æ®å·¥ä¿¡éƒ¨ã€Šäº’è”ç½‘ä¿¡æ¯æœåŠ¡ç®¡ç†åŠæ³•ã€‹(å›½åŠ¡é™¢292å·ä»¤)å’Œå·¥ä¿¡éƒ¨ä»¤ç¬¬33å·ã€Šéç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å¤‡æ¡ˆç®¡ç†åŠæ³•ã€‹è§„å®šï¼Œå›½å®¶å¯¹ç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œè®¸å¯åˆ¶åº¦ï¼Œå¯¹éç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œå¤‡æ¡ˆåˆ¶åº¦ã€‚æœªå–å¾—è®¸å¯æˆ–è€…æœªå±¥è¡Œå¤‡æ¡ˆæ‰‹ç»­çš„ï¼Œä¸å¾—ä»äº‹äº’è”ç½‘ä¿¡æ¯æœåŠ¡ï¼Œå¦åˆ™å±è¿æ³•è¡Œä¸ºã€‚é€šä¿—æ¥è®²ï¼Œè¦å¼€åŠç½‘ç«™å¿…é¡»å…ˆåŠç†ç½‘ç«™å¤‡æ¡ˆï¼Œå¤‡æ¡ˆæˆåŠŸå¹¶è·å–é€šä¿¡ç®¡ç†å±€ä¸‹å‘çš„ICPå¤‡æ¡ˆå·åæ‰èƒ½å¼€é€šè®¿é—®ã€‚<a href=\"https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\">^2</a></p>\n</blockquote>\n<p>è¿™ä¸€æ­¥ä¸å¤šè¯´äº†ï¼Œå…·ä½“æ­¥éª¤æ¯”è¾ƒç¹çï¼ŒèŠ±è´¹çš„æ—¶é—´ä¹Ÿæ¯”è¾ƒé•¿ï¼Œéœ€è¦ä¸€ä¸¤å‘¨ã€‚ç½‘ç«™ä¸Šæœ‰å¾ˆæ¸…æ™°çš„<a href=\"https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html\">æ“ä½œæ–¹æ³•</a>ï¼Œè¯·è‡ªè¡ŒæŸ¥é˜…ï¼Œæ ¹æ®æ­¥éª¤æ“ä½œå³å¯ã€‚éœ€è¦æ³¨æ„ä¸€ç‚¹çš„æ˜¯ï¼Œåœ¨å®¡æ ¸è¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ¥åˆ°æœåŠ¡æä¾›å•†æ‰“æ¥çš„ç”µè¯ï¼Œä¸è¦æ¼æ¥ã€‚</p>\n<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢çš„å¤‡æ¡ˆæ“ä½œæ˜¯åœ¨å·¥ä¿¡éƒ¨å¤‡æ¡ˆçš„ã€‚å®Œæˆäº†åœ¨å·¥ä¿¡éƒ¨çš„å¤‡æ¡ˆä»¥åè¿˜éœ€è¦å…¬å®‰å¤‡æ¡ˆã€‚å…·ä½“<a href=\"http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c\">æ“ä½œæ–¹æ³•</a>ä¹Ÿè¯·è‡ªè¡ŒæŸ¥é˜…ã€‚</p>\n<h3 id=\"åŸŸåè§£æ\"><a href=\"#åŸŸåè§£æ\" class=\"headerlink\" title=\"åŸŸåè§£æ\"></a>åŸŸåè§£æ</h3><p>åœ¨å®Œæˆä¸€ç³»åˆ—ç¹ççš„å¤‡æ¡ˆæµç¨‹ä»¥åï¼Œä½ çš„ç½‘ç«™è¿˜ä¸å¯ä»¥é€šè¿‡åŸŸåè®¿é—®ã€‚åªæœ‰æŠŠä½ çš„åŸŸåè·ŸæœåŠ¡å™¨çš„IPåœ°å€ç»‘å®šåœ¨ä¸€èµ·ä¹‹åï¼Œå¹¶ä¸”åœ¨æœåŠ¡å™¨ä¸Šä¿®æ”¹äº†é…ç½®æ–‡ä»¶ä¹‹åæ‰å¯ä»¥ã€‚</p>\n<p>é¦–å…ˆæ‰“å¼€ç®¡ç†æ§åˆ¶å°ï¼Œåœ¨æ§åˆ¶å°ä¸­é€‰æ‹©â€œåŸŸåæ³¨å†Œâ€ã€‚ç„¶ååœ¨ä¸‹é¢çš„é¡µé¢ä¸­ç‚¹å‡»â€œè§£æâ€ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/domain.png\" alt=\"åŸŸåæ³¨å†Œ\"></p>\n<p>ç‚¹å‡»ä½ çš„åŸŸåï¼Œæ˜¾ç¤ºå¦‚ä¸‹é¡µé¢ã€‚è¿™é‡Œæ˜¾ç¤ºçš„æ˜¯ä½ åŸŸåçš„è®°å½•é›†ï¼Œå‰ä¸¤ä¸ªè®°å½•é›†åº”è¯¥æ˜¯é¢„ç½®è®¾ç½®ï¼Œä¸å¯æš‚åœæœåŠ¡ã€‚<span id=\"1\">ä½ å¯ä»¥åœ¨è¿™åŸºç¡€ä¸Šæ·»åŠ è‡ªå·±çš„è®°å½•é›†ã€‚</span></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/record.png\" alt=\"è®°å½•é›†\"></p>\n<p>ç‚¹å‡»é¡µé¢å³ä¸Šè§’çº¢è‰²æŒ‰é’®ä»¥æ·»åŠ è®°å½•é›†ã€‚æ·»åŠ è®°å½•é›†çš„é…ç½®å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä¸‹å›¾ä¸­ç»™å‡ºçš„ä¾‹å­æ˜¯æ·»åŠ çš„â€œAâ€å‹è®°å½•é›†ï¼Œä¹Ÿå³é€šè¿‡<code>example.com</code>è®¿é—®ç½‘ç«™ã€‚è‹¥éœ€è¦é€šè¿‡<code>www.example.com</code>è®¿é—®ç½‘ç«™ï¼Œåˆ™éœ€è¦ä¸º<code>example.com</code>çš„å­åŸŸåæ·»åŠ â€œAâ€å‹è®°å½•é›†ã€‚å…·ä½“é…ç½®å‚è§ï¼š<a href=\"https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1\">é…ç½®ç½‘ç«™è§£æ_åä¸ºäº‘</a>ã€‚ç‚¹å‡»â€œç¡®å®šâ€ï¼Œå®Œæˆæ·»åŠ ã€‚ä½ å¯ä»¥é€šè¿‡<code>ping ä½ çš„åŸŸå</code>æ¥æµ‹è¯•ä½ æ·»åŠ çš„è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆäº†ã€‚</p>\n<p><img src=\"https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png\" alt=\"æ·»åŠ è®°å½•é›†\"></p>\n<h3 id=\"é…ç½®nginx\"><a href=\"#é…ç½®nginx\" class=\"headerlink\" title=\"é…ç½®nginx\"></a>é…ç½®nginx</h3><p><span id=\"2\">æ‰“å¼€</span>ä½ ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œè¾“å…¥å‘½ä»¤ï¼š<code>ssh ä½ çš„IPåœ°å€</code>ï¼Œè¾“å…¥ä½ çš„æœåŠ¡å™¨çš„å¯†ç ã€‚</p>\n<p>è¿›å…¥ä½ çš„nginxçš„å®‰è£…ç›®å½•ï¼š<code>cd /usr/local/nginx/</code>ã€‚</p>\n<p>ä½¿ç”¨vimæ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ï¼š<code>vim ./conf/nginx.conf</code>ã€‚</p>\n<p>æŒ‰<code>I</code>å¼€å§‹è¾“å…¥ã€‚</p>\n<p>åœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">\t    <span class=\"attribute\">listen</span>   <span class=\"number\">80</span>; <span class=\"comment\">#ç›‘å¬ç«¯å£è®¾ä¸º 80</span></span><br><span class=\"line\">\t    <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#ç»‘å®šæ‚¨çš„åŸŸå</span></span><br><span class=\"line\">\t    <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#æŒ‡å®šé»˜è®¤æ–‡ä»¶</span></span><br><span class=\"line\">\t    <span class=\"attribute\">root</span> html; <span class=\"comment\">#æŒ‡å®šç½‘ç«™æ ¹ç›®å½•</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>ç„¶åæŒ‰<code>esc</code>é€€å‡ºç¼–è¾‘ï¼Œå†æŒ‰<code>Shift+zz</code>ä¿å­˜ã€‚</p>\n<p>è¾“å…¥ï¼š<code>cd ./sbin</code>ï¼Œåˆ‡æ¢æ–‡ä»¶å¤¹ã€‚</p>\n<p>æ‰§è¡Œå‘½ä»¤ï¼š<code>nginx -s relod</code>ï¼Œé‡å¯nginxæœåŠ¡ã€‚</p>\n<p>è¿™æ—¶å€™å†å°è¯•ç”¨æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œåº”è¯¥ä¼šæ˜¾ç¤ºä¹‹å‰å‡ºç°è¿‡çš„â€œWelcome to nginx â€çš„é¡µé¢äº†ï¼</p>\n<h3 id=\"ç”³è¯·SSLè¯ä¹¦\"><a href=\"#ç”³è¯·SSLè¯ä¹¦\" class=\"headerlink\" title=\"ç”³è¯·SSLè¯ä¹¦\"></a>ç”³è¯·SSLè¯ä¹¦</h3><p>SSLè¯ä¹¦å¯ä»¥åœ¨æ•°æ®ä¼ è¾“çš„è¿‡ç¨‹ä¸­å¯¹å…¶è¿›è¡ŒåŠ å¯†å’Œéšè—ï¼Œå¯ä»¥æå¤§åœ°æé«˜æ•°æ®ä¼ è¾“çš„å®‰å…¨æ€§ã€‚æ‹¥æœ‰SSLè¯ä¹¦çš„ç½‘ç«™çš„è¯·æ±‚å¤´éƒ½æ˜¯<code>https</code>ï¼Œå¹¶ä¸”åœ¨é“¾æ¥æ—è¾¹ä¼šå‡ºç°ä¸€æŠŠå°é”ã€‚ä½†æ˜¯ï¼ŒSSLè¯ä¹¦å¹¶ä¸æ˜¯æ‰€æœ‰ç½‘ç«™éƒ½å¿…é¡»çš„ï¼Œè¿™è§†ä½ çš„éœ€è¦è€Œå®šã€‚æ¯”å¦‚ï¼Œå¾®ä¿¡å°ç¨‹åºçš„æœåŠ¡å™¨å°±å¿…é¡»è¦æœ‰åŸŸåå’ŒSSLè¯ä¹¦ã€‚å¦å¤–ï¼Œå‡ºäºä¿¡æ¯ä¼ è¾“çš„å®‰å…¨æ€§æ–¹é¢çš„è€ƒè™‘ï¼Œæœ‰SSLè¯ä¹¦è¿˜æ˜¯æ˜¾å¾—æ›´ä¸ºå¦¥å½“å’Œä¸“ä¸šä¸€ç‚¹ã€‚</p>\n<p>ç°åœ¨å¸‚é¢ä¸Šå„å¤§äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›é…å¥—çš„SSLè¯ä¹¦ç”³è¯·æœåŠ¡ï¼Œä¸€èˆ¬éƒ½æ˜¯æä¾›ä¼ä¸šçº§çš„è¯ä¹¦ï¼Œä»·æ ¼æ¯”è¾ƒæ˜‚è´µã€‚ä½†æ˜¯åŒæ—¶ç½‘ç»œä¸Šä¹Ÿæœ‰ä¸€äº›å…è´¹çš„SSLè¯ä¹¦æœåŠ¡å¯ä»¥é€‰æ‹©ã€‚ä¸‹é¢è¿˜æ˜¯ä»¥åä¸ºäº‘çš„å¹³å°ä¸ºä¾‹ï¼Œç®€å•è¯´æ˜ä¸€ä¸‹å¦‚ä½•ç”³è¯·SSLè¯ä¹¦ã€‚</p>\n<p>é¦–å…ˆåœ¨åä¸ºäº‘é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œå…è´¹è¯ä¹¦â€œï¼Œç„¶åç‚¹å‡»<a href=\"https://marketplace.huaweicloud.com/product/00301-315148-0--0\">äºšæ´²è¯šä¿¡åŸŸåå‹DVå•åŸŸåSSLè¯ä¹¦â€“å…è´¹è¯ä¹¦</a>ï¼Œå¯ä»¥çœ‹åˆ°è¯ä¹¦çš„ä»·æ ¼æ˜¯0.00å…ƒã€‚ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_ssl.png\" alt=\"è´­ä¹°SSLè¯ä¹¦\"></p>\n<p>å®Œæˆè´­ä¹°åè¯·ä¸è¦ç«‹å³å…³é—­é¡µé¢ï¼Œé¡µé¢ä¸­çš„è®¢å•å·åœ¨ä¹‹åè¿˜éœ€è¦ç”¨åˆ°ã€‚å°”åï¼Œç³»ç»Ÿä¼šå‘é€â€HuaweiCloudè´¦æˆ·ç”³è¯·â€é‚®ä»¶è‡³ç”¨æˆ·é‚®ç®±ï¼Œå³ä½ åœ¨åä¸ºäº‘çš„æ³¨å†Œé‚®ç®±ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/request_account.png\" alt=\"HuaweiCloudè´¦æˆ·ç”³è¯·\"></p>\n<p>ç‚¹å‡»é‚®ä»¶ä¸­çš„ç™»å½•åœ°å€è¿›å…¥ç³»ç»Ÿï¼Œå¹¶ä½¿ç”¨é‚®ä»¶æä¾›çš„è´¦å·å’Œåˆå§‹å¯†ç è¿›è¡Œç™»å½•ã€‚ç™»å…¥ç³»ç»Ÿåè¯·ä¿®æ”¹ä½ çš„åˆå§‹å¯†ç ï¼Œç„¶åè¯·æ ¹æ®åä¸ºäº‘ä¸­ç»™ä½ æä¾›çš„è®¢å•å·åœ¨è¯¥ç³»ç»Ÿä¸­æŸ¥è¯¢ä½ çš„è®¢å•ã€‚æŸ¥è¯¢åˆ°ä½ çš„è®¢å•ä»¥åï¼Œéœ€è¦ä½ è¡¥å……ä¸€äº›ä¿¡æ¯ï¼Œè¯·å¦‚å®å¡«å†™ã€‚ç³»ç»Ÿä¼šè¦ä½ å¡«å†™å…¬å¸ä¿¡æ¯ï¼Œå¦‚æœåªæ˜¯ä¸ªäººç½‘ç«™ï¼Œé‚£ä¹ˆå…¬å¸åç§°ç›´æ¥å¡«å†™ä½ çš„åå­—å³å¯ï¼Œå…¬å¸åœ°å€å°±å¡«å†™ä½ çš„ä½å€ã€‚</p>\n<p>å¡«å†™å®Œæˆåä¼šè¿›å…¥å®¡æ ¸é˜¶æ®µï¼Œç³»ç»Ÿä¼šç»™ä½ å‘é€ä¸€å°é‚®ä»¶ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/check.png\" alt=\"è¯ä¹¦å®¡æ ¸\"></p>\n<p>æ ¹æ®é‚®ä»¶çš„æç¤ºï¼Œéœ€è¦åœ¨è®°å½•é›†ä¸­æ·»åŠ æ–°çš„å†…å®¹ã€‚è¯·æ ¹æ®<a href=\"#1\">å‰æ–‡</a>æ‰€è¿°æ–¹æ³•ï¼Œå°†é‚®ä»¶ä¸­çš„å†…å®¹æ·»åŠ è‡³æ–°çš„è®°å½•é›†ã€‚å¡«å†™æ–¹æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/modify_record.png\" alt=\"å¡«å†™è®°å½•é›†\"></p>\n<p>å¡«å†™å®Œæˆåï¼Œå¯ä»¥åœ¨æœ¬åœ°ç”µè„‘çš„ç»ˆç«¯é‡Œè¾“å…¥<code>nslookup -querytype=txt ä½ çš„åŸŸå</code>æ¥æµ‹è¯•è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/test_record.png\" alt=\"æµ‹è¯•è®°å½•é›†\"></p>\n<p>ä¸€èˆ¬æ¥è¯´ï¼Œè®°å½•é›†ç”Ÿæ•ˆå10åˆ†é’Ÿä»¥å†…è¯ä¹¦å°±ä¼šé¢å‘äº†ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/issue.png\" alt=\"è¯ä¹¦é¢å‘\"></p>\n<h3 id=\"SSLè¯ä¹¦éƒ¨ç½²\"><a href=\"#SSLè¯ä¹¦éƒ¨ç½²\" class=\"headerlink\" title=\"SSLè¯ä¹¦éƒ¨ç½²\"></a>SSLè¯ä¹¦éƒ¨ç½²</h3><p>æ¥ä¸‹æ¥æˆ‘ä»¬è¦æŠŠSSLè¯ä¹¦éƒ¨ç½²åˆ°æˆ‘ä»¬çš„æœåŠ¡å™¨ä¸Šã€‚</p>\n<p>åœ¨æ”¶åˆ°çš„â€œè¯ä¹¦é¢å‘â€çš„é‚®ä»¶çš„åº•éƒ¨æœ‰ä¸€æ¡é“¾æ¥ï¼Œç‚¹å‡»è¿™æ¡é“¾æ¥ï¼Œè¿›å…¥è¯ä¹¦ç®¡ç†ç³»ç»Ÿã€‚ç™»å½•ç³»ç»Ÿï¼Œåœ¨å·¦ä¾§å¯¼èˆªæ ä¸­ç‚¹å‡»â€œSSLè¯ä¹¦â€ï¼Œå†ç‚¹å‡»â€œé¢„è§ˆâ€ï¼Œå†åœ¨å³ä¾§çš„â€œä¿¡æ¯é¢„è§ˆâ€ä¸­ç‚¹å‡»â€œä¸‹è½½æœ€æ–°è¯ä¹¦â€œã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert.png\" alt=\"ä¸‹è½½è¯ä¹¦\"></p>\n<p>åœ¨å¼¹å‡ºçš„å¯¹è¯æ¡†å†…ï¼Œé€‰æ‹©è¯ä¹¦æ ¼å¼ä¸ºâ€œPEM(é€‚ç”¨äºNginx,SLB)â€ï¼Œè¾“å…¥ä½ çš„è®¢å•å¯†ç ã€‚è¯ä¹¦å¯†ç å¯ä»¥ç•™ç©ºã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert1.png\" alt=\"ä¸‹è½½è¯ä¹¦\"></p>\n<p>ä¸‹è½½å®Œæˆåï¼Œè§£å‹ä¸‹è½½çš„å‹ç¼©åŒ…ï¼Œéœ€è¦è¾“å…¥ä½ çš„è®¢å•å¯†ç ï¼ˆå¦‚æœä½ æ²¡æœ‰è®¾ç½®è¯ä¹¦å¯†ç ï¼‰ã€‚è§£å‹ä»¥åå¯ä»¥å¾—åˆ°ä¸‹å›¾ä¸¤ä¸ªæ–‡ä»¶ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/unzip_cert.png\" alt=\"è§£å‹ç¼©\"></p>\n<p>æ¥ä¸‹æ¥ï¼Œæ‰“å¼€ä½ çš„ç»ˆç«¯ï¼ŒæŒ‰é¡ºåºè¾“å…¥ä¸‹åˆ—å‘½ä»¤ï¼š</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh ä½ çš„å…¬ç½‘IP #sshç™»å½•ï¼Œè¾“å…¥ä½ çš„å¯†ç </span><br><span class=\"line\">cd /usr/local/nginx #åˆ‡æ¢åˆ°nginxçš„å®‰è£…ç›®å½•</span><br><span class=\"line\">mkdir ./cert #åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹certç”¨äºå­˜æ”¾ä½ çš„è¯ä¹¦</span><br><span class=\"line\">exit #æ–­å¼€ä¸æœåŠ¡å™¨çš„è¿æ¥</span><br><span class=\"line\">scp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.key ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.keyæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹</span><br><span class=\"line\">scp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.crt ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.crtæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹</span><br></pre></td></tr></table></figure>\n\n<p>æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä¿®æ”¹nginxçš„é…ç½®æ–‡ä»¶ã€‚å‚è€ƒ<a href=\"#2\">å‰æ–‡</a>æ‰€è¿°æ–¹æ³•æ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ã€‚å…ˆå°†ä½ ä¹‹å‰æ’å…¥çš„å†…å®¹åˆ é™¤æˆ–è€…ä½¿ç”¨<code>#</code>æ³¨é‡Šæ‰ï¼Œç„¶ååœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">         <span class=\"attribute\">listen</span>       <span class=\"number\">443</span> ssl;</span><br><span class=\"line\">         <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#ä½ è¯ä¹¦ç»‘å®šçš„åŸŸå;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate</span>      /usr/local/nginx/cert/ä½ çš„åŸŸå.crt;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate_key</span>  /usr/local/nginx/cert/ä½ çš„åŸŸå.key;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_cache</span>    shared:SSL:<span class=\"number\">1m</span>;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_timeout</span>  <span class=\"number\">5m</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_ciphers</span>  HIGH:!aNULL:!MD5;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_prefer_server_ciphers</span>  <span class=\"literal\">on</span>;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"attribute\">location</span> / &#123;</span><br><span class=\"line\">            <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#æŒ‡å®šé»˜è®¤æ–‡ä»¶ã€‚</span></span><br><span class=\"line\">\t    \t\t\t<span class=\"attribute\">root</span> html; <span class=\"comment\">#æŒ‡å®šç½‘ç«™æ ¹ç›®å½•ã€‚</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"section\">server</span> &#123; <span class=\"comment\">#å°†ä½ çš„80ç«¯å£é‡å®šå‘è‡³433ç«¯å£ï¼Œå³å¼ºåˆ¶ä½¿ç”¨httpsè®¿é—®</span></span><br><span class=\"line\">  \t\t\t<span class=\"attribute\">listen</span> <span class=\"number\">80</span>;</span><br><span class=\"line\">  \t\t\tserver_name; example.com; #ä½ çš„åŸŸå</span><br><span class=\"line\">\t\t\t\t<span class=\"attribute\">rewrite</span><span class=\"regexp\"> ^/(.*)$</span> https://example.com:443/<span class=\"variable\">$1</span> <span class=\"literal\">permanent</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>å°†æ–‡ä»¶ä¿å­˜ä»¥åé‡å¯nginxæœåŠ¡ã€‚</p>\n<p>é‡å¯ä»¥åä½ å¯èƒ½ä¼šé‡åˆ°è¿™æ ·çš„é—®é¢˜ï¼š<code>**unknown directive â€œsslâ€ in /usr/local/nginx/conf/nginx.conf:121**</code>ï¼Œè¿™æ˜¯å› ä¸ºä½ åœ¨å®‰è£…nginxæ—¶ï¼Œæ²¡æœ‰ç¼–è¯‘SSLæ¨¡å—ã€‚ä½ å¯ä»¥åœ¨ç»ˆç«¯é‡ŒæŒ‰ç…§ä¸‹è¿°æ­¥éª¤è§£å†³<a href=\"https://blog.csdn.net/qq_26369317/article/details/102863613\">^ 3</a>ï¼š</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ../nginx-1.16.1 #è¿›å…¥åˆ°nginxçš„æºç åŒ…çš„ç›®å½•ä¸‹</span><br><span class=\"line\">./configure --with-http_ssl_module #å¸¦å‚æ•°æ‰§è¡Œç¨‹åº</span><br><span class=\"line\">make #ç¼–è¯‘</span><br><span class=\"line\">cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #å¤‡ä»½æ—§çš„nginx</span><br><span class=\"line\">cp ./objs/nginx /usr/local/nginx/sbin/ #ç„¶åå°†æ–°çš„nginxçš„ç¨‹åºå¤åˆ¶ä¸€ä»½</span><br><span class=\"line\">cd /usr/local/nginx/sbin/ #åˆ‡æ¢åˆ°sbinç›®å½•</span><br><span class=\"line\">./nginx -s reload #é‡å¯nginxæœåŠ¡</span><br></pre></td></tr></table></figure>\n\n<p>å¦‚æœé‡å¯æˆåŠŸçš„è¯ï¼Œæ‰“å¼€æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œè¿™æ—¶å€™åº”è¯¥å¯ä»¥åœ¨é“¾æ¥æ—è¾¹çœ‹åˆ°ä¸€ä¸ªå°é”äº†ï¼</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ck6ax1lfy0000j1p2e2ht8nn6","category_id":"ck4zczyf50004vq39a3nk34a3","_id":"ck6ax1lgk000aj1p24du0d7rk"},{"post_id":"ck6ax1lgj0009j1p22bs352jd","category_id":"ck6ax1lgi0007j1p20c54bs9g","_id":"ck6ax1lgr000hj1p2a8c9888a"},{"post_id":"ck6ax1lg50001j1p27ayd7j78","category_id":"ck6ax1lgi0007j1p20c54bs9g","_id":"ck6ax1lgr000ij1p24advef9p"},{"post_id":"ck6ax1lgl000bj1p271y3b93u","category_id":"ck6ax1lgi0007j1p20c54bs9g","_id":"ck6ax1lgs000kj1p2b4qc79mx"},{"post_id":"ck6ax1lgp000ej1p28m9ng16k","category_id":"ck6ax1lgi0007j1p20c54bs9g","_id":"ck6ax1lgs000mj1p22x3t61vs"},{"post_id":"ck6ax1lg80003j1p26y1j8hbi","category_id":"ck6ax1lgi0007j1p20c54bs9g","_id":"ck6ax1lgt000pj1p2egzc5x48"},{"post_id":"ck6ax1lgh0006j1p22s0p8u3s","category_id":"ck6ax1lgi0007j1p20c54bs9g","_id":"ck6ax1lgt000rj1p2bzro8hr2"},{"post_id":"ck4zczyf20002vq391oi1h22d","category_id":"ck4zczyf50004vq39a3nk34a3","_id":"ck6ax1lh2001kj1p2aawz08o6"}],"PostTag":[{"post_id":"ck4zczyf20002vq391oi1h22d","tag_id":"ck4zczyfa0009vq399q1t8lt4","_id":"ck4zczyff000ivq39bk6f16lr"},{"post_id":"ck4zczyf20002vq391oi1h22d","tag_id":"ck4zczyfb000dvq3911dn1jvf","_id":"ck4zczyff000jvq39bsfbbih9"},{"post_id":"ck4zczyf20002vq391oi1h22d","tag_id":"ck4zczyf70005vq3962g4f78i","_id":"ck4zczyff000kvq396bz728uv"},{"post_id":"ck4zczyf20002vq391oi1h22d","tag_id":"ck4zczyf80007vq39hd07brgp","_id":"ck4zczyff000lvq39das1hc8r"},{"post_id":"ck4zczyf20002vq391oi1h22d","tag_id":"ck4zczyf40003vq39gmca1ghi","_id":"ck4zczyff000mvq39aicm1mv5"},{"post_id":"ck6ax1lfy0000j1p2e2ht8nn6","tag_id":"ck4zczyf40003vq39gmca1ghi","_id":"ck6ax1lg80002j1p27306dy7m"},{"post_id":"ck6ax1lfy0000j1p2e2ht8nn6","tag_id":"ck4zczyf70005vq3962g4f78i","_id":"ck6ax1lgh0005j1p29ts9avjs"},{"post_id":"ck6ax1lfy0000j1p2e2ht8nn6","tag_id":"ck4zczyf80007vq39hd07brgp","_id":"ck6ax1lgj0008j1p23pe08xxk"},{"post_id":"ck6ax1lg50001j1p27ayd7j78","tag_id":"ck6ax1lg90004j1p2dvieauok","_id":"ck6ax1lgs000lj1p25ysqfi4y"},{"post_id":"ck6ax1lg50001j1p27ayd7j78","tag_id":"ck6ax1lgn000cj1p2ggygbggr","_id":"ck6ax1lgs000nj1p27xas7x9g"},{"post_id":"ck6ax1lg50001j1p27ayd7j78","tag_id":"ck6ax1lgr000gj1p2avrb8jb1","_id":"ck6ax1lgt000qj1p264lna9df"},{"post_id":"ck6ax1lg80003j1p26y1j8hbi","tag_id":"ck6ax1lgs000jj1p27pu419aw","_id":"ck6ax1lgu000uj1p2ftl75k3m"},{"post_id":"ck6ax1lg80003j1p26y1j8hbi","tag_id":"ck6ax1lgn000cj1p2ggygbggr","_id":"ck6ax1lgu000vj1p2bhg89zhg"},{"post_id":"ck6ax1lg80003j1p26y1j8hbi","tag_id":"ck6ax1lgr000gj1p2avrb8jb1","_id":"ck6ax1lgv000xj1p2ftn614w1"},{"post_id":"ck6ax1lgh0006j1p22s0p8u3s","tag_id":"ck6ax1lgs000jj1p27pu419aw","_id":"ck6ax1lgw0010j1p2h4by2pdv"},{"post_id":"ck6ax1lgh0006j1p22s0p8u3s","tag_id":"ck6ax1lgn000cj1p2ggygbggr","_id":"ck6ax1lgw0011j1p23k0obiha"},{"post_id":"ck6ax1lgh0006j1p22s0p8u3s","tag_id":"ck6ax1lgr000gj1p2avrb8jb1","_id":"ck6ax1lgw0013j1p2ge2w0yyy"},{"post_id":"ck6ax1lgj0009j1p22bs352jd","tag_id":"ck6ax1lgr000gj1p2avrb8jb1","_id":"ck6ax1lgx0015j1p2gm7y07fv"},{"post_id":"ck6ax1lgj0009j1p22bs352jd","tag_id":"ck6ax1lgw0012j1p27s2r4cey","_id":"ck6ax1lgx0016j1p27kr6dpah"},{"post_id":"ck6ax1lgl000bj1p271y3b93u","tag_id":"ck6ax1lgs000jj1p27pu419aw","_id":"ck6ax1lgy001aj1p2faf22g50"},{"post_id":"ck6ax1lgl000bj1p271y3b93u","tag_id":"ck6ax1lgn000cj1p2ggygbggr","_id":"ck6ax1lgy001bj1p20ngz8umg"},{"post_id":"ck6ax1lgl000bj1p271y3b93u","tag_id":"ck6ax1lgr000gj1p2avrb8jb1","_id":"ck6ax1lgz001dj1p2aoyr9haq"},{"post_id":"ck6ax1lgp000ej1p28m9ng16k","tag_id":"ck6ax1lgy0019j1p2dogrhp9r","_id":"ck6ax1lh0001gj1p26a42754l"},{"post_id":"ck6ax1lgp000ej1p28m9ng16k","tag_id":"ck6ax1lgy001cj1p2h70dcs6d","_id":"ck6ax1lh0001hj1p2du941eny"},{"post_id":"ck6ax1lgp000ej1p28m9ng16k","tag_id":"ck6ax1lgz001ej1p26t4x7mfc","_id":"ck6ax1lh0001ij1p25h9n4wot"},{"post_id":"ck6ax1lgp000ej1p28m9ng16k","tag_id":"ck6ax1lgz001fj1p2b2x57i5g","_id":"ck6ax1lh0001jj1p22ikf5j4s"}],"Tag":[{"name":"Astrobear","_id":"ck4zczyf40003vq39gmca1ghi"},{"name":"Life","_id":"ck4zczyf70005vq3962g4f78i"},{"name":"Others","_id":"ck4zczyf80007vq39hd07brgp"},{"name":"Photos","_id":"ck4zczyfa0009vq399q1t8lt4"},{"name":"Astrophotography","_id":"ck4zczyfb000dvq3911dn1jvf"},{"name":"AirSim","_id":"ck6ax1lg90004j1p2dvieauok"},{"name":"Research","_id":"ck6ax1lgn000cj1p2ggygbggr"},{"name":"Python","_id":"ck6ax1lgr000gj1p2avrb8jb1"},{"name":"RL","_id":"ck6ax1lgs000jj1p27pu419aw"},{"name":"Programming Language","_id":"ck6ax1lgw0012j1p27s2r4cey"},{"name":"Nginx","_id":"ck6ax1lgy0019j1p2dogrhp9r"},{"name":"Internet server","_id":"ck6ax1lgy001cj1p2h70dcs6d"},{"name":"Network Technology","_id":"ck6ax1lgz001ej1p26t4x7mfc"},{"name":"Experience","_id":"ck6ax1lgz001fj1p2b2x57i5g"}]}}